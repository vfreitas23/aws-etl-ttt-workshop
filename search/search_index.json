{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ETL Migration & Modernization Train-The-Trainer Workshop - Hands On Practice!!!","title":"Home"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"** Setup **"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"Setup"},{"location":"00-PreSteps/aws-event/","text":"You will be provided with an AWS account to run this workshop. The temporary account is being created using Event Engine. You will be provided a participant hash key to login to your temporary account. Follow these steps to start using your account: Go to AWS Event Engine Portal Event engine Enter the provided hash in the text box . The button on the bottom right corner changes to Accept Terms & Login . Click on that button to continue . Click on Email One - Time Password ( OTP ) ; You also have option to use your personal Amazon.com uid and password (Note - Not your AWS account credentials) Event engine Provide your email address. Event engine Provide the OTP you have received in your email. !Event engine Click AWS Console on the dashboard. Event engine Take the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab. Event engine Now, you should be on the AWS Console page of your workshop environment.","title":"Aws event"},{"location":"00-PreSteps/cloud9-enviroment-setup/","text":"Preparing the Cloud9 Enviroment We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the AWS Cloud9 console in your environment and you should see a Cloud9 with name glueworkshop. Click Open IDE button to enter Cloud9 IDE environment. Cloud 9 console - In Cloud9, close the welcome tab then click on the menu bar Window and then New Terminal. This will create a new tab and load a command-line terminal. You will use this terminal window throughout this pre-step lab to execute the AWS CLI commands and scripts. new Cloud9 terminal During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] 1. Setting up Cloud9 Environment Variables \u00b6 [AT FIRST, REPLACE THE IAM ROLE IN THE INSTANCE FOR THE mod-XXXXX-AWSEC2InstanceProfile-XXXXXXX] - refresh the screen AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. <<<<<< 2. Switching Cloud9 Role Credentials \u00b6 [DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] > aws sts get - caller - identity > aws configure set region $AWS_REGION > aws configure get region > aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole -- etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" 3. Setting up Security Required Groups Inbound Rules \u00b6 ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg 4. Installing Required Libraries (Boto3) \u00b6 [INSTALL BOTO3 FOR STREAMIN LAB FURTHER] cli sudo pip3 install boto3 Go to the AWS S3 console and explore the bucket ${BUCKET_NAME}. [AND OTHER SETTINGS YOU DID!!!] You should see the following prefixes in the bucket. Take some time to explore the content in the bucket. You are finished setting up the workshop environment and can move on to Lab 01 now.","title":"Cloud9 enviroment setup"},{"location":"00-PreSteps/cloud9-enviroment-setup/#1-setting-up-cloud9-environment-variables","text":"[AT FIRST, REPLACE THE IAM ROLE IN THE INSTANCE FOR THE mod-XXXXX-AWSEC2InstanceProfile-XXXXXXX] - refresh the screen AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. <<<<<<","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/cloud9-enviroment-setup/#2-switching-cloud9-role-credentials","text":"[DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] > aws sts get - caller - identity > aws configure set region $AWS_REGION > aws configure get region > aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole -- etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\"","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/cloud9-enviroment-setup/#3-setting-up-security-required-groups-inbound-rules","text":"ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/cloud9-enviroment-setup/#4-installing-required-libraries-boto3","text":"[INSTALL BOTO3 FOR STREAMIN LAB FURTHER] cli sudo pip3 install boto3 Go to the AWS S3 console and explore the bucket ${BUCKET_NAME}. [AND OTHER SETTINGS YOU DID!!!] You should see the following prefixes in the bucket. Take some time to explore the content in the bucket. You are finished setting up the workshop environment and can move on to Lab 01 now.","title":"4. Installing Required Libraries (Boto3)"},{"location":"00-PreSteps/create-iam-user/","text":"","title":"Create iam user"},{"location":"00-PreSteps/deploy-cfn/","text":"","title":"Deploy cfn"},{"location":"00-PreSteps/pre-steps/","text":"Lab Pre-Steps If you are at an AWS hosted event (such as AWS re:Invent, AWS Summit, Immersions Day, on-site Workshop or other event hosted by AWS employees), you will be provided with a temporary Event Engine AWS account for the workshop. Please follow the AWS Event instructions to access your Event Engine AWS account. If you are running this workshop by yourself, your account must have the ability to create new IAM roles and scope other IAM permissions. Please follow Self Paced Labs instructions to prepare your AWS account.","title":"Pre Steps"},{"location":"00-PreSteps/self-paced-labs/","text":"","title":"Self paced labs"},{"location":"00-PreSteps/signup-aws/","text":"","title":"Signup aws"},{"location":"000-Intro/intro/","text":"ETL Migration & Modernization - Train-The-Trainer Workshop This workshop contains a set of XXX hands-on labs which complementes the Day One of the ETL Migration & Modernization - Train The Trainer Workshop. Participants will build an end-to-end ETL Pipeline that will...[add details of the pipeline...] After completing the labs, the participants will have a high level understand of AWS Glue core capabilities as well as they will be able to demonstrate most of the Glue core capabilities. Workshop Parts & Steps Part 0 - PRE STEPS \u00b6 1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3) Part 1 - TPCDS & RDS MySQL \u00b6 1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset Part 2 - GLUE DATABASE, TABLES, CONNECTIONS, CRAWLERS, CLASSIFIERS \u00b6 1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Running the MySQL-RDS-Crawler 3. Creating a New Crawler (and a Classifier for it!) Part 3 - GLUE (STUDIO) STREAMING \u00b6 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!) Part 4 - ORCHESTRATION & DATA ANALYSIS \u00b6 1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Starting Orchestration & Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog 6. Creating Views on Top of Cataloged Tables Part 5 - DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW \u00b6 1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Building & Managing DataBrew Recipes 3. Operationalizing a DataBrew Project into a Glue Job.","title":"Introduction"},{"location":"000-Intro/intro/#part-0-pre-steps","text":"1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3)","title":"Part 0 - PRE STEPS"},{"location":"000-Intro/intro/#part-1-tpcds-rds-mysql","text":"1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset","title":"Part 1 - TPCDS &amp; RDS MySQL"},{"location":"000-Intro/intro/#part-2-glue-database-tables-connections-crawlers-classifiers","text":"1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Running the MySQL-RDS-Crawler 3. Creating a New Crawler (and a Classifier for it!)","title":"Part 2 - GLUE DATABASE, TABLES, CONNECTIONS, CRAWLERS, CLASSIFIERS"},{"location":"000-Intro/intro/#part-3-glue-studio-streaming","text":"1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!)","title":"Part 3 - GLUE (STUDIO) STREAMING"},{"location":"000-Intro/intro/#part-4-orchestration-data-analysis","text":"1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Starting Orchestration & Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog 6. Creating Views on Top of Cataloged Tables","title":"Part 4 - ORCHESTRATION &amp; DATA ANALYSIS"},{"location":"000-Intro/intro/#part-5-data-quality-preparation-with-aws-glue-databrew","text":"1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Building & Managing DataBrew Recipes 3. Operationalizing a DataBrew Project into a Glue Job.","title":"Part 5 - DATA QUALITY &amp; PREPARATION WITH AWS GLUE DATABREW"},{"location":"01-MySQL-TPCDS/mysql-tpcds/","text":"----------[TPCDS DATA CONFIGURATION DEMO PART]--------- mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql [WAIT FOR ABOUT 3 MIN TO COMPLETE] mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt (OPTIONAL) sudo yum -y install telnet mysqlendpoint=$(aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey== MySQLEndpoint ].OutputValue | [0] | [0]' --output text) telnet $mysqlendpoint 3306 [CREATING THE RDS MYSQL DATABASE SCHEMAS FOR TPCDS DATA] tpcds_script_path=~//environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" [BELOW SCRIPT IS STORED IN THE EVENT ENGINE MODULE ASSET BUCKET] \u00b6 mysqlendpoint=$1 DIR=/tmp/dsd ls $DIR/*.dat | while read file; do pipe=$file.pipe mkfifo $pipe table= basename $file .dat | sed -e 's/_[0-9]_[0-9]//' echo $file $table LANG=C && sed -e 's_^| \\N|_g' -e 's || |\\N|_g' -e 's ||_|\\N|_g' $file > $pipe & \\ mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd --local-infile -Dtpcds -e \\ \"load data local infile '$pipe' replace into table $table character set latin1 fields terminated by '|'\" rm -f $pipe done [POPULATING THE MYSQL DATABASE WITH TPCDS TABLES'D DATA] aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint [EXTRACT TABLES FOR THE LABS INTO CSV FORMAT AND UPLOADING IT INTO S3] mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3://$BUCKET_NAME/etl-ttt-demo/csv_tables/","title":"TPCDS & RDS MySQL Lab"},{"location":"01-MySQL-TPCDS/mysql-tpcds/#below-script-is-stored-in-the-event-engine-module-asset-bucket","text":"mysqlendpoint=$1 DIR=/tmp/dsd ls $DIR/*.dat | while read file; do pipe=$file.pipe mkfifo $pipe table= basename $file .dat | sed -e 's/_[0-9]_[0-9]//' echo $file $table LANG=C && sed -e 's_^| \\N|_g' -e 's || |\\N|_g' -e 's ||_|\\N|_g' $file > $pipe & \\ mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd --local-infile -Dtpcds -e \\ \"load data local infile '$pipe' replace into table $table character set latin1 fields terminated by '|'\" rm -f $pipe done [POPULATING THE MYSQL DATABASE WITH TPCDS TABLES'D DATA] aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint [EXTRACT TABLES FOR THE LABS INTO CSV FORMAT AND UPLOADING IT INTO S3] mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${mysqlendpoint} -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3://$BUCKET_NAME/etl-ttt-demo/csv_tables/","title":"[BELOW SCRIPT IS STORED IN THE EVENT ENGINE MODULE ASSET BUCKET]"},{"location":"02-GlueComponentsLab/glue-components-lab/","text":"----------[SETTING UP GLUE DATABASE, ALL CRAWLERS AND CLASSIFIER FOR SUBSEQUENT LABS]--------- ---> TEST MYSQL CONNECTION IN GLUE CONSOLE ---> RUN THE ALREADY CREATED MYSQL-RDS-CRAWLER [3-5 MINUTES] - (THIS CRAWLER AND DATABASE IS ALREADY CREATED) -------> 2 TABLES GETS CREATED [WHILE ABOVE CRAWLER IS RUNNING, DO THE FOLLOWING TWO PIECES FOR THE STREAMING CRAWL (FOLLOWING LAB)] ---> [CREATE A CLASSIFIER] CSV Comma (,) Double-quote (\") Has headings c_full_name,c_email_address,total_clicks ---->[CREATE A CRAWLER TO READ FROM THE PATH OF THE STREAMING JOB - USE A CUSTOM CSV CLASSIFIER FOR THAT!!!] --> Name: crawl_streammed_data --> Classifier: CSV --> Include path: s3://etl-ttt-demo-${BUCKET_NAME}-${ACCOUNT_REGION}/etl-ttt-demo/output/gluestreaming/total_clicks/ --> Sample Size: 1 --> Exclude Pattern: **00001 --> Database: glue-ttt-demo-db --> Grouping Behavior: Create a single schema for each S3 path [check] !!!!DO NOT RUN THIS CRAWLER YET!!!!","title":"GLUE DATABASE, TABLES, CONNECTIONS, CRAWLERS, CLASSIFIERS"},{"location":"03-StreamingLab/streaming-lab/","text":"----------[SETTING UP GLUE DATABASE, ALL CRAWLERS AND CLASSIFIER FOR SUBSEQUENT LABS]--------- ---> TEST MYSQL CONNECTION IN GLUE CONSOLE ---> RUN THE ALREADY CREATED MYSQL-RDS-CRAWLER [3-5 MINUTES] - (THIS CRAWLER AND DATABASE IS ALREADY CREATED) -------> 2 TABLES GETS CREATED [WHILE ABOVE CRAWLER IS RUNNING, DO THE FOLLOWING TWO PIECES FOR THE STREAMING CRAWL (FOLLOWING LAB)] ---> [CREATE A CLASSIFIER] CSV Comma (,) Double-quote (\") Has headings c_full_name,c_email_address,total_clicks ---->[CREATE A CRAWLER TO READ FROM THE PATH OF THE STREAMING JOB - USE A CUSTOM CSV CLASSIFIER FOR THAT!!!] --> Name: crawl_streammed_data --> Classifier: CSV --> Include path: s3://etl-ttt-demo-${BUCKET_NAME}-${ACCOUNT_REGION}/etl-ttt-demo/output/gluestreaming/total_clicks/ --> Sample Size: 1 --> Exclude Pattern: **00001 --> Database: glue-ttt-demo-db --> Grouping Behavior: Create a single schema for each S3 path [check] !!!!DO NOT RUN THIS CRAWLER YET!!!!","title":"GLUE (STUDIO) STREAMING"},{"location":"04-OrchestrationLab/orchestration-lab/","text":"----------[EVENT BRIDGE LAB + STREAMING FOLLOWING STEPS]------------ EVENT BRIDGE TRIGGERS A WORKFLOW AS SOON AS STREAMING DATA LANDS IN S3 - WORKFLOW CRAWLS THE DATA --> MAKE SURE THE TRAIL HAS THE FOLLOWING PATH CONFIGURED: Bucket name: etl-ttt-demo-171714944327-ca-central-1 Prefix: /etl-ttt-demo/output/gluestreaming/total_clicks/ --- create the workflow first ---- aws glue create-workflow --name etl-ttt-event-driven-workflow aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl_streammed_data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" --- create the event rule ---- aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\"${BUCKET_NAME}\\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"glueworkflow-totalclicks\",\"Arn\"=\"arn:aws:glue:${AWS_REGION}:${AWS_ACCOUNT_ID}:workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam::${AWS_ACCOUNT_ID}:role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${AWS_REGION} [GET THE SCRIPT FROM BUCKET] cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py ------SCRIPT SHOULD LOOK LIKE THIS ----------------- import csv import json import boto3 import time import string import random def generate(stream_name, kinesis_client): with open(\"/tmp/dsd/csv_tables/web_page.csv\", encoding='utf-8') as csvf: csvReader = csv.DictReader(csvf) for rows in csvReader: partitionaKey = ''.join(random.choices(string.ascii_uppercase + string.digits, k = 20)) jsonMsg = json.dumps(rows) kinesis_client.put_record(StreamName = stream_name, Data = jsonMsg, PartitionKey = partitionaKey) print(jsonMsg) time.sleep(0.2) if name == ' main ': generate('etl-ttt-demo-stream', boto3.client('kinesis')) [end of script] [ONCE EVENT BRIDGE IS FULLY SETUP WITH TARGET AND ALL...] [START STREAMING DATA BY RUNNIG THE ABOVE SCRIPT WITH THE FOLLOWING COMMAND IN CLOUD9] [WAIT UP TO 1 OR 2 MINUTES] python PutRecord_Kinesis.py [OBSERVE THE WORKFLOW GETTING TRIGGER AS THE FIRST FILES ARRIVE IN S3 FROM THEM STREAMING JOB] ---OBSERVE THE FAILURES DUE TO MULTIPLE FILES --- [OBSERVE THAT WORKFLOW TRIGGERS THE CRAWLER WHICH CREATES THE TABLE (total_clicks) FOR FOLLOWING ANALYSIS LAB] [DELETE THE WORKFLOW OR EDIT THE TRIGGER TO 100 SO IT STOP RUNNING AT EVERY BATCH OF STREAMING DATA] ------------------------------------[ANALYSIS LAB]----------------------------------- [ADD DEFAULT BUCKET FOR ATHENA: /athena-output/] [RUN THE FOLLOWING QUERY] --> SELECT * FROM \"AwsDataCatalog\".\"glue-ttt-demo-db\".\"total_clicks\" order by 3 desc limit 10; [CREATE A VIEW ON TOP OF THE CREATED TABLE TO BROWSE THE TOP 5 CUSTOMERS CLICKING ON THE WEBSITE] \u00b6 CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\"(CAST(\"total_clicks\" AS integer)) total_clicks FROM \"total_clicks\" GROUP BY 1, 2 ORDER BY 3 DESC SELECT * FROM \"glue-ttt-demo-db\".\"tpc_customer_inter\" order by 3 desc limit 10; [ADD MORE DATA TO THE STREAM AND KEEP CHECKING] - EVERY 30-60 SECONDS","title":"ORCHESTRATION & DATA ANALYSIS"},{"location":"04-OrchestrationLab/orchestration-lab/#create-a-view-on-top-of-the-created-table-to-browse-the-top-5-customers-clicking-on-the-website","text":"CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\"(CAST(\"total_clicks\" AS integer)) total_clicks FROM \"total_clicks\" GROUP BY 1, 2 ORDER BY 3 DESC SELECT * FROM \"glue-ttt-demo-db\".\"tpc_customer_inter\" order by 3 desc limit 10; [ADD MORE DATA TO THE STREAM AND KEEP CHECKING] - EVERY 30-60 SECONDS","title":"[CREATE A VIEW ON TOP OF THE CREATED TABLE TO BROWSE THE TOP 5 CUSTOMERS CLICKING ON THE WEBSITE]"},{"location":"05-DataBrewLab/databrew-lab/","text":"------------------------------------[DATABREW LAB]----------------------------------- [CREATE A DATASET FROM THE CATALOGED STREAMED TABLE - TOTAL_CLICKS] [PROFILE THE JOB WITH DATA QUALITY RULES] --> not negative values? --> null values --> duplicates? (less than --> any other relevant rule? --> missing values (2%)","title":"DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW"}]}