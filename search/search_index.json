{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ETL Migration & Modernization Train-The-Trainer Workshop - Hands On Practice!!!","title":"Home"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"** Setup **"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"Setup"},{"location":"00-PreSteps/001-intro/","text":"Lab Pre-Steps If you are at an AWS hosted event (such as AWS re:Invent, AWS Summit, Immersions Day, on-site Workshop or other event hosted by AWS employees), you will be provided with a temporary Event Engine AWS account for the workshop. Please follow the AWS Event instructions to access your Event Engine AWS account. If you are running this workshop by yourself, your account must have the ability to create new IAM roles and scope other IAM permissions. Please follow Self Paced Labs instructions to prepare your AWS account.","title":"001 intro"},{"location":"00-PreSteps/AWSEvent/002-ee-account-setup/","text":"You will be provided with an AWS account to run this workshop. The temporary account is being created using Event Engine. You will be provided a participant hash key to login to your temporary account. Follow these steps to start using your account: Go to AWS Event Engine Portal Event engine Enter the provided hash in the text box . The button on the bottom right corner changes to Accept Terms & Login . Click on that button to continue . Click on Email One - Time Password ( OTP ) ; You also have option to use your personal Amazon.com uid and password (Note - Not your AWS account credentials) Event engine Provide your email address. Event engine Provide the OTP you have received in your email. !Event engine Click AWS Console on the dashboard. Event engine Take the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab. Event engine Now, you should be on the AWS Console page of your workshop environment.","title":"002 ee account setup"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/","text":"Preparing the Cloud9 Enviroment We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the AWS Cloud9 console in your environment and you should see a Cloud9 with name glueworkshop. Click Open IDE button to enter Cloud9 IDE environment. Cloud 9 console - In Cloud9, close the welcome tab then click on the menu bar Window and then New Terminal. This will create a new tab and load a command-line terminal. You will use this terminal window throughout this pre-step lab to execute the AWS CLI commands and scripts. new Cloud9 terminal During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] 1. Setting up Cloud9 Environment Variables \u00b6 [AT FIRST, REPLACE THE IAM ROLE IN THE INSTANCE FOR THE mod-XXXXX-AWSEC2InstanceProfile-XXXXXXX] - refresh the screen AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. <<<<<< 2. Switching Cloud9 Role Credentials \u00b6 [DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" 3. Setting up Security Required Groups Inbound Rules \u00b6 ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg 4. Installing Required Libraries (Boto3) \u00b6 [INSTALL BOTO3 FOR STREAMIN LAB FURTHER] sudo pip3 install boto3 Go to the AWS S3 console and explore the bucket ${BUCKET_NAME}. [AND OTHER SETTINGS YOU DID!!!] You should see the following prefixes in the bucket. Take some time to explore the content in the bucket. You are finished setting up the workshop environment and can move on to Lab 01 now.","title":"003 cloud9 enviroment setup"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#1-setting-up-cloud9-environment-variables","text":"[AT FIRST, REPLACE THE IAM ROLE IN THE INSTANCE FOR THE mod-XXXXX-AWSEC2InstanceProfile-XXXXXXX] - refresh the screen AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. <<<<<<","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#2-switching-cloud9-role-credentials","text":"[DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\"","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#3-setting-up-security-required-groups-inbound-rules","text":"ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#4-installing-required-libraries-boto3","text":"[INSTALL BOTO3 FOR STREAMIN LAB FURTHER] sudo pip3 install boto3 Go to the AWS S3 console and explore the bucket ${BUCKET_NAME}. [AND OTHER SETTINGS YOU DID!!!] You should see the following prefixes in the bucket. Take some time to explore the content in the bucket. You are finished setting up the workshop environment and can move on to Lab 01 now.","title":"4. Installing Required Libraries (Boto3)"},{"location":"00-PreSteps/SelfPaced/004-signup-aws/","text":"If you've already signed up for Amazon Web Services (AWS) and have a user with administrator access, you can move on to Deploy CloudFormation Template. If you haven't signed up for AWS account yet, or if you need assistance, follow the following instructions to set up your AWS account for the workshop. Sign Up for AWS When you sign up for Amazon Web Services (AWS), your AWS account is automatically signed up for all services in AWS, including Lake Formation. You are charged only for the services that you use. If you have an AWS account already, skip to the next task. If you don't have an AWS account, use the following procedure to create one. To create an AWS account Open https : // aws . amazon . com / and then choose Create an AWS Account . If you previously signed in to the AWS Management Console using AWS account root user credentials, choose Sign in to a different account. If you previously signed in to the console using IAM credentials, choose Sign-in using root account credentials. Then choose Create a new AWS account. Follow the online instructions . Part of the sign - up procedure involves receiving a phone call and entering a verification code using the phone keypad . Note your AWS account number , because you ' ll need it for the next task.","title":"004 signup aws"},{"location":"00-PreSteps/SelfPaced/005-create-iam-user/","text":"Create IAM User Services in AWS, such as AWS Glue, require that you provide credentials when you access them, so that the service can determine whether you have permission to access its resources. The console requires your password. You can create access keys for your AWS account to access the command line interface or API. However, we don't recommend that you access AWS using the credentials for your AWS account; we recommend that you use AWS Identity and Access Management (IAM) instead. Create an IAM user, and then add the user to an IAM group with administrative permissions or grant this user administrative permissions. You can then access AWS using a special URL and the credentials for the IAM user. If you signed up for AWS but have not created an IAM user for yourself, you can create one using the IAM console. If you aren't familiar with using the console, see Working with the AWS Management Console for an overview. To create an IAM user for yourself and add the user to an Administrators group Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ We strongly recommend that you adhere to the best practice of using the Administrator IAM user below and securely lock away the root user credentials. Sign in as the root user only to perform a few account and service management tasks . In the navigation pane of the console , choose Users , and then choose Add user . For User name , type Administrator . Select the check box next to AWS Management Console access , select Custom password , and then type the new user ' s password in the text box. You can optionally select Require password reset to force the user to create a new password the next time the user signs in. Choose Next : Permissions . On the Set permissions page , choose Add user to group . Choose Create group. In the Create group dialog box , for Group name type Administrators . For Filter policies , select the check box for AWS managed - job function . In the policy list , select the check box for AdministratorAccess . Then choose Create group . Back in the list of groups , select the check box for your new group . Choose Refresh if necessary to see the group in the list . Choose Next : Tags to add metadata to the user by attaching tags as key - value pairs . Choose Next : Review to see the list of group memberships to be added to the new user . When you are ready to proceed , choose Create user . You can use this same process to create more groups and users , and to give your users access to your AWS account resources . To sign in as this new IAM user, sign out of the AWS console, then use the following URL, where your_aws_account_id is your AWS account number without the hyphens (for example, if your AWS account number is 1234-5678-9012, your AWS account ID is 123456789012): https://your_aws_account_id.signin.aws.amazon.com/console/ Enter the IAM user name (not your email address) and password that you just created. When you're signed in, the navigation bar displays \"your_user_name @ your_aws_account_id\". If you don't want the URL for your sign-in page to contain your AWS account ID, you can create an account alias. From the IAM console, choose Dashboard in the navigation pane. From the dashboard, choose Customize and enter an alias such as your company name. To sign in after you create an account alias, use the following URL: https://your_account_alias.signin.aws.amazon.com/console/ To verify the sign-in link for IAM users for your account, open the IAM console and check under IAM users sign-in link on the dashboard.","title":"005 create iam user"},{"location":"00-PreSteps/SelfPaced/006-deploy-cfn/","text":"Deploy CloudFormation Template We will use CloudFormation to set up the AWS environments for the labs. AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources, and provision and manage them in an orderly and predictable fashion. We will create one CloudFormation stack which includes all the resources; we will be using in the following labs. When you finish all labs, please be sure to delete the stack as per the Clean up section to avoid unnecessary usage charges. Click here to launch Workshop Cloudformation Click Next in Create stack screen, accept the default stack name glueworkshop in Specify stack details screen and click Next, Click Next in Configure stack options screen, check the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources with custom names. and click Create stack. You should see a new CloudFormation stack with the name glueworkshop being created. Wait for the status of the stack to be CREATE_COMPLETE before moving on. You can click on the stack name and go to Resource tab to check what AWS resoures were created as part of the CloudFormation stack. You should expect to wait approximately 10 minutes for the stack to move to the CREATE_COMPLETE state. Cloudformation creation","title":"006 deploy cfn"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup%20copy/","text":"Preparing the Cloud9 Enviroment We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the AWS Cloud9 console in your environment and you should see a Cloud9 with name glueworkshop. Click Open IDE button to enter Cloud9 IDE environment. Cloud 9 console - In Cloud9, close the welcome tab then click on the menu bar Window and then New Terminal. This will create a new tab and load a command-line terminal. You will use this terminal window throughout this pre-step lab to execute the AWS CLI commands and scripts. new Cloud9 terminal During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] 1. Setting up Cloud9 Environment Variables \u00b6 [AT FIRST, REPLACE THE IAM ROLE IN THE INSTANCE FOR THE mod-XXXXX-AWSEC2InstanceProfile-XXXXXXX] - refresh the screen AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. <<<<<< 2. Switching Cloud9 Role Credentials \u00b6 [DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" 3. Setting up Security Required Groups Inbound Rules \u00b6 ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg 4. Installing Required Libraries (Boto3) \u00b6 [INSTALL BOTO3 FOR STREAMIN LAB FURTHER] sudo pip3 install boto3 Go to the AWS S3 console and explore the bucket ${BUCKET_NAME}. [AND OTHER SETTINGS YOU DID!!!] You should see the following prefixes in the bucket. Take some time to explore the content in the bucket. You are finished setting up the workshop environment and can move on to Lab 01 now.","title":"007 cloud9 enviroment setup copy"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup%20copy/#1-setting-up-cloud9-environment-variables","text":"[AT FIRST, REPLACE THE IAM ROLE IN THE INSTANCE FOR THE mod-XXXXX-AWSEC2InstanceProfile-XXXXXXX] - refresh the screen AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. <<<<<<","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup%20copy/#2-switching-cloud9-role-credentials","text":"[DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\"","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup%20copy/#3-setting-up-security-required-groups-inbound-rules","text":"ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup%20copy/#4-installing-required-libraries-boto3","text":"[INSTALL BOTO3 FOR STREAMIN LAB FURTHER] sudo pip3 install boto3 Go to the AWS S3 console and explore the bucket ${BUCKET_NAME}. [AND OTHER SETTINGS YOU DID!!!] You should see the following prefixes in the bucket. Take some time to explore the content in the bucket. You are finished setting up the workshop environment and can move on to Lab 01 now.","title":"4. Installing Required Libraries (Boto3)"},{"location":"000-Intro/000-intro/","text":"ETL MIGRATION & MODERNIZATION TRAIN-THE-TRAINER WORKSHOP This workshop contains a set of seven hands-on labs which complementes the Day One of the ETL Migration & Modernization - Train The Trainer Workshop. Participants will build an end-to-end ETL Pipeline that will...[add details of the pipeline...] After completing the labs, the participants will have a high level understand of AWS Glue core capabilities as well as they will be able to demonstrate most of the Glue core capabilities. WORKSHOP PARTS & STEPS Part 0 - PRE STEPS \u00b6 1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3) Part 1 - TPCDS & RDS MySQL \u00b6 1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3 Part 2 - AWS GLUE COMPONENTS (Databases, Tables, Connections, Crawlers and Classifiers) \u00b6 1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Testing and running pre-created Glue Resources (Connection & MySQL-RDS-Crawler) 3. Creating new Glue Resources (Crawler & Classifier) Part 3 - GLUE (STUDIO) STREAMING \u00b6 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!) Part 4 - ORCHESTRATION & DATA ANALYSIS \u00b6 1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Triggering Orchestration & Following The Flow 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog Part 5 - MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOK \u00b6 0. (Pre Steps) - Understading & Setting up the Resources for the ML Lab 1. Creating and Training the Glue FindMatches ML Transform 2. Testing FindMatches Transform with Glue Studio Notebook 3. Deploying & Running a FindMatches Glue Job Part 6 - WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS \u00b6 1. Creating the Step Function Workflow 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI) Part 7 - DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW \u00b6 1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Working with DataBrew Recipes & Projects","title":"000 intro"},{"location":"000-Intro/000-intro/#part-0-pre-steps","text":"1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3)","title":"Part 0 - PRE STEPS"},{"location":"000-Intro/000-intro/#part-1-tpcds-rds-mysql","text":"1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3","title":"Part 1 - TPCDS &amp; RDS MySQL"},{"location":"000-Intro/000-intro/#part-2-aws-glue-components-databases-tables-connections-crawlers-and-classifiers","text":"1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Testing and running pre-created Glue Resources (Connection & MySQL-RDS-Crawler) 3. Creating new Glue Resources (Crawler & Classifier)","title":"Part 2 - AWS GLUE COMPONENTS (Databases, Tables, Connections, Crawlers and Classifiers)"},{"location":"000-Intro/000-intro/#part-3-glue-studio-streaming","text":"1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!)","title":"Part 3 - GLUE (STUDIO) STREAMING"},{"location":"000-Intro/000-intro/#part-4-orchestration-data-analysis","text":"1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Triggering Orchestration & Following The Flow 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog","title":"Part 4 - ORCHESTRATION &amp; DATA ANALYSIS"},{"location":"000-Intro/000-intro/#part-5-machine-learning-with-glue-glue-studio-notebook","text":"0. (Pre Steps) - Understading & Setting up the Resources for the ML Lab 1. Creating and Training the Glue FindMatches ML Transform 2. Testing FindMatches Transform with Glue Studio Notebook 3. Deploying & Running a FindMatches Glue Job","title":"Part 5 - MACHINE LEARNING WITH GLUE &amp; GLUE STUDIO NOTEBOOK"},{"location":"000-Intro/000-intro/#part-6-workflow-orchestration-with-aws-step-functions","text":"1. Creating the Step Function Workflow 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI)","title":"Part 6 - WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS"},{"location":"000-Intro/000-intro/#part-7-data-quality-preparation-with-aws-glue-databrew","text":"1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Working with DataBrew Recipes & Projects","title":"Part 7 - DATA QUALITY &amp; PREPARATION WITH AWS GLUE DATABREW"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/","text":"Working with TPC-DS Data & RDS MySQL Database <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Preparing & Generating TPCDS Dataset \u00b6 <<<<< write the right stuff [TPCDS DATA CONFIGURATION DEMO PART] tpcds - This data is used for a decision support benchmark. When you load this data the schema tpcds is updated with sample data. For more information about the tpcds data, see TPC-DS <<<<<< mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql [WAIT FOR ABOUT 3 MIN TO COMPLETE] <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt Testing RDS MySQL Database Connection - (OPTIONAL) <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< sudo yum -y install telnet mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) telnet $mysqlendpoint 3306 <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset \u00b6 <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< tpcds_script_path = ~/environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" [BELOW SCRIPT IS STORED IN THE EVENT ENGINE MODULE ASSET BUCKET] mysqlendpoint = $1 DIR = /tmp/dsd ls $DIR /*.dat | while read file ; do pipe = $file .pipe mkfifo $pipe table = ` basename $file .dat | sed -e 's/_[0-9]_[0-9]//' ` echo $file $table LANG = C && sed -e 's_^|_\\\\N|_g' -e 's_||_|\\\\N|_g' -e 's_||_|\\\\N|_g' $file > $pipe & \\ mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd --local-infile -Dtpcds -e \\ \"load data local infile ' $pipe ' replace into table $table character set latin1 fields terminated by '|'\" rm -f $pipe done <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3 \u00b6 <<<<< write the right stuff [EXTRACT TABLES FOR THE LABS INTO CSV FORMAT AND UPLOADING IT INTO S3] The following commands will blah blah lah blah <<<<<< mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/","title":"01 mysql tpcds"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#1-preparing-generating-tpcds-dataset","text":"<<<<< write the right stuff [TPCDS DATA CONFIGURATION DEMO PART] tpcds - This data is used for a decision support benchmark. When you load this data the schema tpcds is updated with sample data. For more information about the tpcds data, see TPC-DS <<<<<< mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql [WAIT FOR ABOUT 3 MIN TO COMPLETE] <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt Testing RDS MySQL Database Connection - (OPTIONAL) <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< sudo yum -y install telnet mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) telnet $mysqlendpoint 3306 <<<<< write the right stuff The following commands will blah blah lah blah <<<<<<","title":"1. Preparing &amp; Generating TPCDS Dataset"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#2-populating-the-amazon-rds-mysql-database-with-tpcds-dataset","text":"<<<<< write the right stuff The following commands will blah blah lah blah <<<<<< tpcds_script_path = ~/environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" [BELOW SCRIPT IS STORED IN THE EVENT ENGINE MODULE ASSET BUCKET] mysqlendpoint = $1 DIR = /tmp/dsd ls $DIR /*.dat | while read file ; do pipe = $file .pipe mkfifo $pipe table = ` basename $file .dat | sed -e 's/_[0-9]_[0-9]//' ` echo $file $table LANG = C && sed -e 's_^|_\\\\N|_g' -e 's_||_|\\\\N|_g' -e 's_||_|\\\\N|_g' $file > $pipe & \\ mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd --local-infile -Dtpcds -e \\ \"load data local infile ' $pipe ' replace into table $table character set latin1 fields terminated by '|'\" rm -f $pipe done <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint","title":"2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#3-unloading-tables-in-csv-from-rds-mysql-database-and-uploading-to-s3","text":"<<<<< write the right stuff [EXTRACT TABLES FOR THE LABS INTO CSV FORMAT AND UPLOADING IT INTO S3] The following commands will blah blah lah blah <<<<<< mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/","title":"3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3"},{"location":"02-GlueComponentsLab/02-glue-components-lab/","text":"AWS GLUE COMPONENTS (Databases, Tables, Connections, Crawlers And Classifiers) <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Understanding the Glue Resources Provided (CloudFormation Resources) \u00b6 <<<<< write the right stuff ----------[SETTING UP GLUE DATABASE, ALL CRAWLERS AND CLASSIFIER FOR SUBSEQUENT LABS]--------- <<<<<< 2. Testing and running pre-created Glue Resources (Glue Connection & Glue Crawler for MySQL RDS) \u00b6 <<<<< write the right stuff ---> TEST MYSQL CONNECTION IN GLUE CONSOLE ---> RUN THE ALREADY CREATED MYSQL-RDS-CRAWLER [3-5 MINUTES] - (THIS CRAWLER AND DATABASE IS ALREADY CREATED) -------> 2 TABLES GETS CREATED <<<<< write the right stuff 3. Creating new Glue Resources (New Crawler & Classifier) \u00b6 <<<<< write the right stuff [WHILE ABOVE CRAWLER IS RUNNING, DO THE FOLLOWING TWO PIECES FOR THE STREAMING CRAWL (FOLLOWING LAB)] ---> [CREATE A CLASSIFIER] CSV Comma (,) Double-quote (\") Has headings c_full_name,c_email_address,total_clicks ---->[CREATE A CRAWLER TO READ FROM THE PATH OF THE STREAMING JOB - USE A CUSTOM CSV CLASSIFIER FOR THAT!!!] --> Name: crawl_streammed_data --> Classifier: CSV --> Include path: [In Cloud9] - echo \"s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/gluestreaming/total_clicks/\" --> Sample Size: 1 --> Exclude Pattern: **00001 --> Database: glue-ttt-demo-db --> Grouping Behavior: Create a single schema for each S3 path [check] !!!!DO NOT RUN THIS CRAWLER YET!!!! <<<<< write the right stuff","title":"02 glue components lab"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#1-understanding-the-glue-resources-provided-cloudformation-resources","text":"<<<<< write the right stuff ----------[SETTING UP GLUE DATABASE, ALL CRAWLERS AND CLASSIFIER FOR SUBSEQUENT LABS]--------- <<<<<<","title":"1. Understanding the Glue Resources Provided (CloudFormation Resources)"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#2-testing-and-running-pre-created-glue-resources-glue-connection-glue-crawler-for-mysql-rds","text":"<<<<< write the right stuff ---> TEST MYSQL CONNECTION IN GLUE CONSOLE ---> RUN THE ALREADY CREATED MYSQL-RDS-CRAWLER [3-5 MINUTES] - (THIS CRAWLER AND DATABASE IS ALREADY CREATED) -------> 2 TABLES GETS CREATED <<<<< write the right stuff","title":"2. Testing and running pre-created Glue Resources (Glue Connection &amp; Glue Crawler for MySQL RDS)"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#3-creating-new-glue-resources-new-crawler-classifier","text":"<<<<< write the right stuff [WHILE ABOVE CRAWLER IS RUNNING, DO THE FOLLOWING TWO PIECES FOR THE STREAMING CRAWL (FOLLOWING LAB)] ---> [CREATE A CLASSIFIER] CSV Comma (,) Double-quote (\") Has headings c_full_name,c_email_address,total_clicks ---->[CREATE A CRAWLER TO READ FROM THE PATH OF THE STREAMING JOB - USE A CUSTOM CSV CLASSIFIER FOR THAT!!!] --> Name: crawl_streammed_data --> Classifier: CSV --> Include path: [In Cloud9] - echo \"s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/gluestreaming/total_clicks/\" --> Sample Size: 1 --> Exclude Pattern: **00001 --> Database: glue-ttt-demo-db --> Grouping Behavior: Create a single schema for each S3 path [check] !!!!DO NOT RUN THIS CRAWLER YET!!!! <<<<< write the right stuff","title":"3. Creating new Glue Resources (New Crawler &amp; Classifier)"},{"location":"03-StreamingLab/03-streaming-lab/","text":"GLUE (STUDIO) STREAMING <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) \u00b6 <<<<< write the right stuff [STREAMING DEMO PART] [STREAM IN KINESIS IS PART OF THE TEMPLATE (ADD THE TABLE) -----> Use etl-ttt-demo-stream KINESIS DATA STREAM (ALREADY CREATED) [DONT NEED TO BELOW TABLE FOR THE STREAMING WEB_PAGE - BUILT IT IN CLOUD FORMATION] [TABLE SHOULD LOOK LIKE THE FOLLOWING] mysql -h ${mysqlendpoint} -u admin -pVitor123 -Dtpcds -e \"DESCRIBE web_page\" Field Type Null wp_web_page_sk int (PK) NO wp_web_page_id char(16) NO wp_rec_start_date date YES wp_rec_end_date date YES wp_creation_date_sk int YES wp_access_date_sk int YES wp_autogen_flag char(1) YES wp_customer_sk int YES wp_url varchar(100) YES wp_type char(50) YES wp_char_count int YES wp_link_count int YES wp_image_count int YES wp_max_ad_count int YES KINESIS INGESTION SCRIPT [GET THE SCRIPT FROM BUCKET] cd ~/environment/ttt-demo/ aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py [SCRIPT SHOULD LOOK LIKE THIS] import csv import json import boto3 import time import string import random def generate ( stream_name , kinesis_client ): with open ( \"/tmp/dsd/csv_tables/web_page.csv\" , encoding = 'utf-8' ) as csvf : csvReader = csv . DictReader ( csvf ) for rows in csvReader : partitionaKey = '' . join ( random . choices ( string . ascii_uppercase + string . digits , k = 20 )) jsonMsg = json . dumps ( rows ) kinesis_client . put_record ( StreamName = stream_name , Data = jsonMsg , PartitionKey = partitionaKey ) print ( jsonMsg ) time . sleep ( 0.2 ) if __name__ == '__main__' : generate ( 'etl-ttt-demo-stream' , boto3 . client ( 'kinesis' )) [ end of script ] <<<<<< 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) \u00b6 <<<<< write the right stuff [CREATE A DUMMY GLUE STUDIO JOB TO TEST THE JOIN BETWEEN RDS CUSTOMER AND WEB_PAGE CSV FILE IN S3 - DUMMY DATA TO TEST THE PREVIEW IN GLE STUDIO] ---> give the job a name ---> set the job details (workers, role, retries, etc) and save it. [SHOW THE CONNECTION IN GLUE AND SHOW CUSTOM CONNECTOR - SEE WHAT'S BETTER - MENTION MARKETPLACE CONNECTORS] -> Navigate to WebPage S3 source: s3://${BUCKET_NAME}/etl-ttt-demo/csv_tables/web_page.csv -> Customer RDS source: glue-ttt-demo-db/rds_table_tpcds_customer [ADD A JOIN TRANSFORM WITH BOTH SOURCES AS PARENTS AND USE BELOW QUERY TO JOIN RDS TO KINESIS STREAM - FROM SHIV'S DEMO] select CONCAT ( cust . c_first_name , ' ' , cust . c_last_name ) as full_name , cust . c_email_address , count ( webpage . wp_char_count ) as total_clicks from cust left join webpage on cust . c_customer_sk = webpage . wp_customer_sk where 1 = 2 --remove it after building the output group by 1 , 2 order by total_clicks desc [PREVIEW THE QUERY WITH where 1=2 FIRST TO MAKE PREVIEWING FASTER AND AVOID ISSUES] [ONCE PREVIEW IS COMPLETED - APPLY THE OUTPUT SCHEMA FROM DATAPREVIEW - SQL JOIN NODE!] -> Appply Mapping: Do it after previewing data with Use Output Previewed button ---> full_name to c_full_name [Target Node] Format: CSV S3 Target Location (in cloud9): echo \"s3:// $BUCKET_NAME /etl-ttt-demo/output/gluestreaming/total_clicks/\" [!!! SAVE IT BUT DON'T RUN IT, ALL YOU NEED IS THE PREVIEW TO CONFIRM THE JOIN IS WORKING!!!] <<<<< write the right stuff 3. Creating the Glue Streaming Job (Cloning Jobs!) \u00b6 <<<<< write the right stuff [CLONE THE DUMMY JOB AND REPLACE THE CSV WEB_PAGE SOURCE BY THE KINESIS WEB_PAGE STREAMING TABLE] !!!! RUN THE JOB BUT DON'T SEND ANY STREAMING DATA YET!!!! (SETUP EVENT BRIDGE LAB FIRST !!!!!! <<<<< write the right stuff","title":"03 streaming lab"},{"location":"03-StreamingLab/03-streaming-lab/#1-understanding-the-streaming-resources-provided-cloudformation-scripts","text":"<<<<< write the right stuff [STREAMING DEMO PART] [STREAM IN KINESIS IS PART OF THE TEMPLATE (ADD THE TABLE) -----> Use etl-ttt-demo-stream KINESIS DATA STREAM (ALREADY CREATED) [DONT NEED TO BELOW TABLE FOR THE STREAMING WEB_PAGE - BUILT IT IN CLOUD FORMATION] [TABLE SHOULD LOOK LIKE THE FOLLOWING] mysql -h ${mysqlendpoint} -u admin -pVitor123 -Dtpcds -e \"DESCRIBE web_page\" Field Type Null wp_web_page_sk int (PK) NO wp_web_page_id char(16) NO wp_rec_start_date date YES wp_rec_end_date date YES wp_creation_date_sk int YES wp_access_date_sk int YES wp_autogen_flag char(1) YES wp_customer_sk int YES wp_url varchar(100) YES wp_type char(50) YES wp_char_count int YES wp_link_count int YES wp_image_count int YES wp_max_ad_count int YES KINESIS INGESTION SCRIPT [GET THE SCRIPT FROM BUCKET] cd ~/environment/ttt-demo/ aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py [SCRIPT SHOULD LOOK LIKE THIS] import csv import json import boto3 import time import string import random def generate ( stream_name , kinesis_client ): with open ( \"/tmp/dsd/csv_tables/web_page.csv\" , encoding = 'utf-8' ) as csvf : csvReader = csv . DictReader ( csvf ) for rows in csvReader : partitionaKey = '' . join ( random . choices ( string . ascii_uppercase + string . digits , k = 20 )) jsonMsg = json . dumps ( rows ) kinesis_client . put_record ( StreamName = stream_name , Data = jsonMsg , PartitionKey = partitionaKey ) print ( jsonMsg ) time . sleep ( 0.2 ) if __name__ == '__main__' : generate ( 'etl-ttt-demo-stream' , boto3 . client ( 'kinesis' )) [ end of script ] <<<<<<","title":"1. Understanding the Streaming Resources Provided (CloudFormation &amp; Scripts)"},{"location":"03-StreamingLab/03-streaming-lab/#2-validating-streaming-job-logic-and-data-glue-studio-dummy-job","text":"<<<<< write the right stuff [CREATE A DUMMY GLUE STUDIO JOB TO TEST THE JOIN BETWEEN RDS CUSTOMER AND WEB_PAGE CSV FILE IN S3 - DUMMY DATA TO TEST THE PREVIEW IN GLE STUDIO] ---> give the job a name ---> set the job details (workers, role, retries, etc) and save it. [SHOW THE CONNECTION IN GLUE AND SHOW CUSTOM CONNECTOR - SEE WHAT'S BETTER - MENTION MARKETPLACE CONNECTORS] -> Navigate to WebPage S3 source: s3://${BUCKET_NAME}/etl-ttt-demo/csv_tables/web_page.csv -> Customer RDS source: glue-ttt-demo-db/rds_table_tpcds_customer [ADD A JOIN TRANSFORM WITH BOTH SOURCES AS PARENTS AND USE BELOW QUERY TO JOIN RDS TO KINESIS STREAM - FROM SHIV'S DEMO] select CONCAT ( cust . c_first_name , ' ' , cust . c_last_name ) as full_name , cust . c_email_address , count ( webpage . wp_char_count ) as total_clicks from cust left join webpage on cust . c_customer_sk = webpage . wp_customer_sk where 1 = 2 --remove it after building the output group by 1 , 2 order by total_clicks desc [PREVIEW THE QUERY WITH where 1=2 FIRST TO MAKE PREVIEWING FASTER AND AVOID ISSUES] [ONCE PREVIEW IS COMPLETED - APPLY THE OUTPUT SCHEMA FROM DATAPREVIEW - SQL JOIN NODE!] -> Appply Mapping: Do it after previewing data with Use Output Previewed button ---> full_name to c_full_name [Target Node] Format: CSV S3 Target Location (in cloud9): echo \"s3:// $BUCKET_NAME /etl-ttt-demo/output/gluestreaming/total_clicks/\" [!!! SAVE IT BUT DON'T RUN IT, ALL YOU NEED IS THE PREVIEW TO CONFIRM THE JOIN IS WORKING!!!] <<<<< write the right stuff","title":"2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job)"},{"location":"03-StreamingLab/03-streaming-lab/#3-creating-the-glue-streaming-job-cloning-jobs","text":"<<<<< write the right stuff [CLONE THE DUMMY JOB AND REPLACE THE CSV WEB_PAGE SOURCE BY THE KINESIS WEB_PAGE STREAMING TABLE] !!!! RUN THE JOB BUT DON'T SEND ANY STREAMING DATA YET!!!! (SETUP EVENT BRIDGE LAB FIRST !!!!!! <<<<< write the right stuff","title":"3. Creating the Glue Streaming Job (Cloning Jobs!)"},{"location":"04-OrchestrationLab/04orchestration-lab/","text":"ORCHESTRATION & DATA ANALYSIS <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Understanding the Orchestration Flow \u00b6 <<<<< write the right stuff EVENT BRIDGE TRIGGERS A WORKFLOW AS SOON AS STREAMING DATA LANDS IN S3 - WORKFLOW CRAWLS THE DATA --> MAKE SURE THE TRAIL HAS THE FOLLOWING PATH CONFIGURED: Bucket name: etl-ttt-demo-171714944327-ca-central-1 Prefix: /etl-ttt-demo/output/gluestreaming/total_clicks/ <<<<< write the right stuff 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) \u00b6 <<<<< write the right stuff --- create the workflow first ---- aws glue create-workflow --name etl-ttt-event-driven-workflow --- then the trigger ---- aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl_streammed_data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" <<<<< write the right stuff 3. Creating Event Bridge Rule and Target (via CLI) \u00b6 <<<<< write the right stuff --- next the event rule ---- aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" --- finally the event rule target ---- aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"glueworkflow-totalclicks\",\"Arn\"=\"arn:aws:glue: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } <<<<< write the right stuff 4. Triggering Orchestration & Following The Flow \u00b6 <<<<< write the right stuff [CHECK THE KINESIS SCRIPT ONCE AGAIN] cd ~/environment/ttt-demo/ cat PutRecord_Kinesis.py [SCRIPT SHOULD LOOK LIKE THIS] import csv import json import boto3 import time import string import random def generate ( stream_name , kinesis_client ): with open ( \"/tmp/dsd/csv_tables/web_page.csv\" , encoding = 'utf-8' ) as csvf : csvReader = csv . DictReader ( csvf ) for rows in csvReader : partitionaKey = '' . join ( random . choices ( string . ascii_uppercase + string . digits , k = 20 )) jsonMsg = json . dumps ( rows ) kinesis_client . put_record ( StreamName = stream_name , Data = jsonMsg , PartitionKey = partitionaKey ) print ( jsonMsg ) time . sleep ( 0.2 ) if __name__ == '__main__' : generate ( 'etl-ttt-demo-stream' , boto3 . client ( 'kinesis' )) [ end of script ] [ ONCE EVENT BRIDGE IS FULLY SETUP WITH TARGET AND ALL ... ] [ START STREAMING DATA BY RUNNIG THE ABOVE SCRIPT WITH THE FOLLOWING COMMAND IN CLOUD9 ] [ WAIT UP TO 1 OR 2 MINUTES ] [START THE SCRIPT] python PutRecord_Kinesis.py [OBSERVE THE WORKFLOW GETTING TRIGGER AS THE FIRST FILES ARRIVE IN S3 FROM THEM STREAMING JOB] [OBSERVE THAT WORKFLOW TRIGGERS THE CRAWLER WHICH CREATES THE TABLE (total_clicks) FOR FOLLOWING ANALYSIS LAB] [DELETE THE WORKFLOW] [ maybe try cli to EDIT THE TRIGGER TO 100 SO IT STOP RUNNING AT EVERY BATCH OF STREAMING DATA] <<<<< write the right stuff 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog \u00b6 <<<<< write the right stuff [ADD DEFAULT BUCKET FOR ATHENA: /athena-output/] [RUN THE FOLLOWING QUERY] SELECT * FROM \"AwsDataCatalog\" . \"glue-ttt-demo-db\" . \"total_clicks\" order by 3 desc limit 10 ; [Creating Views on Top of Cataloged Tables] [CREATE A VIEW ON TOP OF THE CREATED TABLE TO BROWSE THE TOP 5 CUSTOMERS CLICKING ON THE WEBSITE] CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\" ( CAST ( \"total_clicks\" AS integer )) total_clicks FROM \"total_clicks\" GROUP BY 1 , 2 ORDER BY 3 DESC [QUERY THE VIEW WITH BELOW QUERY] SELECT * FROM \"glue-ttt-demo-db\" . \"tpc_customer_inter\" order by 3 desc limit 10 ; [ADD MORE DATA TO THE STREAM AND KEEP CHECKING WITH ABOVE QUERY] - EVERY 30-60 SECONDS [IF YEAR, MONTH, DAY OR HOUR CHANGES NEED TO UPDATE PARTITIONS BY RUNNING CRAWLER AGAIN] <<<<< write the right stuff","title":"04orchestration lab"},{"location":"04-OrchestrationLab/04orchestration-lab/#1-understanding-the-orchestration-flow","text":"<<<<< write the right stuff EVENT BRIDGE TRIGGERS A WORKFLOW AS SOON AS STREAMING DATA LANDS IN S3 - WORKFLOW CRAWLS THE DATA --> MAKE SURE THE TRAIL HAS THE FOLLOWING PATH CONFIGURED: Bucket name: etl-ttt-demo-171714944327-ca-central-1 Prefix: /etl-ttt-demo/output/gluestreaming/total_clicks/ <<<<< write the right stuff","title":"1. Understanding the Orchestration Flow"},{"location":"04-OrchestrationLab/04orchestration-lab/#2-creating-glue-workflow-and-glue-event-based-trigger-via-cli","text":"<<<<< write the right stuff --- create the workflow first ---- aws glue create-workflow --name etl-ttt-event-driven-workflow --- then the trigger ---- aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl_streammed_data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" <<<<< write the right stuff","title":"2. Creating Glue Workflow and Glue Event Based Trigger (via CLI)"},{"location":"04-OrchestrationLab/04orchestration-lab/#3-creating-event-bridge-rule-and-target-via-cli","text":"<<<<< write the right stuff --- next the event rule ---- aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" --- finally the event rule target ---- aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"glueworkflow-totalclicks\",\"Arn\"=\"arn:aws:glue: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } <<<<< write the right stuff","title":"3. Creating Event Bridge Rule and Target (via CLI)"},{"location":"04-OrchestrationLab/04orchestration-lab/#4-triggering-orchestration-following-the-flow","text":"<<<<< write the right stuff [CHECK THE KINESIS SCRIPT ONCE AGAIN] cd ~/environment/ttt-demo/ cat PutRecord_Kinesis.py [SCRIPT SHOULD LOOK LIKE THIS] import csv import json import boto3 import time import string import random def generate ( stream_name , kinesis_client ): with open ( \"/tmp/dsd/csv_tables/web_page.csv\" , encoding = 'utf-8' ) as csvf : csvReader = csv . DictReader ( csvf ) for rows in csvReader : partitionaKey = '' . join ( random . choices ( string . ascii_uppercase + string . digits , k = 20 )) jsonMsg = json . dumps ( rows ) kinesis_client . put_record ( StreamName = stream_name , Data = jsonMsg , PartitionKey = partitionaKey ) print ( jsonMsg ) time . sleep ( 0.2 ) if __name__ == '__main__' : generate ( 'etl-ttt-demo-stream' , boto3 . client ( 'kinesis' )) [ end of script ] [ ONCE EVENT BRIDGE IS FULLY SETUP WITH TARGET AND ALL ... ] [ START STREAMING DATA BY RUNNIG THE ABOVE SCRIPT WITH THE FOLLOWING COMMAND IN CLOUD9 ] [ WAIT UP TO 1 OR 2 MINUTES ] [START THE SCRIPT] python PutRecord_Kinesis.py [OBSERVE THE WORKFLOW GETTING TRIGGER AS THE FIRST FILES ARRIVE IN S3 FROM THEM STREAMING JOB] [OBSERVE THAT WORKFLOW TRIGGERS THE CRAWLER WHICH CREATES THE TABLE (total_clicks) FOR FOLLOWING ANALYSIS LAB] [DELETE THE WORKFLOW] [ maybe try cli to EDIT THE TRIGGER TO 100 SO IT STOP RUNNING AT EVERY BATCH OF STREAMING DATA] <<<<< write the right stuff","title":"4. Triggering Orchestration &amp; Following The Flow"},{"location":"04-OrchestrationLab/04orchestration-lab/#5-exploring-and-analyzing-tables-data-cataloged-in-glue-data-catalog","text":"<<<<< write the right stuff [ADD DEFAULT BUCKET FOR ATHENA: /athena-output/] [RUN THE FOLLOWING QUERY] SELECT * FROM \"AwsDataCatalog\" . \"glue-ttt-demo-db\" . \"total_clicks\" order by 3 desc limit 10 ; [Creating Views on Top of Cataloged Tables] [CREATE A VIEW ON TOP OF THE CREATED TABLE TO BROWSE THE TOP 5 CUSTOMERS CLICKING ON THE WEBSITE] CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\" ( CAST ( \"total_clicks\" AS integer )) total_clicks FROM \"total_clicks\" GROUP BY 1 , 2 ORDER BY 3 DESC [QUERY THE VIEW WITH BELOW QUERY] SELECT * FROM \"glue-ttt-demo-db\" . \"tpc_customer_inter\" order by 3 desc limit 10 ; [ADD MORE DATA TO THE STREAM AND KEEP CHECKING WITH ABOVE QUERY] - EVERY 30-60 SECONDS [IF YEAR, MONTH, DAY OR HOUR CHANGES NEED TO UPDATE PARTITIONS BY RUNNING CRAWLER AGAIN] <<<<< write the right stuff","title":"5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/","text":"MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOK <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 0. (Pre Steps) - Understading & Setting up the Resources for ML Lab \u00b6 <<<<< write the right stuff [MACHINE LEARNING TRANSFORM LAB] DESCRIPTION: CSV with duplicated customers but with mkdir -p /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling /tmp/dsd/csv_tables/ml-lab/ml-customer-full /tmp/dsd/csv_tables/ml-lab/ml-labeling-file aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/sample_topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling/sample_topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/full-topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/topcuustomer-labeling-file.csv /tmp/dsd/csv_tables/ml-lab/ml-labeling-file/topcuustomer-labeling-file.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ml-lab s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ml-lab/ <<<<< write the right stuff 1. Creating and Training the Glue FindMatches ML Transform \u00b6 Run the ML- Crawler (add this one to the CFN template) Use the sample file to generate labeling file (in S3) Download the generated labeling file and show how to add the labels to similar records (Just show 2 or 3 records to demonstrate) Use the already-filled labeling file to train the ML FindMatches transform (get it from S3) [Don't need to wait anything to complete, just show it. Then use the pre-created ones] 2. Testing FindMatches Transform with Glue Studio Notebook \u00b6 <<<<< write the right stuff Once the FindMatches transform is trained, Open Glue studio and create a new Glue Studio Notebook. - Name: ml-lab-notebook-job - IAM Role: Glue Service Role - etl-ttt-demo Write the following Spark Code into 4 differente cells Cell 1 [replace the existing Cell 1]: % glue_version 2.0 % idle_timeout 60 import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglueml.transforms import FindMatches sc = SparkContext . getOrCreate () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) Cell 2: datasource0 = glueContext . create_dynamic_frame . from_catalog ( database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"datasource0\" ) resolvechoice1 = ResolveChoice . apply ( frame = datasource0 , choice = \"MATCH_CATALOG\" , database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"resolvechoice1\" ) resolvechoice1 . printSchema () Cell 3: findmatches2 = FindMatches . apply ( frame = resolvechoice1 , transformId = \"tfm-733f49ce1a5f98c4e2b8a9604ae4427368b08b07\" , transformation_ctx = \"findmatches2\" ) resolvechoice3 = ResolveChoice . apply ( frame = findmatches2 , choice = \"make_struct\" , transformation_ctx = \"resolvechoice3\" ) dropnullfield4 = DropNullFields . apply ( frame = resolvechoice3 , transformation_ctx = \"dropnullfield4\" ) df1 = dropnullfield4 . toDF () print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) Cell 4: custidlist = [ \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAJLKGBAAA\" , \"AAAAAAAAJLKGBAAA\" , \"AAAAAAAAPLNDAAAA\" , \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAPLNDAAAA\" , \"BAAAAAAANDCFBAAA\" , \"AAAAAAAANKFAAAAA\" , \"BAAAAAAANKFAAAAA\" , \"AAAAAAAADFHGAAAA\" , \"BAAAAAAADFHGAAAA\" ] cols = [ 'c_customer_id' , 'c_current_addr_sk' , 'c_salutation' , 'c_first_name' , 'c_last_name' , 'c_birth_day' , 'c_birth_month' , 'c_birth_year' , 'c_birth_country' , 'c_login' , 'c_email_address' , 'match_id' ] print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) df1 . filter ( df1 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) Cell 5: df2 = df1 . drop_duplicates ( subset = [ 'match_id' ]) print ( \"Distinct count of matching IDs : \" + str ( df2 . count ())) df2 . filter ( df2 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) ---> Wait for the last cell to finish and output the results. <<<<< write the right stuff 3. Validating and Deploying the FindMatches Glue Job \u00b6 <<<<< write the right stuff ---> validate the results: --> Reference this blog: Reference This Blog: http://blog.zenof.ai/machine-learning-based-fuzzy-matching-using-aws-glue-ml-transforms/ \"However, the Find matches transform adds another column named match_id to identify matching records in the output. Rows with the same match_id are considered matching records.\" Once done explaining the Matching IDs logic, add a new cell to the notebook to sink the very final dataframe to S3 but DON'T RUN this cell!!! [DON'T RUN THIS CELL] Cell 6: new_df = df2 . select ( * cols ) . orderBy ( \"match_id\" ) new_df . coalesce ( 1 ) . write . option ( \"header\" , \"true\" ) . mode ( \"overwrite\" ) . csv ( \"s3://etl-ttt-demo-589541849248-us-east-2/etl-ttt-demo/output/ml-lab/topcustomers-dedup/\" ) [DON'T RUN THIS CELL] JUST SAVE THE JOB TO DEPLOY IT PROPERLY AND MOVE TO THE NEXT LAB] <<<<< write the right stuff","title":"05 gstudio notebook lab"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#0-pre-steps-understading-setting-up-the-resources-for-ml-lab","text":"<<<<< write the right stuff [MACHINE LEARNING TRANSFORM LAB] DESCRIPTION: CSV with duplicated customers but with mkdir -p /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling /tmp/dsd/csv_tables/ml-lab/ml-customer-full /tmp/dsd/csv_tables/ml-lab/ml-labeling-file aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/sample_topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling/sample_topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/full-topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/topcuustomer-labeling-file.csv /tmp/dsd/csv_tables/ml-lab/ml-labeling-file/topcuustomer-labeling-file.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ml-lab s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ml-lab/ <<<<< write the right stuff","title":"0. (Pre Steps) - Understading &amp; Setting up the Resources for ML Lab"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#1-creating-and-training-the-glue-findmatches-ml-transform","text":"Run the ML- Crawler (add this one to the CFN template) Use the sample file to generate labeling file (in S3) Download the generated labeling file and show how to add the labels to similar records (Just show 2 or 3 records to demonstrate) Use the already-filled labeling file to train the ML FindMatches transform (get it from S3) [Don't need to wait anything to complete, just show it. Then use the pre-created ones]","title":"1. Creating and Training the Glue FindMatches ML Transform"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#2-testing-findmatches-transform-with-glue-studio-notebook","text":"<<<<< write the right stuff Once the FindMatches transform is trained, Open Glue studio and create a new Glue Studio Notebook. - Name: ml-lab-notebook-job - IAM Role: Glue Service Role - etl-ttt-demo Write the following Spark Code into 4 differente cells Cell 1 [replace the existing Cell 1]: % glue_version 2.0 % idle_timeout 60 import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglueml.transforms import FindMatches sc = SparkContext . getOrCreate () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) Cell 2: datasource0 = glueContext . create_dynamic_frame . from_catalog ( database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"datasource0\" ) resolvechoice1 = ResolveChoice . apply ( frame = datasource0 , choice = \"MATCH_CATALOG\" , database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"resolvechoice1\" ) resolvechoice1 . printSchema () Cell 3: findmatches2 = FindMatches . apply ( frame = resolvechoice1 , transformId = \"tfm-733f49ce1a5f98c4e2b8a9604ae4427368b08b07\" , transformation_ctx = \"findmatches2\" ) resolvechoice3 = ResolveChoice . apply ( frame = findmatches2 , choice = \"make_struct\" , transformation_ctx = \"resolvechoice3\" ) dropnullfield4 = DropNullFields . apply ( frame = resolvechoice3 , transformation_ctx = \"dropnullfield4\" ) df1 = dropnullfield4 . toDF () print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) Cell 4: custidlist = [ \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAJLKGBAAA\" , \"AAAAAAAAJLKGBAAA\" , \"AAAAAAAAPLNDAAAA\" , \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAPLNDAAAA\" , \"BAAAAAAANDCFBAAA\" , \"AAAAAAAANKFAAAAA\" , \"BAAAAAAANKFAAAAA\" , \"AAAAAAAADFHGAAAA\" , \"BAAAAAAADFHGAAAA\" ] cols = [ 'c_customer_id' , 'c_current_addr_sk' , 'c_salutation' , 'c_first_name' , 'c_last_name' , 'c_birth_day' , 'c_birth_month' , 'c_birth_year' , 'c_birth_country' , 'c_login' , 'c_email_address' , 'match_id' ] print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) df1 . filter ( df1 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) Cell 5: df2 = df1 . drop_duplicates ( subset = [ 'match_id' ]) print ( \"Distinct count of matching IDs : \" + str ( df2 . count ())) df2 . filter ( df2 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) ---> Wait for the last cell to finish and output the results. <<<<< write the right stuff","title":"2. Testing FindMatches Transform with Glue Studio Notebook"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#3-validating-and-deploying-the-findmatches-glue-job","text":"<<<<< write the right stuff ---> validate the results: --> Reference this blog: Reference This Blog: http://blog.zenof.ai/machine-learning-based-fuzzy-matching-using-aws-glue-ml-transforms/ \"However, the Find matches transform adds another column named match_id to identify matching records in the output. Rows with the same match_id are considered matching records.\" Once done explaining the Matching IDs logic, add a new cell to the notebook to sink the very final dataframe to S3 but DON'T RUN this cell!!! [DON'T RUN THIS CELL] Cell 6: new_df = df2 . select ( * cols ) . orderBy ( \"match_id\" ) new_df . coalesce ( 1 ) . write . option ( \"header\" , \"true\" ) . mode ( \"overwrite\" ) . csv ( \"s3://etl-ttt-demo-589541849248-us-east-2/etl-ttt-demo/output/ml-lab/topcustomers-dedup/\" ) [DON'T RUN THIS CELL] JUST SAVE THE JOB TO DEPLOY IT PROPERLY AND MOVE TO THE NEXT LAB] <<<<< write the right stuff","title":"3. Validating and Deploying the FindMatches Glue Job"},{"location":"06-StepFunctionsLab/06stepfunctions-lab/","text":"WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS <<<<< write the right stuff [NEED TO CREATE A FOLDER NAMED (Cloud9): .../etl-ttt-demo/data_analytics_team_folder/ [SIMULATE A FILE BEING SHARED FROM THE ANALYTICS TEAM] [THIS FILE WILL TRIGGER THE STEP FUNCTION WORKFLOW WHICH CONSEQUENTLY WILL RUN THE FINDMATCHES ML VALIDATION JOB TO CLEAN UP THE DUPLICAIONS. AFTER THAT, STEP FUNCTION WORKFLOW WILL START THE CRAWLER TO CATALOG THE VALIDATED DATA WHICH WILL THEN BE SERVED AS THE SOURCE DATASET FOR THE ANALYTICS TEAM TO USE IN THE DATABREW LAB] [NEED TO CREATE A CRAWLER TO CRAWL THE GENERATED FILE - STEP FUNCTION WILL TRIGER IT - ADD THIS CRAWLER TO THE CFN TOO] <<<<< write the right stuff 1. Creating the Step Function Workflow \u00b6 <<<<< write the right stuff Write down the steps to create the workflow... First is a Glue Job: StateName: FindMatch Job JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To remove duplicates in the dataset using FIndMatches ML Transform\" Then the Glue Crawler: StateName: Crawl Dedup Data JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To crawl the data once the FindMatches job conclude the dedup higiene process\" STEP FUNCTION WORKFLOW - JSON GENERATED CODE SNIPPET { \"Comment\" : \"A description of my state machine\" , \"StartAt\" : \"Find Match Job\" , \"States\" : { \"Find Match Job\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::glue:startJobRun.sync\" , \"Parameters\" : { \"JobName\" : \"ml-lab-notebook-job\" }, \"Comment\" : \"To remove duplicates in the dataset using FIndMatches ML Transform\" , \"Next\" : \"Crawl Dedup Data\" }, \"Crawl Dedup Data\" : { \"Type\" : \"Task\" , \"End\" : true , \"Parameters\" : { \"Name\" : \"ml_final_crawler\" }, \"Resource\" : \"arn:aws:states:::aws-sdk:glue:startCrawler\" , \"Comment\" : \"To crawl the data after FindMatches dedup higiene process\" } } } State Machine Name: findmatches-ML-dedup-workflow Existing Role: AWSStepFunctionRole-etl-ttt-demo <<<<< write the right stuff 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI) \u00b6 <<<<< write the right stuff --- first the event rule ---- aws events put-rule \\ --name \"find-matches-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/data_analytics_team_folder/\\\"}] } \\ } \\ }\" --- then the event rule target ---- aws events put-targets \\ --rule find-matches-rule \\ --targets \"Id\"=\"stepfunction-workflow\",\"Arn\"=\"arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:findmatches-ML-dedup-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } [SIMMULATE THE FILE BEING UPLOADED BY A DIFFERENT TEAM TO CENTRAL BUCKET FOR FURTHER PROCESSING] aws s3 cp /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv s3://$BUCKET_NAME/etl-ttt-demo/data_analytics_team_folder/ <<<<< write the right stuff","title":"06stepfunctions lab"},{"location":"06-StepFunctionsLab/06stepfunctions-lab/#1-creating-the-step-function-workflow","text":"<<<<< write the right stuff Write down the steps to create the workflow... First is a Glue Job: StateName: FindMatch Job JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To remove duplicates in the dataset using FIndMatches ML Transform\" Then the Glue Crawler: StateName: Crawl Dedup Data JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To crawl the data once the FindMatches job conclude the dedup higiene process\" STEP FUNCTION WORKFLOW - JSON GENERATED CODE SNIPPET { \"Comment\" : \"A description of my state machine\" , \"StartAt\" : \"Find Match Job\" , \"States\" : { \"Find Match Job\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::glue:startJobRun.sync\" , \"Parameters\" : { \"JobName\" : \"ml-lab-notebook-job\" }, \"Comment\" : \"To remove duplicates in the dataset using FIndMatches ML Transform\" , \"Next\" : \"Crawl Dedup Data\" }, \"Crawl Dedup Data\" : { \"Type\" : \"Task\" , \"End\" : true , \"Parameters\" : { \"Name\" : \"ml_final_crawler\" }, \"Resource\" : \"arn:aws:states:::aws-sdk:glue:startCrawler\" , \"Comment\" : \"To crawl the data after FindMatches dedup higiene process\" } } } State Machine Name: findmatches-ML-dedup-workflow Existing Role: AWSStepFunctionRole-etl-ttt-demo <<<<< write the right stuff","title":"1. Creating the Step Function Workflow"},{"location":"06-StepFunctionsLab/06stepfunctions-lab/#2-creating-an-eventbridge-rule-target-for-the-step-function-workflow-via-cli","text":"<<<<< write the right stuff --- first the event rule ---- aws events put-rule \\ --name \"find-matches-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/data_analytics_team_folder/\\\"}] } \\ } \\ }\" --- then the event rule target ---- aws events put-targets \\ --rule find-matches-rule \\ --targets \"Id\"=\"stepfunction-workflow\",\"Arn\"=\"arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:findmatches-ML-dedup-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } [SIMMULATE THE FILE BEING UPLOADED BY A DIFFERENT TEAM TO CENTRAL BUCKET FOR FURTHER PROCESSING] aws s3 cp /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv s3://$BUCKET_NAME/etl-ttt-demo/data_analytics_team_folder/ <<<<< write the right stuff","title":"2. Creating an EventBridge Rule &amp; Target for the Step Function Workflow (via CLI)"},{"location":"07-DataBrewLab/07-databrew-lab/","text":"DATA QUALITY & DATA PREPARATION WITH AWS GLUE DATABREW <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Creating Datasets & Profiling the Data (with Quality Rules) \u00b6 <<<<< write the right stuff [CREATE A DATASET FROM THE SHIVS THIRD DEMO (INCOME BAND AND CUSTOMER TABLE] ---> Create TopCustomer Dataset from Data Catalog ---> (Cataloged via crawler + step functions in the previous lab after removing duplicates with ML Transform job) ---> Create Income Band from RDS connection (delete income band from crawler - check template and lab instruction) [PROFILE THE CUSTOMER DATASET WITH DATA QUALITY RULES - 3-5 minutes for profile job to finish] --> Full dataset --> Output Settings: Navigate to -> s3://$BUCKET_NAME/etl-ttt-demo/output/databrew/jobprofiles/ [CREATE RULE SET] ---> TopCustomer-Ruleset [ADD RECOMMENDED QUALITY RULES + ANY OTHER I WANT] Rule Set Name: TopCustomer-Ruleset Associated Dataset: TopCustomers recommended rules: --> duplicates? (The rule will pass if dataset has duplicate rows count <= 1 ) --> missing values less than 2% custom rules: --> The rule will pass if c_email_address has length <= 5 FOR greater than or equal to 100% of rows [PROFILE INCOME_BAND DATASET WITH 1 QUALITY RULE] Full dataset Rule Name: Income Check for Negative Values Rule Summary: The rule will pass if ib_lower_bound , ib_upper_bound has values >= 0 FOR greater than or equal to 100% of rows 2. Working with DataBrew Recipes & Projects \u00b6 <<<<< write the right stuff [CREATE A PROJECT FOR INCOME_BAND DATASET] Project Name: TopCustomers-PerBand ---> show all the transformations available (quick overview) Choose: Join -> Choose Customer Dataset to join with A RIGHT JOIN --> Join by Income_Band Sk and Birthday_day (income band goes from 1-20 only) --> Remove ib_income_band_sk ---> To add the steps: -> Go to Recipes and import the recipe received from data analytics team -> Show all the steps in the recipe and the JSON original recipe. -> Publish the recipe -> Go back to the Project: ---> Import the Recipe into the project ---> Explain step validation (append/overwrite) ---> Reorder the Rename steps (17,18 and 19) ---> Create an Age field using DATE_DIFF only. ---> Drop the c_birth_column, c_login and match_id ---> Save and Publish this as a new recipe. ---> Run the Job and share the results. <<<<< write the right stuff","title":"07 databrew lab"},{"location":"07-DataBrewLab/07-databrew-lab/#1-creating-datasets-profiling-the-data-with-quality-rules","text":"<<<<< write the right stuff [CREATE A DATASET FROM THE SHIVS THIRD DEMO (INCOME BAND AND CUSTOMER TABLE] ---> Create TopCustomer Dataset from Data Catalog ---> (Cataloged via crawler + step functions in the previous lab after removing duplicates with ML Transform job) ---> Create Income Band from RDS connection (delete income band from crawler - check template and lab instruction) [PROFILE THE CUSTOMER DATASET WITH DATA QUALITY RULES - 3-5 minutes for profile job to finish] --> Full dataset --> Output Settings: Navigate to -> s3://$BUCKET_NAME/etl-ttt-demo/output/databrew/jobprofiles/ [CREATE RULE SET] ---> TopCustomer-Ruleset [ADD RECOMMENDED QUALITY RULES + ANY OTHER I WANT] Rule Set Name: TopCustomer-Ruleset Associated Dataset: TopCustomers recommended rules: --> duplicates? (The rule will pass if dataset has duplicate rows count <= 1 ) --> missing values less than 2% custom rules: --> The rule will pass if c_email_address has length <= 5 FOR greater than or equal to 100% of rows [PROFILE INCOME_BAND DATASET WITH 1 QUALITY RULE] Full dataset Rule Name: Income Check for Negative Values Rule Summary: The rule will pass if ib_lower_bound , ib_upper_bound has values >= 0 FOR greater than or equal to 100% of rows","title":"1. Creating Datasets &amp; Profiling the Data (with Quality Rules)"},{"location":"07-DataBrewLab/07-databrew-lab/#2-working-with-databrew-recipes-projects","text":"<<<<< write the right stuff [CREATE A PROJECT FOR INCOME_BAND DATASET] Project Name: TopCustomers-PerBand ---> show all the transformations available (quick overview) Choose: Join -> Choose Customer Dataset to join with A RIGHT JOIN --> Join by Income_Band Sk and Birthday_day (income band goes from 1-20 only) --> Remove ib_income_band_sk ---> To add the steps: -> Go to Recipes and import the recipe received from data analytics team -> Show all the steps in the recipe and the JSON original recipe. -> Publish the recipe -> Go back to the Project: ---> Import the Recipe into the project ---> Explain step validation (append/overwrite) ---> Reorder the Rename steps (17,18 and 19) ---> Create an Age field using DATE_DIFF only. ---> Drop the c_birth_column, c_login and match_id ---> Save and Publish this as a new recipe. ---> Run the Job and share the results. <<<<< write the right stuff","title":"2. Working with DataBrew Recipes &amp; Projects"}]}