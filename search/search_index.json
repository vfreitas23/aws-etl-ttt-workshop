{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ETL Migration & Modernization Train-The-Trainer Workshop - Hands On Practice!!!","title":"Home"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"** Setup **"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"Setup"},{"location":"00-PreSteps/001-intro/","text":"Lab Pre-Steps If you are at an AWS hosted event (such as AWS re:Invent, AWS Summit, Immersions Day, on-site Workshop or other event hosted by AWS employees), you will be provided with a temporary Event Engine AWS account for the workshop. Please follow the AWS Event instructions to access your Event Engine AWS account. If you are running this workshop by yourself, your account must have the ability to create new IAM roles and scope other IAM permissions. Please follow Self Paced Labs instructions to prepare your AWS account.","title":"Intro"},{"location":"00-PreSteps/AWSEvent/002-ee-account-setup/","text":"You will be provided with an AWS account to run this workshop. The temporary account is being created using Event Engine. You will be provided a participant hash key to login to your temporary account. Follow these steps to start using your account: Go to AWS Event Engine Portal Event engine Enter the provided hash in the text box . The button on the bottom right corner changes to Accept Terms & Login . Click on that button to continue . Click on Email One - Time Password ( OTP ) ; You also have option to use your personal Amazon.com uid and password (Note - Not your AWS account credentials) Event engine Provide your email address. Event engine Provide the OTP you have received in your email. !Event engine Click AWS Console on the dashboard. Event engine Take the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab. Event engine Now, you should be on the AWS Console page of your workshop environment.","title":"Account Setup"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/","text":"PREPARING THE CLOUD9 ENVIROMENT We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the [ADD CLOUD9 LINK HERE] AWS Cloud9 Console in your environment and you should see a Cloud9 Enviroment named \"etl-ttt-demo\". Select this enviroment and click on the View details button. Under EC2 Instance, click on Go To Instance link. This will open a new tab on your browser and take you to the AWS EC2 Console. TIP: Leave the AWS Cloud9 Console tab open for now! In the AWS EC2 Console, select the Cloud9 EC2 Instance that starts with \"aws-cloud9-\". Click on the Actions dropdown button, followed by Security, then click on Modify IAM Role! In the next screen's dropdown box select the role that contains \"-AWSEC2InstanceProfileTTT-\" and click Save. TIP: You can close the AWS EC2 Console tab now! Go back to the AWS Cloud9 Console tab and, while still on the Enviroment Details page, click on the Open IDE button to launch your AWS Cloud9 IDE environment. Tip: Now, you can close the AWS Cloud9 Console tab. Allow for the launched AWS Cloud9 Enviroment's setup to finish, then close the Welcome tab and click on the green plus icon (+) to add a New Terminal tab. This will create a new tab and load a command-line terminal. From now on, you will use this terminal window throughout this workshop lab to execute all the AWS CLI commands and scripts. TIP: Keep this tab open! 1. Setting up Cloud9 Environment Variables \u00b6 [EXPLAIN WHAT GETS CREATED IN THIS LAB] During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff [EXPLAIN WHAT THIS COMMAND WILL DO...] BLAH BLAH BLAH .... The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. BLAH BLAH BLAH... <<<<<< 2. Switching Cloud9 Role Credentials \u00b6 [DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials wich is controled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far top right), the Preferences tab will open. There, scroll to AWS Settings and disable the option \"AWS managed temporary credentials. Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the \"AWSEC2InstanceProfileTTT\" you setup earlier in for the Cloud9 EC2 instance. aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output. 3. Setting up Security Required Groups Inbound Rules \u00b6 Next, a new Inbound Rule will be added to the Default Security group for this workshop which is tagged with the name \"etl-ttt-demo\" Security Group. This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the \"etl-ttt-demo\". Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX are the IDs of both, EC2's and DefaultVPC's Security Group. [Optional]: You can go to your VPC Console [add link here] to verify that a new Inbound Role has been added to the \"etl-ttt-demo\", it should have two Inbound Rules now. 4. Installing Required Libraries (Boto3) \u00b6 In this next step, BOTO3 Library will be installed to your Cloud9 enviroment. That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the workshop's bucket: etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"Prepare the Enviroment"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#1-setting-up-cloud9-environment-variables","text":"[EXPLAIN WHAT GETS CREATED IN THIS LAB] During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff [EXPLAIN WHAT THIS COMMAND WILL DO...] BLAH BLAH BLAH .... The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. BLAH BLAH BLAH... <<<<<<","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#2-switching-cloud9-role-credentials","text":"[DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials wich is controled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far top right), the Preferences tab will open. There, scroll to AWS Settings and disable the option \"AWS managed temporary credentials. Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the \"AWSEC2InstanceProfileTTT\" you setup earlier in for the Cloud9 EC2 instance. aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output.","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#3-setting-up-security-required-groups-inbound-rules","text":"Next, a new Inbound Rule will be added to the Default Security group for this workshop which is tagged with the name \"etl-ttt-demo\" Security Group. This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the \"etl-ttt-demo\". Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX are the IDs of both, EC2's and DefaultVPC's Security Group. [Optional]: You can go to your VPC Console [add link here] to verify that a new Inbound Role has been added to the \"etl-ttt-demo\", it should have two Inbound Rules now.","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#4-installing-required-libraries-boto3","text":"In this next step, BOTO3 Library will be installed to your Cloud9 enviroment. That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the workshop's bucket: etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"4. Installing Required Libraries (Boto3)"},{"location":"00-PreSteps/SelfPaced/004-signup-aws/","text":"If you've already signed up for Amazon Web Services (AWS) and have a user with administrator access, you can move on to Deploy CloudFormation Template. If you haven't signed up for AWS account yet, or if you need assistance, follow the following instructions to set up your AWS account for the workshop. Sign Up for AWS When you sign up for Amazon Web Services (AWS), your AWS account is automatically signed up for all services in AWS, including Lake Formation. You are charged only for the services that you use. If you have an AWS account already, skip to the next task. If you don't have an AWS account, use the following procedure to create one. To create an AWS account Open https : // aws . amazon . com / and then choose Create an AWS Account . If you previously signed in to the AWS Management Console using AWS account root user credentials, choose Sign in to a different account. If you previously signed in to the console using IAM credentials, choose Sign-in using root account credentials. Then choose Create a new AWS account. Follow the online instructions . Part of the sign - up procedure involves receiving a phone call and entering a verification code using the phone keypad . Note your AWS account number , because you ' ll need it for the next task.","title":"Sign Up for AWS"},{"location":"00-PreSteps/SelfPaced/005-create-iam-user/","text":"Create IAM User Services in AWS, such as AWS Glue, require that you provide credentials when you access them, so that the service can determine whether you have permission to access its resources. The console requires your password. You can create access keys for your AWS account to access the command line interface or API. However, we don't recommend that you access AWS using the credentials for your AWS account; we recommend that you use AWS Identity and Access Management (IAM) instead. Create an IAM user, and then add the user to an IAM group with administrative permissions or grant this user administrative permissions. You can then access AWS using a special URL and the credentials for the IAM user. If you signed up for AWS but have not created an IAM user for yourself, you can create one using the IAM console. If you aren't familiar with using the console, see Working with the AWS Management Console for an overview. To create an IAM user for yourself and add the user to an Administrators group Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ We strongly recommend that you adhere to the best practice of using the Administrator IAM user below and securely lock away the root user credentials. Sign in as the root user only to perform a few account and service management tasks . In the navigation pane of the console , choose Users , and then choose Add user . For User name , type Administrator . Select the check box next to AWS Management Console access , select Custom password , and then type the new user ' s password in the text box. You can optionally select Require password reset to force the user to create a new password the next time the user signs in. Choose Next : Permissions . On the Set permissions page , choose Add user to group . Choose Create group. In the Create group dialog box , for Group name type Administrators . For Filter policies , select the check box for AWS managed - job function . In the policy list , select the check box for AdministratorAccess . Then choose Create group . Back in the list of groups , select the check box for your new group . Choose Refresh if necessary to see the group in the list . Choose Next : Tags to add metadata to the user by attaching tags as key - value pairs . Choose Next : Review to see the list of group memberships to be added to the new user . When you are ready to proceed , choose Create user . You can use this same process to create more groups and users , and to give your users access to your AWS account resources . To sign in as this new IAM user, sign out of the AWS console, then use the following URL, where your_aws_account_id is your AWS account number without the hyphens (for example, if your AWS account number is 1234-5678-9012, your AWS account ID is 123456789012): https://your_aws_account_id.signin.aws.amazon.com/console/ Enter the IAM user name (not your email address) and password that you just created. When you're signed in, the navigation bar displays \"your_user_name @ your_aws_account_id\". If you don't want the URL for your sign-in page to contain your AWS account ID, you can create an account alias. From the IAM console, choose Dashboard in the navigation pane. From the dashboard, choose Customize and enter an alias such as your company name. To sign in after you create an account alias, use the following URL: https://your_account_alias.signin.aws.amazon.com/console/ To verify the sign-in link for IAM users for your account, open the IAM console and check under IAM users sign-in link on the dashboard.","title":"Create IAM User"},{"location":"00-PreSteps/SelfPaced/006-deploy-cfn/","text":"Deploy CloudFormation Template We will use CloudFormation to set up the AWS environments for the labs. AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources, and provision and manage them in an orderly and predictable fashion. We will create one CloudFormation stack which includes all the resources; we will be using in the following labs. When you finish all labs, please be sure to delete the stack as per the Clean up section to avoid unnecessary usage charges. Click here to launch Workshop Cloudformation Click Next in Create stack screen, accept the default stack name glueworkshop in Specify stack details screen and click Next, Click Next in Configure stack options screen, check the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources with custom names. and click Create stack. You should see a new CloudFormation stack with the name glueworkshop being created. Wait for the status of the stack to be CREATE_COMPLETE before moving on. You can click on the stack name and go to Resource tab to check what AWS resoures were created as part of the CloudFormation stack. You should expect to wait approximately 10 minutes for the stack to move to the CREATE_COMPLETE state. Cloudformation creation","title":"Deploy CloudFormation"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/","text":"PREPARING THE CLOUD9 ENVIROMENT We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the [ADD CLOUD9 LINK HERE] AWS Cloud9 Console in your environment and you should see a Cloud9 Enviroment named \"etl-ttt-demo\". Select this enviroment and click on the View details button. Under EC2 Instance, click on Go To Instance link. This will open a new tab on your browser and take you to the AWS EC2 Console. TIP: Leave the AWS Cloud9 Console tab open for now! In the AWS EC2 Console, select the Cloud9 EC2 Instance that starts with \"aws-cloud9-\". Click on the Actions dropdown button, followed by Security, then click on Modify IAM Role! In the next screen's dropdown box select the role that contains \"-AWSEC2InstanceProfileTTT-\" and click Save. TIP: You can close the AWS EC2 Console tab now! Go back to the AWS Cloud9 Console tab and, while still on the Enviroment Details page, click on the Open IDE button to launch your AWS Cloud9 IDE environment. Tip: Now, you can close the AWS Cloud9 Console tab. Allow for the launched AWS Cloud9 Enviroment's setup to finish, then close the Welcome tab and click on the green plus icon (+) to add a New Terminal tab. This will create a new tab and load a command-line terminal. From now on, you will use this terminal window throughout this workshop lab to execute all the AWS CLI commands and scripts. TIP: Keep this tab open! 1. Setting up Cloud9 Environment Variables \u00b6 [EXPLAIN WHAT GETS CREATED IN THIS LAB] During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff [EXPLAIN WHAT THIS COMMAND WILL DO...] BLAH BLAH BLAH .... The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. BLAH BLAH BLAH... <<<<<< 2. Switching Cloud9 Role Credentials \u00b6 [DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials wich is controled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far top right), the Preferences tab will open. There, scroll to AWS Settings and disable the option \"AWS managed temporary credentials. Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the \"AWSEC2InstanceProfileTTT\" you setup earlier in for the Cloud9 EC2 instance. aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output. 3. Setting up Security Required Groups Inbound Rules \u00b6 Next, a new Inbound Rule will be added to the Default Security group for this workshop which is tagged with the name \"etl-ttt-demo\" Security Group. This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the \"etl-ttt-demo\". Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX are the IDs of both, EC2's and DefaultVPC's Security Group. [Optional]: You can go to your VPC Console [add link here] to verify that a new Inbound Role has been added to the \"etl-ttt-demo\", it should have two Inbound Rules now. 4. Installing Required Libraries (Boto3) \u00b6 In this next step, BOTO3 Library will be installed to your Cloud9 enviroment. That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the workshop's bucket: etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"Prepare the Enviroment"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#1-setting-up-cloud9-environment-variables","text":"[EXPLAIN WHAT GETS CREATED IN THIS LAB] During the workshop environment setup, a S3 bucket is created for storing lab files and CloudTrail logs. Copy following command to Cloud9 terminal window to get your lab S3 bucket name to ${BUCKET_NAME}. Please save the ${BUCKET_NAME} value, you will be using it through out the workshop. [detail more what we are doing below] AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint <<<<< write the right stuff [EXPLAIN WHAT THIS COMMAND WILL DO...] BLAH BLAH BLAH .... The following commands will download and unzip the lab files to your Cloud9 environment, download and zip the 3rd party Python library, upload lab files to your S3 bucket, then copy the COVID-19 dataset to your S3 bucket. We will use data from a public COVID-19 dataset curated by AWS. We will show details about how to access and exchange data on AWS platform Access COVID-19 Dataset. If you are interested in learning more about this COVID-19 dataset, read this AWS blog for more information. The 3rd party Python library is from github and you could find details about the library there. BLAH BLAH BLAH... <<<<<<","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#2-switching-cloud9-role-credentials","text":"[DISABLE AWS CREDENTIALS FROM CLOUD 9 CONFIG] After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials wich is controled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far top right), the Preferences tab will open. There, scroll to AWS Settings and disable the option \"AWS managed temporary credentials. Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the \"AWSEC2InstanceProfileTTT\" you setup earlier in for the Cloud9 EC2 instance. aws sts get - caller - identity aws configure set region $AWS_REGION aws configure get region aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output.","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#3-setting-up-security-required-groups-inbound-rules","text":"Next, a new Inbound Rule will be added to the Default Security group for this workshop which is tagged with the name \"etl-ttt-demo\" Security Group. This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the \"etl-ttt-demo\". Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX are the IDs of both, EC2's and DefaultVPC's Security Group. [Optional]: You can go to your VPC Console [add link here] to verify that a new Inbound Role has been added to the \"etl-ttt-demo\", it should have two Inbound Rules now.","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#4-installing-required-libraries-boto3","text":"In this next step, BOTO3 Library will be installed to your Cloud9 enviroment. That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the workshop's bucket: etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"4. Installing Required Libraries (Boto3)"},{"location":"000-Intro/000-intro/","text":"ETL MIGRATION & MODERNIZATION TRAIN-THE-TRAINER WORKSHOP This workshop contains a set of seven hands-on labs which complementes the Day One of the ETL Migration & Modernization - Train The Trainer Workshop. Participants will build an end-to-end ETL Pipeline that will...[add details of the pipeline...] After completing the labs, the participants will have a high level understand of AWS Glue core capabilities as well as they will be able to demonstrate most of the Glue core capabilities. WORKSHOP PARTS & STEPS Part 0 - PRE STEPS \u00b6 1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3) Part 1 - TPCDS & RDS MySQL \u00b6 1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3 Part 2 - AWS GLUE DISCOVERY COMPONENTS (Databases, Tables, Connections, Crawlers and Classifiers) \u00b6 1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Testing and running pre-created Glue Resources (Connection & MySQL-RDS-Crawler) 3. Creating new Glue Resources (Crawler & Classifier) Part 3 - GLUE (STUDIO) STREAMING \u00b6 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!) Part 4 - ORCHESTRATION & DATA ANALYSIS \u00b6 1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Triggering Orchestration & Following The Flow 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog Part 5 - MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOKS \u00b6 0. (Pre Steps) - Understading & Setting up the Resources for the ML Lab 1. Creating and Training the Glue FindMatches ML Transform 2. Testing FindMatches Transform with Glue Studio Notebook 3. Deploying & Running a FindMatches Glue Job Part 6 - WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS \u00b6 1. Creating the Step Function Workflow 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI) Part 7 - DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW \u00b6 1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Working with DataBrew Recipes & Projects","title":"Introduction"},{"location":"000-Intro/000-intro/#part-0-pre-steps","text":"1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3)","title":"Part 0 - PRE STEPS"},{"location":"000-Intro/000-intro/#part-1-tpcds-rds-mysql","text":"1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3","title":"Part 1 - TPCDS &amp; RDS MySQL"},{"location":"000-Intro/000-intro/#part-2-aws-glue-discovery-components-databases-tables-connections-crawlers-and-classifiers","text":"1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Testing and running pre-created Glue Resources (Connection & MySQL-RDS-Crawler) 3. Creating new Glue Resources (Crawler & Classifier)","title":"Part 2 - AWS GLUE DISCOVERY COMPONENTS (Databases, Tables, Connections, Crawlers and Classifiers)"},{"location":"000-Intro/000-intro/#part-3-glue-studio-streaming","text":"1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!)","title":"Part 3 - GLUE (STUDIO) STREAMING"},{"location":"000-Intro/000-intro/#part-4-orchestration-data-analysis","text":"1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Triggering Orchestration & Following The Flow 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog","title":"Part 4 - ORCHESTRATION &amp; DATA ANALYSIS"},{"location":"000-Intro/000-intro/#part-5-machine-learning-with-glue-glue-studio-notebooks","text":"0. (Pre Steps) - Understading & Setting up the Resources for the ML Lab 1. Creating and Training the Glue FindMatches ML Transform 2. Testing FindMatches Transform with Glue Studio Notebook 3. Deploying & Running a FindMatches Glue Job","title":"Part 5 - MACHINE LEARNING WITH GLUE &amp; GLUE STUDIO NOTEBOOKS"},{"location":"000-Intro/000-intro/#part-6-workflow-orchestration-with-aws-step-functions","text":"1. Creating the Step Function Workflow 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI)","title":"Part 6 - WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS"},{"location":"000-Intro/000-intro/#part-7-data-quality-preparation-with-aws-glue-databrew","text":"1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Working with DataBrew Recipes & Projects","title":"Part 7 - DATA QUALITY &amp; PREPARATION WITH AWS GLUE DATABREW"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/","text":"WORKING WITH TPC-DS DATA & RDS MYSQL DATABASE Welcome to Part 1! In the Part 1 - Working with TPC-DS Data & RDS MySQL Database - you will be installing and configuring everything that is required to prepare and load TPC-DS* data into a RDS MySQL Database Instance in order to supply the necessary dataset samples for each piece of the ETL Traine The Trainer Workshop. 1. Preparing & Generating TPCDS Dataset \u00b6 TPC-DS - TPC data is used for a decision support benchmark. When you load this data the schema tpcds is updated with sample data. For more information about the tpcds data, see TPC-DS. Before you can install and generate TPC-Data, you need to build the necessary folders and download the necessary files. Run the following command to create the base level directory for the ETL Train the Trainer Workshop and to download the TCP-DS tool to your local AWS Cloud9 Enviroment mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip Next, run the following commands to unzip the previous downloaded (.zip) file and get into the DSGen software's directory to install the tool. <<<<<< unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql The last command (head) prints out the TPC-DS sql script for the tables' schemas to validate the instalation of the tool. Now, let's run the DSGen software to generate the sample data for all the TPC-DS tables and store the sample datasets in a temporary directory. TIP: This should take about 3 minutes to complete! <<<<<< mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt [OPTIONAL STEP] - Performing this optional step should allow for enough time for the previous step to complete! <<< write the right stuff While the above command is running, you can proactively test the reacheability of your RDS MySQL Database Instance for access coming from your AWS Cloud9 Enviroment. To do that, run the following command in a NEW terminal tab: sudo yum -y install telnet mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) telnet $mysqlendpoint 3306 TIP: Close this additional Terminal tab once Telnet command succeed! 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset \u00b6 Once the dataset samples get generated, it is time to load the data into the RDS MySQL Database. But first, run the following commands to create the TPC-DS tables' schemas in the MySQL database: tpcds_script_path = ~/environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" The last output should be a list of all the tables that have been created in the database. Now, it is time to load the data into the tables. To easily do that, you can run the following commands to download and run the a Shell script that does all the loading process into the RDS MySQL Database: aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"delete from customer where c_customer_sk > 1000\" ; mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"commit\" ; NOTE: Observe that for this workshop's purposes only, once the loading process is done a SQL Delete Statement is performed in order to delete multiple rows of the customer table since this table is too big originally. You can see all the tables getting loaded one by one... And here's a snippet of the Shell Script code (You don't need to create it!): 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3 \u00b6 Finally, running the following commands will: Extract the 3 required tables (web_page, customers and income_band) from the MySQL database; Save these tables in the local temporary directory (in CSV format); And, subsequently, upload these CSV files into the Workshop's S3 Bucket's path: \" .../etl-ttt-demo/csv_tables/ \" mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ You are finished populating the RDS MySQL Database with TPC-DS sample data. Now, feel free to explore the workshop's bucket again ( etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) and check the files insite the \"etl-ttt-demo/csv_tables\" path. Once you are ready you can move on to Part 2 - AWS GLUE COMPONENTS!","title":"TPCDS & RDS MySQL Lab"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#1-preparing-generating-tpcds-dataset","text":"TPC-DS - TPC data is used for a decision support benchmark. When you load this data the schema tpcds is updated with sample data. For more information about the tpcds data, see TPC-DS. Before you can install and generate TPC-Data, you need to build the necessary folders and download the necessary files. Run the following command to create the base level directory for the ETL Train the Trainer Workshop and to download the TCP-DS tool to your local AWS Cloud9 Enviroment mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip Next, run the following commands to unzip the previous downloaded (.zip) file and get into the DSGen software's directory to install the tool. <<<<<< unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql The last command (head) prints out the TPC-DS sql script for the tables' schemas to validate the instalation of the tool. Now, let's run the DSGen software to generate the sample data for all the TPC-DS tables and store the sample datasets in a temporary directory. TIP: This should take about 3 minutes to complete! <<<<<< mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt [OPTIONAL STEP] - Performing this optional step should allow for enough time for the previous step to complete! <<< write the right stuff While the above command is running, you can proactively test the reacheability of your RDS MySQL Database Instance for access coming from your AWS Cloud9 Enviroment. To do that, run the following command in a NEW terminal tab: sudo yum -y install telnet mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) telnet $mysqlendpoint 3306 TIP: Close this additional Terminal tab once Telnet command succeed!","title":"1. Preparing &amp; Generating TPCDS Dataset"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#2-populating-the-amazon-rds-mysql-database-with-tpcds-dataset","text":"Once the dataset samples get generated, it is time to load the data into the RDS MySQL Database. But first, run the following commands to create the TPC-DS tables' schemas in the MySQL database: tpcds_script_path = ~/environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" The last output should be a list of all the tables that have been created in the database. Now, it is time to load the data into the tables. To easily do that, you can run the following commands to download and run the a Shell script that does all the loading process into the RDS MySQL Database: aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"delete from customer where c_customer_sk > 1000\" ; mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"commit\" ; NOTE: Observe that for this workshop's purposes only, once the loading process is done a SQL Delete Statement is performed in order to delete multiple rows of the customer table since this table is too big originally. You can see all the tables getting loaded one by one... And here's a snippet of the Shell Script code (You don't need to create it!):","title":"2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#3-unloading-tables-in-csv-from-rds-mysql-database-and-uploading-to-s3","text":"Finally, running the following commands will: Extract the 3 required tables (web_page, customers and income_band) from the MySQL database; Save these tables in the local temporary directory (in CSV format); And, subsequently, upload these CSV files into the Workshop's S3 Bucket's path: \" .../etl-ttt-demo/csv_tables/ \" mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ You are finished populating the RDS MySQL Database with TPC-DS sample data. Now, feel free to explore the workshop's bucket again ( etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) and check the files insite the \"etl-ttt-demo/csv_tables\" path. Once you are ready you can move on to Part 2 - AWS GLUE COMPONENTS!","title":"3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3"},{"location":"02-GlueComponentsLab/02-glue-components-lab/","text":"AWS GLUE DISCOVERY COMPONENTS (Databases, Tables, Connections, Crawlers And Classifiers) Welcome to Part 2! In the Part 2 - AWS Glue Discovery Componets - of this ETL Train The Trainer workshop, you will understand the AWS Glue Data Catalog and all the Glue resources associated with it. At first, you will explore all the Glue resources created for you as part of the CloudFormation. Then, you will create and run couple more Glue Resources that will be required for the subsequent parts of the workshop. The Glue components that you work with during this lab include Glue Databases, Tables, Connection, Crawlers and Classifiers. 1. Understanding the Glue Resources Provided (CloudFormation Resources) \u00b6 To browse through all the Glue provided resources first go to the AWS Glue Console . In the far left menu of the AWS Glue Console you can access all your resources like Database, Tables, Crawlers, Jobs, etc. Databases Definition: A database is a set of associated table definitions, organized into a logical group. To see all your databases, click on Databases. There, you should see 2 databases, the Default - which, as the name suggests, is the default database for Glue in this region - and the \"glue_ttt_demo_db\" which is the database where you will be creating and storing all the workshop's tables metadata for all the labs. Tables Definition: A table is the metadata definition that represents your data, including its schema. A table can be used as a source or target in a job definition. To see all your databases, click on Tables, under Databases. There, you should see just one table named \"web-page-streaming-table\". This table has been created as part of the CloudFormation template and you will explore this table in details in the subsequent Part 3 - Glue Streaming Lab of this workshop. Connections Definition: A connection contains the properties needed to connect to your data. To see all your connections, click on Connections, under Databases. There, you should see only one connection named \"rds-mysql-connection\" which we will explore and test in the next step of this lab. Crawlers Definition: A crawler connects to a data store, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in your data catalog. To see all your crawlers, click on Crawlers. There, you should see 2 crawlers which were created as part of the CloudFormation template: - ml_bootstrap_crawler : This crawler is to bootstrap a table that is required for the Part 5 - Machine Learning with Glue & Glue Studio Notebooks. You will explore it later. - mysql-rds-crawler : This crawler is to crawl RDS MySQL customer's table. It uses a the above Glue Connection reach the RDS MySQL database instance. You will explore it in the next step. Classifiers Definition: A classifier determines the schema of your data. You can use the AWS Glue built-in classifiers or write your own. To see all your classifiers, click on Classifiers, under Crawlers. There is no classifiers created yet but you will be creating one in the third step step of this lab. 2. Testing and running pre-created Glue Resources (Glue Connection & Glue Crawler for MySQL RDS) \u00b6 Now, let's run a Crawler and verify the tables it will create. But first, let's confirm that the connection that the crawler is going to use is working properly. Click on Connections, then click on the blue name \"rds-mysql-connection\" to open the details of that connection. You can see that the connection has a JDBC URL that points to the \"tpcds\" database that you created earlier in the RDS MySQL. The JDBC URL uses the RDS instance's endpoint. The connection also has the right VPC and Subnet configured as well as a Security Group. Note 1: As part of the CloudFormation template, this security has been added to an Inboud Rule at the RDS Instance's Security Group which allows traffic on port 3306 (MySQL's default port) of the RDS instance. Click on Connections again, then check \"rds-mysql-connection\" connections box and click on Test Connection button. In the pop-up screen, choose the required IAM Role to perform the connection test (\"AWSGlueServiceRole-etl-ttt-demo\"). Finally, click on Test Connection. Note 2: The AWSGlueServiceRole-etl-ttt-demo has been created as part of CloudFormation template and has all the required permissions for all the labs in this workshop. While the connection test is running, you will notice the following banner at the: You don't need to wait for it to finish, you can now click on Crawlers and start exploring the \"mysql-rds-crawler\" by click on its blue name. As you can see, this crawler uses the aforementioned Glue Connection to crawl the path \"tpcds/customer\" which is the database/table that you created and loaded data earlier. From this screen, just click on Run Crawler at the top. By starting this crawler, you will notice its Status column progressing from Ready -> Starting -> Xmin elapsed -> Stopping -> Ready again. TIP: This crawler should take about 2-3 minutes to complete so move to the next lab then come back in 3 minutes to verify the results of running the crawler! At this point, the previous connection test should have already succeeded. You will see a green banner at top saying \"rds-mysql-connection connected successfully to your instance.\". At crawler's successfull completion, you should also see, in the Tables added column, that 1 table has been added. Go to Databases > Tables on the left to check the 1 table created. You will see that, apart from the streaming table, there will be another table named \"rds_crawled_tpcds_customer\" created in the \"glue_ttt_demo_db\" in which Location says \"tpcds.customers\" and Classification says \"mysql\". Explore this table by clicking on its blue name. 3. Creating new Glue Resources (New Crawler & Classifier) \u00b6 While the above \"mysql-rds-crawler\" is running (if you followed the previous tip), it is time to create couple more resources in Glue. First is a Glue Custom CSV Classifier. Create Classifier To create a classifier, click on Classifiers in the far left menu, then click on Add Classifier. Fill out the details with the following details: Classifier name: My-Custom-CSV-Classifier Classifier type: CSV Column delimiter: Comma (,) Quote symbol: Double-quote (\") Column headings: Has headings (headings list): c_full_name,c_email_address,total_clicks Click on the Create button at the bottom of pop-up page. A new classifier will be added to the list of classifiers. Create Crawler Next, as an advanced step, you are going to create a new crawler to crawl the path of the future Glue Streaming job's output that you will still develop in Part 3 - Glue (Studio) Streaming. To create a crawler, click on Crawlers in the far left menu, then click on Add Crawler. Fill out the details on each screen as following details: 1. On Add information about your crawler page, provide a name for the new Crawler such as \"crawl_streammed_data\". Then, expand the option that says \"Tags, description, security configuration, and classifiers (optional)\" and scroll to the bottom to see the list of custom classifiers on the right. Click on Add in front of \"My-Custom-CSV-Classifier\". It will appear on the left side as well now. Click Next. 2. On Specify crawler source type page, simply click Next. 3. On Add a data store page, under Include path, pick \"s3://\\${BUCKET_NAME}/etl-ttt-demo/output/gluestreaming/total_clicks/\" . Make sure you pick the csv folder rather than the file inside the folder, and then click Next. TIP: Switch back quickly to your Cloud9 enviroment in order to get the full path above by running the following command: echo s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/gluestreaming/total_clicks/ 4. Still on Add a data store page, under Sample Size (optional) choose 1. Then, expand where it says \"Exclude patterns (optional)\" and in the Exclude patterns field add: **00001 5. On Add another data store page, choose No and click Next. 6. On Choose an IAM role page, click Choose an existing IAM role and pick the role \"AWSGlueServiceRole-etl-ttt-demo\", then click Next. 7. On Create a schedule for this crawler page, click Next. 8. On Configure the crawler's output page, choose \"glue_ttt_demo_db\" from the Database dropdown list. Then, expand where it says \"Grouping behavior for S3 data (optional)\" and check the box \"Create a single schema for each S3 path\". Click Next. 9. Review everything in the last page and click Finish. !!! DO NOT RUN THIS CRAWLER YET!!!! You are finished populating reviewing and setting up new Glue Resources. Once you are ready you can move on to Part 3 - Glue (Studio) Streaming!","title":"GLUE DATABASE, TABLES, CONNECTIONS, CRAWLERS, CLASSIFIERS"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#1-understanding-the-glue-resources-provided-cloudformation-resources","text":"To browse through all the Glue provided resources first go to the AWS Glue Console . In the far left menu of the AWS Glue Console you can access all your resources like Database, Tables, Crawlers, Jobs, etc. Databases Definition: A database is a set of associated table definitions, organized into a logical group. To see all your databases, click on Databases. There, you should see 2 databases, the Default - which, as the name suggests, is the default database for Glue in this region - and the \"glue_ttt_demo_db\" which is the database where you will be creating and storing all the workshop's tables metadata for all the labs. Tables Definition: A table is the metadata definition that represents your data, including its schema. A table can be used as a source or target in a job definition. To see all your databases, click on Tables, under Databases. There, you should see just one table named \"web-page-streaming-table\". This table has been created as part of the CloudFormation template and you will explore this table in details in the subsequent Part 3 - Glue Streaming Lab of this workshop. Connections Definition: A connection contains the properties needed to connect to your data. To see all your connections, click on Connections, under Databases. There, you should see only one connection named \"rds-mysql-connection\" which we will explore and test in the next step of this lab. Crawlers Definition: A crawler connects to a data store, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in your data catalog. To see all your crawlers, click on Crawlers. There, you should see 2 crawlers which were created as part of the CloudFormation template: - ml_bootstrap_crawler : This crawler is to bootstrap a table that is required for the Part 5 - Machine Learning with Glue & Glue Studio Notebooks. You will explore it later. - mysql-rds-crawler : This crawler is to crawl RDS MySQL customer's table. It uses a the above Glue Connection reach the RDS MySQL database instance. You will explore it in the next step. Classifiers Definition: A classifier determines the schema of your data. You can use the AWS Glue built-in classifiers or write your own. To see all your classifiers, click on Classifiers, under Crawlers. There is no classifiers created yet but you will be creating one in the third step step of this lab.","title":"1. Understanding the Glue Resources Provided (CloudFormation Resources)"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#2-testing-and-running-pre-created-glue-resources-glue-connection-glue-crawler-for-mysql-rds","text":"Now, let's run a Crawler and verify the tables it will create. But first, let's confirm that the connection that the crawler is going to use is working properly. Click on Connections, then click on the blue name \"rds-mysql-connection\" to open the details of that connection. You can see that the connection has a JDBC URL that points to the \"tpcds\" database that you created earlier in the RDS MySQL. The JDBC URL uses the RDS instance's endpoint. The connection also has the right VPC and Subnet configured as well as a Security Group. Note 1: As part of the CloudFormation template, this security has been added to an Inboud Rule at the RDS Instance's Security Group which allows traffic on port 3306 (MySQL's default port) of the RDS instance. Click on Connections again, then check \"rds-mysql-connection\" connections box and click on Test Connection button. In the pop-up screen, choose the required IAM Role to perform the connection test (\"AWSGlueServiceRole-etl-ttt-demo\"). Finally, click on Test Connection. Note 2: The AWSGlueServiceRole-etl-ttt-demo has been created as part of CloudFormation template and has all the required permissions for all the labs in this workshop. While the connection test is running, you will notice the following banner at the: You don't need to wait for it to finish, you can now click on Crawlers and start exploring the \"mysql-rds-crawler\" by click on its blue name. As you can see, this crawler uses the aforementioned Glue Connection to crawl the path \"tpcds/customer\" which is the database/table that you created and loaded data earlier. From this screen, just click on Run Crawler at the top. By starting this crawler, you will notice its Status column progressing from Ready -> Starting -> Xmin elapsed -> Stopping -> Ready again. TIP: This crawler should take about 2-3 minutes to complete so move to the next lab then come back in 3 minutes to verify the results of running the crawler! At this point, the previous connection test should have already succeeded. You will see a green banner at top saying \"rds-mysql-connection connected successfully to your instance.\". At crawler's successfull completion, you should also see, in the Tables added column, that 1 table has been added. Go to Databases > Tables on the left to check the 1 table created. You will see that, apart from the streaming table, there will be another table named \"rds_crawled_tpcds_customer\" created in the \"glue_ttt_demo_db\" in which Location says \"tpcds.customers\" and Classification says \"mysql\". Explore this table by clicking on its blue name.","title":"2. Testing and running pre-created Glue Resources (Glue Connection &amp; Glue Crawler for MySQL RDS)"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#3-creating-new-glue-resources-new-crawler-classifier","text":"While the above \"mysql-rds-crawler\" is running (if you followed the previous tip), it is time to create couple more resources in Glue. First is a Glue Custom CSV Classifier. Create Classifier To create a classifier, click on Classifiers in the far left menu, then click on Add Classifier. Fill out the details with the following details: Classifier name: My-Custom-CSV-Classifier Classifier type: CSV Column delimiter: Comma (,) Quote symbol: Double-quote (\") Column headings: Has headings (headings list): c_full_name,c_email_address,total_clicks Click on the Create button at the bottom of pop-up page. A new classifier will be added to the list of classifiers. Create Crawler Next, as an advanced step, you are going to create a new crawler to crawl the path of the future Glue Streaming job's output that you will still develop in Part 3 - Glue (Studio) Streaming. To create a crawler, click on Crawlers in the far left menu, then click on Add Crawler. Fill out the details on each screen as following details: 1. On Add information about your crawler page, provide a name for the new Crawler such as \"crawl_streammed_data\". Then, expand the option that says \"Tags, description, security configuration, and classifiers (optional)\" and scroll to the bottom to see the list of custom classifiers on the right. Click on Add in front of \"My-Custom-CSV-Classifier\". It will appear on the left side as well now. Click Next. 2. On Specify crawler source type page, simply click Next. 3. On Add a data store page, under Include path, pick \"s3://\\${BUCKET_NAME}/etl-ttt-demo/output/gluestreaming/total_clicks/\" . Make sure you pick the csv folder rather than the file inside the folder, and then click Next. TIP: Switch back quickly to your Cloud9 enviroment in order to get the full path above by running the following command: echo s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/gluestreaming/total_clicks/ 4. Still on Add a data store page, under Sample Size (optional) choose 1. Then, expand where it says \"Exclude patterns (optional)\" and in the Exclude patterns field add: **00001 5. On Add another data store page, choose No and click Next. 6. On Choose an IAM role page, click Choose an existing IAM role and pick the role \"AWSGlueServiceRole-etl-ttt-demo\", then click Next. 7. On Create a schedule for this crawler page, click Next. 8. On Configure the crawler's output page, choose \"glue_ttt_demo_db\" from the Database dropdown list. Then, expand where it says \"Grouping behavior for S3 data (optional)\" and check the box \"Create a single schema for each S3 path\". Click Next. 9. Review everything in the last page and click Finish.","title":"3. Creating new Glue Resources (New Crawler &amp; Classifier)"},{"location":"03-StreamingLab/03-streaming-lab/","text":"GLUE (STUDIO) STREAMING The Part 3 - Glue (Studio) Streaming of the ETL Train the Trainer workshop is where you are going to use AWS Glue Studio Graphical Interface for the first time. You are going to use Glue Studio do build 2 jobs here. The first job is a dummy Glue Streaming Job that will validate the JOIN between the customer RDS table and a web_page CSV data that you uploaded earlier to S3. Note: This CSV data is basically a representation of the actual web-page-streaming-table (with the different of that one being in JSON format) that you will use in real Glue Streaming Job Apart from validate the JOIN between datasets, this dummy job will also validate the SQL code used in the SQL Transform node of this lab. If validation succeeds, you will then be able to Preview the Data before outputting it or, in this case, before building the real Glue Streaming Job. 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) \u00b6 For this lab to work, the CloudFormation template provided few resources for you already. Kinesis Data Stream A Kinesis Data Stream named etl-ttt-demo-stream has been added already which will receive the stream of data from a script you will run later on this lab. Click here to explore the Kinesis Data Stream if you want. Cataloged Streaming Table As you already saw it, a table named etl-ttt-demo-stream has been created for you with a proper schema definition and all the necessary settings. You can see the details of this table in the AWS Glue Console . Here's a snapshot of that table's schema. Kinesis Data Ingestion Script (python) As mentioned, you will be running the following script in order to simulate streaming data being added to your Kinesis Data Stream etl-ttt-demo-stream To download the script (and to take a loot at it), run the following commands: cd ~/environment/ttt-demo/ aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) \u00b6 Let's start building the dummy job to validate our Glue Streaming Job logic. From the AWS Glue Console, click on AWS Glue Studio under the ETL section in the far left menu. This will take you to the AWS Glue Studio Console where you will be authoring the jobs graphically. For that, click on Jobs on the far left menu, select Visual with a blank canvas option and click on Create . You will be presented with a blank canva. The first thing you must do there is to rename you job. Just click where it says \"Untitled job\" , type dummy-streaming-job and click out the Job name box. Now, the next thing you must do, is to set your jobs detail. Click on the tab Job details tab and set the following configurations: Under IAM role select AWSGlueServiceRole-etl-ttt-demo Check the Automatically scale the number of workers option (GA regions only, otherwise set number of workers to 4 ) Under Job bookmark select Disable Set Number of retries to 0 Note: In production environments, you want to enable the bookmark and set retries to bigger than 0. You don't need to change any other settings here, but you should take some time to explore what settings are available in this tab. When you are done exploring, click Save on the upper right to save the changed settings. Click the Visual tab again to go back to visual editor. You should see 3 dropdown buttons: Source , Transform , and Target . Let's start creating the following 2 sources: Web Page Source For the Web Page CSV source, click on the Source dropdown icon and choose Amazon S3 in dropdown list. Click on the node that has been automatically added to the canvas to highlight it. Make the following changes to it: Click Node properties tab Set Name to Web Page S3 Click Data source properties - S3 tab Under S3 source type select S3 location Set S3 URL to: s3://${BUCKET_NAME}/etl-ttt-demo/csv_tables/web_page.csv - (use the Browse S3 bucket to navigate.) Uncheck the Recursive option Click Infer schema button at the bottom Click on Output schema tab to see schema infered, then click Save Customer RDS Source For the Customer RDS source, click on the Source dropdown icon and choose AWS Glue Data Catalog in the dropdown list. Click Node properties tab Change Name to Customer RDS Click Data source properties - Data Catalog tab Under Database select glue_ttt_demo_db Under Table select rds_crawled_tpcds_customer Click on Output schema tab to see schema, then click Save To join both datasets, click on the Web Page S3 node first to highlight it, then click on Transform dropdown icon. Note: You will notice there are pre-build transformations and custom transformations. Glue Studio is designed to be used by developers who could write custom Apache Spark, Glue and SQL code , but it also provides pre-build common transformations. For this lab you wil lbe using a custom SQL Transform to write your own SQL Join code. In the list of transforms that appears, scroll to the bottom and choose SQL . A new SQL node will be linked to the Web Page S3 node. Click on this new SQL node to highligt it and do the following: Click Node properties tab Under Node parents , select Customer RDS by clicking the checkbox next to it Click Transform tab Set Input sources Web Page S3 with Spark SQL aliases value to webpage Set Input sources Customer RDS with Spark SQL aliases value to cust Copy the following code to SQL query and click Save select CONCAT ( cust . c_first_name , ' ' , cust . c_last_name ) as full_name , cust . c_email_address , count ( webpage . wp_char_count ) as total_clicks from cust left join webpage on cust . c_customer_sk = webpage . wp_customer_sk where 1 = 2 --remove the where clause after preview!! group by 1 , 2 order by total_clicks desc Click on Data Preview tab Click on the Start data preview session button there. In the pop-up window that will open, choose the IAM role AWSGlueServiceRole-etl-ttt-demo Wait for Data Preview to finish (it takes about 5 minutes to complete) Note: Preview the query with where 1=2 first to make previewing faster and to avoid issues. If issues happen, close this job and clone it into a new one (clone steps is following). Once preview is completed, remove the where 1=2 clause. Click on the Data Preview tab again to see the data there. Note that that only relevant columns were brought from the SQL query. Click on Output schema tab to see schema, Note that you have 2 outputs: Output 1 and Output 2 . Click on the Use datapreview schema button that you see right there in the Output schema tab. Note that the schema now matches only the relevant columns of the query. Click Save . Now, add an Apply Mapping transform by clickinng on the SQL node first to highlight it, then click on Transform dropdown icon and choose Apply Mapping from the list. A new Apply Mapping node will be linked to the SQL node. Click on this new Apply Mapping node to highligt it and do the following: Click on Transform tab Change Source key full_name to c_full_name Click on Output schema tab to see the change and Save . Finally, add a Target to the job. To do this, first click on the Apply Mapping node to highlight it, then click on Target dropdown, choose Amazon S3 from the list and a new Amazon S3 node will be linked to the Apply Mapping node. Click on this new Amazon S3 node to highligt it and do the following: Click on Data target properties - S3 tab Set Format to CSV Set S3 Target Location to s3://$BUCKET_NAME/etl-ttt-demo/output/gluestreaming/total_clicks/ TIP: Switch back quickly to your Cloud9 enviroment and use the following command to build the entire path you need for the S3 Target Location above. echo \"s3:// $BUCKET_NAME /etl-ttt-demo/output/gluestreaming/total_clicks/\" !!! You can now save this job for the last time but DO NOT RUN IT!!!! 3. Creating the Glue Streaming Job (Cloning Jobs!) \u00b6 <<<<< write the right stuff [CLONE THE DUMMY JOB AND REPLACE THE CSV WEB_PAGE SOURCE BY THE KINESIS WEB_PAGE STREAMING TABLE] !!!! RUN THE JOB BUT DON'T SEND ANY STREAMING DATA YET!!!! (SETUP EVENT BRIDGE LAB FIRST !!!!!! <<<<< write the right stuff","title":"GLUE (STUDIO) STREAMING"},{"location":"03-StreamingLab/03-streaming-lab/#1-understanding-the-streaming-resources-provided-cloudformation-scripts","text":"For this lab to work, the CloudFormation template provided few resources for you already. Kinesis Data Stream A Kinesis Data Stream named etl-ttt-demo-stream has been added already which will receive the stream of data from a script you will run later on this lab. Click here to explore the Kinesis Data Stream if you want. Cataloged Streaming Table As you already saw it, a table named etl-ttt-demo-stream has been created for you with a proper schema definition and all the necessary settings. You can see the details of this table in the AWS Glue Console . Here's a snapshot of that table's schema. Kinesis Data Ingestion Script (python) As mentioned, you will be running the following script in order to simulate streaming data being added to your Kinesis Data Stream etl-ttt-demo-stream To download the script (and to take a loot at it), run the following commands: cd ~/environment/ttt-demo/ aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py","title":"1. Understanding the Streaming Resources Provided (CloudFormation &amp; Scripts)"},{"location":"03-StreamingLab/03-streaming-lab/#2-validating-streaming-job-logic-and-data-glue-studio-dummy-job","text":"Let's start building the dummy job to validate our Glue Streaming Job logic. From the AWS Glue Console, click on AWS Glue Studio under the ETL section in the far left menu. This will take you to the AWS Glue Studio Console where you will be authoring the jobs graphically. For that, click on Jobs on the far left menu, select Visual with a blank canvas option and click on Create . You will be presented with a blank canva. The first thing you must do there is to rename you job. Just click where it says \"Untitled job\" , type dummy-streaming-job and click out the Job name box. Now, the next thing you must do, is to set your jobs detail. Click on the tab Job details tab and set the following configurations: Under IAM role select AWSGlueServiceRole-etl-ttt-demo Check the Automatically scale the number of workers option (GA regions only, otherwise set number of workers to 4 ) Under Job bookmark select Disable Set Number of retries to 0 Note: In production environments, you want to enable the bookmark and set retries to bigger than 0. You don't need to change any other settings here, but you should take some time to explore what settings are available in this tab. When you are done exploring, click Save on the upper right to save the changed settings. Click the Visual tab again to go back to visual editor. You should see 3 dropdown buttons: Source , Transform , and Target . Let's start creating the following 2 sources: Web Page Source For the Web Page CSV source, click on the Source dropdown icon and choose Amazon S3 in dropdown list. Click on the node that has been automatically added to the canvas to highlight it. Make the following changes to it: Click Node properties tab Set Name to Web Page S3 Click Data source properties - S3 tab Under S3 source type select S3 location Set S3 URL to: s3://${BUCKET_NAME}/etl-ttt-demo/csv_tables/web_page.csv - (use the Browse S3 bucket to navigate.) Uncheck the Recursive option Click Infer schema button at the bottom Click on Output schema tab to see schema infered, then click Save Customer RDS Source For the Customer RDS source, click on the Source dropdown icon and choose AWS Glue Data Catalog in the dropdown list. Click Node properties tab Change Name to Customer RDS Click Data source properties - Data Catalog tab Under Database select glue_ttt_demo_db Under Table select rds_crawled_tpcds_customer Click on Output schema tab to see schema, then click Save To join both datasets, click on the Web Page S3 node first to highlight it, then click on Transform dropdown icon. Note: You will notice there are pre-build transformations and custom transformations. Glue Studio is designed to be used by developers who could write custom Apache Spark, Glue and SQL code , but it also provides pre-build common transformations. For this lab you wil lbe using a custom SQL Transform to write your own SQL Join code. In the list of transforms that appears, scroll to the bottom and choose SQL . A new SQL node will be linked to the Web Page S3 node. Click on this new SQL node to highligt it and do the following: Click Node properties tab Under Node parents , select Customer RDS by clicking the checkbox next to it Click Transform tab Set Input sources Web Page S3 with Spark SQL aliases value to webpage Set Input sources Customer RDS with Spark SQL aliases value to cust Copy the following code to SQL query and click Save select CONCAT ( cust . c_first_name , ' ' , cust . c_last_name ) as full_name , cust . c_email_address , count ( webpage . wp_char_count ) as total_clicks from cust left join webpage on cust . c_customer_sk = webpage . wp_customer_sk where 1 = 2 --remove the where clause after preview!! group by 1 , 2 order by total_clicks desc Click on Data Preview tab Click on the Start data preview session button there. In the pop-up window that will open, choose the IAM role AWSGlueServiceRole-etl-ttt-demo Wait for Data Preview to finish (it takes about 5 minutes to complete) Note: Preview the query with where 1=2 first to make previewing faster and to avoid issues. If issues happen, close this job and clone it into a new one (clone steps is following). Once preview is completed, remove the where 1=2 clause. Click on the Data Preview tab again to see the data there. Note that that only relevant columns were brought from the SQL query. Click on Output schema tab to see schema, Note that you have 2 outputs: Output 1 and Output 2 . Click on the Use datapreview schema button that you see right there in the Output schema tab. Note that the schema now matches only the relevant columns of the query. Click Save . Now, add an Apply Mapping transform by clickinng on the SQL node first to highlight it, then click on Transform dropdown icon and choose Apply Mapping from the list. A new Apply Mapping node will be linked to the SQL node. Click on this new Apply Mapping node to highligt it and do the following: Click on Transform tab Change Source key full_name to c_full_name Click on Output schema tab to see the change and Save . Finally, add a Target to the job. To do this, first click on the Apply Mapping node to highlight it, then click on Target dropdown, choose Amazon S3 from the list and a new Amazon S3 node will be linked to the Apply Mapping node. Click on this new Amazon S3 node to highligt it and do the following: Click on Data target properties - S3 tab Set Format to CSV Set S3 Target Location to s3://$BUCKET_NAME/etl-ttt-demo/output/gluestreaming/total_clicks/ TIP: Switch back quickly to your Cloud9 enviroment and use the following command to build the entire path you need for the S3 Target Location above. echo \"s3:// $BUCKET_NAME /etl-ttt-demo/output/gluestreaming/total_clicks/\"","title":"2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job)"},{"location":"03-StreamingLab/03-streaming-lab/#3-creating-the-glue-streaming-job-cloning-jobs","text":"<<<<< write the right stuff [CLONE THE DUMMY JOB AND REPLACE THE CSV WEB_PAGE SOURCE BY THE KINESIS WEB_PAGE STREAMING TABLE] !!!! RUN THE JOB BUT DON'T SEND ANY STREAMING DATA YET!!!! (SETUP EVENT BRIDGE LAB FIRST !!!!!! <<<<< write the right stuff","title":"3. Creating the Glue Streaming Job (Cloning Jobs!)"},{"location":"04-OrchestrationLab/04-orchestration-lab/","text":"ORCHESTRATION & DATA ANALYSIS <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Understanding the Orchestration Flow \u00b6 <<<<< write the right stuff EVENT BRIDGE TRIGGERS A WORKFLOW AS SOON AS STREAMING DATA LANDS IN S3 - WORKFLOW CRAWLS THE DATA --> MAKE SURE THE TRAIL HAS THE FOLLOWING PATH CONFIGURED: Bucket name: etl-ttt-demo-171714944327-ca-central-1 Prefix: /etl-ttt-demo/output/gluestreaming/total_clicks/ <<<<< write the right stuff 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) \u00b6 <<<<< write the right stuff --- create the workflow first ---- aws glue create-workflow --name etl-ttt-event-driven-workflow --- then the trigger ---- aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl_streammed_data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" <<<<< write the right stuff 3. Creating Event Bridge Rule and Target (via CLI) \u00b6 <<<<< write the right stuff --- next the event rule ---- aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" --- finally the event rule target ---- aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"glueworkflow-totalclicks\",\"Arn\"=\"arn:aws:glue: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } <<<<< write the right stuff 4. Triggering Orchestration & Following The Flow \u00b6 <<<<< write the right stuff [CHECK THE KINESIS SCRIPT ONCE AGAIN] cd ~/environment/ttt-demo/ cat PutRecord_Kinesis.py [SCRIPT SHOULD LOOK LIKE THIS] import csv import json import boto3 import time import string import random def generate ( stream_name , kinesis_client ): with open ( \"/tmp/dsd/csv_tables/web_page.csv\" , encoding = 'utf-8' ) as csvf : csvReader = csv . DictReader ( csvf ) for rows in csvReader : partitionaKey = '' . join ( random . choices ( string . ascii_uppercase + string . digits , k = 20 )) jsonMsg = json . dumps ( rows ) kinesis_client . put_record ( StreamName = stream_name , Data = jsonMsg , PartitionKey = partitionaKey ) print ( jsonMsg ) time . sleep ( 0.2 ) if __name__ == '__main__' : generate ( 'etl-ttt-demo-stream' , boto3 . client ( 'kinesis' )) [ end of script ] [ ONCE EVENT BRIDGE IS FULLY SETUP WITH TARGET AND ALL ... ] [ START STREAMING DATA BY RUNNIG THE ABOVE SCRIPT WITH THE FOLLOWING COMMAND IN CLOUD9 ] [ WAIT UP TO 1 OR 2 MINUTES ] [START THE SCRIPT] python PutRecord_Kinesis.py [OBSERVE THE WORKFLOW GETTING TRIGGER AS THE FIRST FILES ARRIVE IN S3 FROM THEM STREAMING JOB] [OBSERVE THAT WORKFLOW TRIGGERS THE CRAWLER WHICH CREATES THE TABLE (total_clicks) FOR FOLLOWING ANALYSIS LAB] [DELETE THE WORKFLOW] [ maybe try cli to EDIT THE TRIGGER TO 100 SO IT STOP RUNNING AT EVERY BATCH OF STREAMING DATA] <<<<< write the right stuff 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog \u00b6 <<<<< write the right stuff [ADD DEFAULT BUCKET FOR ATHENA: /athena-output/] [RUN THE FOLLOWING QUERY] SELECT * FROM \"AwsDataCatalog\" . \"glue-ttt-demo-db\" . \"total_clicks\" order by 3 desc limit 10 ; [Creating Views on Top of Cataloged Tables] [CREATE A VIEW ON TOP OF THE CREATED TABLE TO BROWSE THE TOP 5 CUSTOMERS CLICKING ON THE WEBSITE] CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\" ( CAST ( \"total_clicks\" AS integer )) total_clicks FROM \"total_clicks\" GROUP BY 1 , 2 ORDER BY 3 DESC [QUERY THE VIEW WITH BELOW QUERY] SELECT * FROM \"glue-ttt-demo-db\" . \"tpc_customer_inter\" order by 3 desc limit 10 ; [ADD MORE DATA TO THE STREAM AND KEEP CHECKING WITH ABOVE QUERY] - EVERY 30-60 SECONDS [IF YEAR, MONTH, DAY OR HOUR CHANGES NEED TO UPDATE PARTITIONS BY RUNNING CRAWLER AGAIN] <<<<< write the right stuff","title":"ORCHESTRATION & DATA ANALYSIS"},{"location":"04-OrchestrationLab/04-orchestration-lab/#1-understanding-the-orchestration-flow","text":"<<<<< write the right stuff EVENT BRIDGE TRIGGERS A WORKFLOW AS SOON AS STREAMING DATA LANDS IN S3 - WORKFLOW CRAWLS THE DATA --> MAKE SURE THE TRAIL HAS THE FOLLOWING PATH CONFIGURED: Bucket name: etl-ttt-demo-171714944327-ca-central-1 Prefix: /etl-ttt-demo/output/gluestreaming/total_clicks/ <<<<< write the right stuff","title":"1. Understanding the Orchestration Flow"},{"location":"04-OrchestrationLab/04-orchestration-lab/#2-creating-glue-workflow-and-glue-event-based-trigger-via-cli","text":"<<<<< write the right stuff --- create the workflow first ---- aws glue create-workflow --name etl-ttt-event-driven-workflow --- then the trigger ---- aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl_streammed_data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" <<<<< write the right stuff","title":"2. Creating Glue Workflow and Glue Event Based Trigger (via CLI)"},{"location":"04-OrchestrationLab/04-orchestration-lab/#3-creating-event-bridge-rule-and-target-via-cli","text":"<<<<< write the right stuff --- next the event rule ---- aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" --- finally the event rule target ---- aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"glueworkflow-totalclicks\",\"Arn\"=\"arn:aws:glue: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } <<<<< write the right stuff","title":"3. Creating Event Bridge Rule and Target (via CLI)"},{"location":"04-OrchestrationLab/04-orchestration-lab/#4-triggering-orchestration-following-the-flow","text":"<<<<< write the right stuff [CHECK THE KINESIS SCRIPT ONCE AGAIN] cd ~/environment/ttt-demo/ cat PutRecord_Kinesis.py [SCRIPT SHOULD LOOK LIKE THIS] import csv import json import boto3 import time import string import random def generate ( stream_name , kinesis_client ): with open ( \"/tmp/dsd/csv_tables/web_page.csv\" , encoding = 'utf-8' ) as csvf : csvReader = csv . DictReader ( csvf ) for rows in csvReader : partitionaKey = '' . join ( random . choices ( string . ascii_uppercase + string . digits , k = 20 )) jsonMsg = json . dumps ( rows ) kinesis_client . put_record ( StreamName = stream_name , Data = jsonMsg , PartitionKey = partitionaKey ) print ( jsonMsg ) time . sleep ( 0.2 ) if __name__ == '__main__' : generate ( 'etl-ttt-demo-stream' , boto3 . client ( 'kinesis' )) [ end of script ] [ ONCE EVENT BRIDGE IS FULLY SETUP WITH TARGET AND ALL ... ] [ START STREAMING DATA BY RUNNIG THE ABOVE SCRIPT WITH THE FOLLOWING COMMAND IN CLOUD9 ] [ WAIT UP TO 1 OR 2 MINUTES ] [START THE SCRIPT] python PutRecord_Kinesis.py [OBSERVE THE WORKFLOW GETTING TRIGGER AS THE FIRST FILES ARRIVE IN S3 FROM THEM STREAMING JOB] [OBSERVE THAT WORKFLOW TRIGGERS THE CRAWLER WHICH CREATES THE TABLE (total_clicks) FOR FOLLOWING ANALYSIS LAB] [DELETE THE WORKFLOW] [ maybe try cli to EDIT THE TRIGGER TO 100 SO IT STOP RUNNING AT EVERY BATCH OF STREAMING DATA] <<<<< write the right stuff","title":"4. Triggering Orchestration &amp; Following The Flow"},{"location":"04-OrchestrationLab/04-orchestration-lab/#5-exploring-and-analyzing-tables-data-cataloged-in-glue-data-catalog","text":"<<<<< write the right stuff [ADD DEFAULT BUCKET FOR ATHENA: /athena-output/] [RUN THE FOLLOWING QUERY] SELECT * FROM \"AwsDataCatalog\" . \"glue-ttt-demo-db\" . \"total_clicks\" order by 3 desc limit 10 ; [Creating Views on Top of Cataloged Tables] [CREATE A VIEW ON TOP OF THE CREATED TABLE TO BROWSE THE TOP 5 CUSTOMERS CLICKING ON THE WEBSITE] CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\" ( CAST ( \"total_clicks\" AS integer )) total_clicks FROM \"total_clicks\" GROUP BY 1 , 2 ORDER BY 3 DESC [QUERY THE VIEW WITH BELOW QUERY] SELECT * FROM \"glue-ttt-demo-db\" . \"tpc_customer_inter\" order by 3 desc limit 10 ; [ADD MORE DATA TO THE STREAM AND KEEP CHECKING WITH ABOVE QUERY] - EVERY 30-60 SECONDS [IF YEAR, MONTH, DAY OR HOUR CHANGES NEED TO UPDATE PARTITIONS BY RUNNING CRAWLER AGAIN] <<<<< write the right stuff","title":"5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/","text":"MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOK <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 0. (Pre Steps) - Understading & Setting up the Resources for ML Lab \u00b6 <<<<< write the right stuff [MACHINE LEARNING TRANSFORM LAB] DESCRIPTION: CSV with duplicated customers but with mkdir -p /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling /tmp/dsd/csv_tables/ml-lab/ml-customer-full /tmp/dsd/csv_tables/ml-lab/ml-labeling-file aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/sample_topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling/sample_topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/full-topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/topcuustomer-labeling-file.csv /tmp/dsd/csv_tables/ml-lab/ml-labeling-file/topcuustomer-labeling-file.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ml-lab s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ml-lab/ <<<<< write the right stuff 1. Creating and Training the Glue FindMatches ML Transform \u00b6 Run the ML- Crawler (add this one to the CFN template) Use the sample file to generate labeling file (in S3) Download the generated labeling file and show how to add the labels to similar records (Just show 2 or 3 records to demonstrate) Use the already-filled labeling file to train the ML FindMatches transform (get it from S3) [Don't need to wait anything to complete, just show it. Then use the pre-created ones] 2. Testing FindMatches Transform with Glue Studio Notebook \u00b6 <<<<< write the right stuff Once the FindMatches transform is trained, Open Glue studio and create a new Glue Studio Notebook. - Name: ml-lab-notebook-job - IAM Role: Glue Service Role - etl-ttt-demo Write the following Spark Code into 4 differente cells Cell 1 [replace the existing Cell 1]: % glue_version 2.0 % idle_timeout 60 import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglueml.transforms import FindMatches sc = SparkContext . getOrCreate () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) Cell 2: datasource0 = glueContext . create_dynamic_frame . from_catalog ( database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"datasource0\" ) resolvechoice1 = ResolveChoice . apply ( frame = datasource0 , choice = \"MATCH_CATALOG\" , database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"resolvechoice1\" ) resolvechoice1 . printSchema () Cell 3: findmatches2 = FindMatches . apply ( frame = resolvechoice1 , transformId = \"tfm-733f49ce1a5f98c4e2b8a9604ae4427368b08b07\" , transformation_ctx = \"findmatches2\" ) resolvechoice3 = ResolveChoice . apply ( frame = findmatches2 , choice = \"make_struct\" , transformation_ctx = \"resolvechoice3\" ) dropnullfield4 = DropNullFields . apply ( frame = resolvechoice3 , transformation_ctx = \"dropnullfield4\" ) df1 = dropnullfield4 . toDF () print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) Cell 4: custidlist = [ \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAJLKGBAAA\" , \"AAAAAAAAJLKGBAAA\" , \"AAAAAAAAPLNDAAAA\" , \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAPLNDAAAA\" , \"BAAAAAAANDCFBAAA\" , \"AAAAAAAANKFAAAAA\" , \"BAAAAAAANKFAAAAA\" , \"AAAAAAAADFHGAAAA\" , \"BAAAAAAADFHGAAAA\" ] cols = [ 'c_customer_id' , 'c_current_addr_sk' , 'c_salutation' , 'c_first_name' , 'c_last_name' , 'c_birth_day' , 'c_birth_month' , 'c_birth_year' , 'c_birth_country' , 'c_login' , 'c_email_address' , 'match_id' ] print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) df1 . filter ( df1 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) Cell 5: df2 = df1 . drop_duplicates ( subset = [ 'match_id' ]) print ( \"Distinct count of matching IDs : \" + str ( df2 . count ())) df2 . filter ( df2 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) ---> Wait for the last cell to finish and output the results. <<<<< write the right stuff 3. Validating and Deploying the FindMatches Glue Job \u00b6 <<<<< write the right stuff ---> validate the results: --> Reference this blog: Reference This Blog: http://blog.zenof.ai/machine-learning-based-fuzzy-matching-using-aws-glue-ml-transforms/ \"However, the Find matches transform adds another column named match_id to identify matching records in the output. Rows with the same match_id are considered matching records.\" Once done explaining the Matching IDs logic, add a new cell to the notebook to sink the very final dataframe to S3 but DON'T RUN this cell!!! [DON'T RUN THIS CELL] Cell 6: new_df = df2 . select ( * cols ) . orderBy ( \"match_id\" ) new_df . coalesce ( 1 ) . write . option ( \"header\" , \"true\" ) . mode ( \"overwrite\" ) . csv ( \"s3://etl-ttt-demo-589541849248-us-east-2/etl-ttt-demo/output/ml-lab/topcustomers-dedup/\" ) [DON'T RUN THIS CELL] JUST SAVE THE JOB TO DEPLOY IT PROPERLY AND MOVE TO THE NEXT LAB] <<<<< write the right stuff","title":"MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOKS"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#0-pre-steps-understading-setting-up-the-resources-for-ml-lab","text":"<<<<< write the right stuff [MACHINE LEARNING TRANSFORM LAB] DESCRIPTION: CSV with duplicated customers but with mkdir -p /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling /tmp/dsd/csv_tables/ml-lab/ml-customer-full /tmp/dsd/csv_tables/ml-lab/ml-labeling-file aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/sample_topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling/sample_topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/full-topcustomer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/topcuustomer-labeling-file.csv /tmp/dsd/csv_tables/ml-lab/ml-labeling-file/topcuustomer-labeling-file.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ml-lab s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ml-lab/ <<<<< write the right stuff","title":"0. (Pre Steps) - Understading &amp; Setting up the Resources for ML Lab"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#1-creating-and-training-the-glue-findmatches-ml-transform","text":"Run the ML- Crawler (add this one to the CFN template) Use the sample file to generate labeling file (in S3) Download the generated labeling file and show how to add the labels to similar records (Just show 2 or 3 records to demonstrate) Use the already-filled labeling file to train the ML FindMatches transform (get it from S3) [Don't need to wait anything to complete, just show it. Then use the pre-created ones]","title":"1. Creating and Training the Glue FindMatches ML Transform"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#2-testing-findmatches-transform-with-glue-studio-notebook","text":"<<<<< write the right stuff Once the FindMatches transform is trained, Open Glue studio and create a new Glue Studio Notebook. - Name: ml-lab-notebook-job - IAM Role: Glue Service Role - etl-ttt-demo Write the following Spark Code into 4 differente cells Cell 1 [replace the existing Cell 1]: % glue_version 2.0 % idle_timeout 60 import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglueml.transforms import FindMatches sc = SparkContext . getOrCreate () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) Cell 2: datasource0 = glueContext . create_dynamic_frame . from_catalog ( database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"datasource0\" ) resolvechoice1 = ResolveChoice . apply ( frame = datasource0 , choice = \"MATCH_CATALOG\" , database = \"glue-ttt-demo-db\" , table_name = \"ml_dedup_full_topcustomer_csv\" , transformation_ctx = \"resolvechoice1\" ) resolvechoice1 . printSchema () Cell 3: findmatches2 = FindMatches . apply ( frame = resolvechoice1 , transformId = \"tfm-733f49ce1a5f98c4e2b8a9604ae4427368b08b07\" , transformation_ctx = \"findmatches2\" ) resolvechoice3 = ResolveChoice . apply ( frame = findmatches2 , choice = \"make_struct\" , transformation_ctx = \"resolvechoice3\" ) dropnullfield4 = DropNullFields . apply ( frame = resolvechoice3 , transformation_ctx = \"dropnullfield4\" ) df1 = dropnullfield4 . toDF () print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) Cell 4: custidlist = [ \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAJLKGBAAA\" , \"AAAAAAAAJLKGBAAA\" , \"AAAAAAAAPLNDAAAA\" , \"AAAAAAAANDCFBAAA\" , \"BAAAAAAAPLNDAAAA\" , \"BAAAAAAANDCFBAAA\" , \"AAAAAAAANKFAAAAA\" , \"BAAAAAAANKFAAAAA\" , \"AAAAAAAADFHGAAAA\" , \"BAAAAAAADFHGAAAA\" ] cols = [ 'c_customer_id' , 'c_current_addr_sk' , 'c_salutation' , 'c_first_name' , 'c_last_name' , 'c_birth_day' , 'c_birth_month' , 'c_birth_year' , 'c_birth_country' , 'c_login' , 'c_email_address' , 'match_id' ] print ( \"Duplicate count of matching IDs : \" + str ( df1 . count ())) df1 . filter ( df1 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) Cell 5: df2 = df1 . drop_duplicates ( subset = [ 'match_id' ]) print ( \"Distinct count of matching IDs : \" + str ( df2 . count ())) df2 . filter ( df2 . c_customer_id . isin ( custidlist ) ) . select ( * cols ) . orderBy ( \"match_id\" ) . show ( truncate = False ) ---> Wait for the last cell to finish and output the results. <<<<< write the right stuff","title":"2. Testing FindMatches Transform with Glue Studio Notebook"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#3-validating-and-deploying-the-findmatches-glue-job","text":"<<<<< write the right stuff ---> validate the results: --> Reference this blog: Reference This Blog: http://blog.zenof.ai/machine-learning-based-fuzzy-matching-using-aws-glue-ml-transforms/ \"However, the Find matches transform adds another column named match_id to identify matching records in the output. Rows with the same match_id are considered matching records.\" Once done explaining the Matching IDs logic, add a new cell to the notebook to sink the very final dataframe to S3 but DON'T RUN this cell!!! [DON'T RUN THIS CELL] Cell 6: new_df = df2 . select ( * cols ) . orderBy ( \"match_id\" ) new_df . coalesce ( 1 ) . write . option ( \"header\" , \"true\" ) . mode ( \"overwrite\" ) . csv ( \"s3://etl-ttt-demo-589541849248-us-east-2/etl-ttt-demo/output/ml-lab/topcustomers-dedup/\" ) [DON'T RUN THIS CELL] JUST SAVE THE JOB TO DEPLOY IT PROPERLY AND MOVE TO THE NEXT LAB] <<<<< write the right stuff","title":"3. Validating and Deploying the FindMatches Glue Job"},{"location":"06-StepFunctionsLab/06-step-functions-lab/","text":"WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS <<<<< write the right stuff [NEED TO CREATE A FOLDER NAMED (Cloud9): .../etl-ttt-demo/data_analytics_team_folder/ [SIMULATE A FILE BEING SHARED FROM THE ANALYTICS TEAM] [THIS FILE WILL TRIGGER THE STEP FUNCTION WORKFLOW WHICH CONSEQUENTLY WILL RUN THE FINDMATCHES ML VALIDATION JOB TO CLEAN UP THE DUPLICAIONS. AFTER THAT, STEP FUNCTION WORKFLOW WILL START THE CRAWLER TO CATALOG THE VALIDATED DATA WHICH WILL THEN BE SERVED AS THE SOURCE DATASET FOR THE ANALYTICS TEAM TO USE IN THE DATABREW LAB] [NEED TO CREATE A CRAWLER TO CRAWL THE GENERATED FILE - STEP FUNCTION WILL TRIGER IT - ADD THIS CRAWLER TO THE CFN TOO] <<<<< write the right stuff 1. Creating the Step Function Workflow \u00b6 <<<<< write the right stuff Write down the steps to create the workflow... First is a Glue Job: StateName: FindMatch Job JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To remove duplicates in the dataset using FIndMatches ML Transform\" Then the Glue Crawler: StateName: Crawl Dedup Data JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To crawl the data once the FindMatches job conclude the dedup higiene process\" STEP FUNCTION WORKFLOW - JSON GENERATED CODE SNIPPET { \"Comment\" : \"A description of my state machine\" , \"StartAt\" : \"Find Match Job\" , \"States\" : { \"Find Match Job\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::glue:startJobRun.sync\" , \"Parameters\" : { \"JobName\" : \"ml-lab-notebook-job\" }, \"Comment\" : \"To remove duplicates in the dataset using FIndMatches ML Transform\" , \"Next\" : \"Crawl Dedup Data\" }, \"Crawl Dedup Data\" : { \"Type\" : \"Task\" , \"End\" : true , \"Parameters\" : { \"Name\" : \"ml_final_crawler\" }, \"Resource\" : \"arn:aws:states:::aws-sdk:glue:startCrawler\" , \"Comment\" : \"To crawl the data after FindMatches dedup higiene process\" } } } State Machine Name: findmatches-ML-dedup-workflow Existing Role: AWSStepFunctionRole-etl-ttt-demo <<<<< write the right stuff 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI) \u00b6 <<<<< write the right stuff --- first the event rule ---- aws events put-rule \\ --name \"find-matches-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/data_analytics_team_folder/\\\"}] } \\ } \\ }\" --- then the event rule target ---- aws events put-targets \\ --rule find-matches-rule \\ --targets \"Id\"=\"stepfunction-workflow\",\"Arn\"=\"arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:findmatches-ML-dedup-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } [SIMMULATE THE FILE BEING UPLOADED BY A DIFFERENT TEAM TO CENTRAL BUCKET FOR FURTHER PROCESSING] aws s3 cp /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv s3://$BUCKET_NAME/etl-ttt-demo/data_analytics_team_folder/ <<<<< write the right stuff","title":"WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#1-creating-the-step-function-workflow","text":"<<<<< write the right stuff Write down the steps to create the workflow... First is a Glue Job: StateName: FindMatch Job JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To remove duplicates in the dataset using FIndMatches ML Transform\" Then the Glue Crawler: StateName: Crawl Dedup Data JobName: ml-lab-notebook-job Wait for task to complete - optional Next State: Add a next state Comment: \"To crawl the data once the FindMatches job conclude the dedup higiene process\" STEP FUNCTION WORKFLOW - JSON GENERATED CODE SNIPPET { \"Comment\" : \"A description of my state machine\" , \"StartAt\" : \"Find Match Job\" , \"States\" : { \"Find Match Job\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::glue:startJobRun.sync\" , \"Parameters\" : { \"JobName\" : \"ml-lab-notebook-job\" }, \"Comment\" : \"To remove duplicates in the dataset using FIndMatches ML Transform\" , \"Next\" : \"Crawl Dedup Data\" }, \"Crawl Dedup Data\" : { \"Type\" : \"Task\" , \"End\" : true , \"Parameters\" : { \"Name\" : \"ml_final_crawler\" }, \"Resource\" : \"arn:aws:states:::aws-sdk:glue:startCrawler\" , \"Comment\" : \"To crawl the data after FindMatches dedup higiene process\" } } } State Machine Name: findmatches-ML-dedup-workflow Existing Role: AWSStepFunctionRole-etl-ttt-demo <<<<< write the right stuff","title":"1. Creating the Step Function Workflow"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#2-creating-an-eventbridge-rule-target-for-the-step-function-workflow-via-cli","text":"<<<<< write the right stuff --- first the event rule ---- aws events put-rule \\ --name \"find-matches-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/data_analytics_team_folder/\\\"}] } \\ } \\ }\" --- then the event rule target ---- aws events put-targets \\ --rule find-matches-rule \\ --targets \"Id\"=\"stepfunction-workflow\",\"Arn\"=\"arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:findmatches-ML-dedup-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } [SIMMULATE THE FILE BEING UPLOADED BY A DIFFERENT TEAM TO CENTRAL BUCKET FOR FURTHER PROCESSING] aws s3 cp /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-topcustomer.csv s3://$BUCKET_NAME/etl-ttt-demo/data_analytics_team_folder/ <<<<< write the right stuff","title":"2. Creating an EventBridge Rule &amp; Target for the Step Function Workflow (via CLI)"},{"location":"07-DataBrewLab/07-databrew-lab/","text":"DATA QUALITY & DATA PREPARATION WITH AWS GLUE DATABREW <<<<< write the right stuff The following commands will blah blah lah blah <<<<<< 1. Creating Datasets & Profiling the Data (with Quality Rules) \u00b6 <<<<< write the right stuff [CREATE A DATASET FROM THE SHIVS THIRD DEMO (INCOME BAND AND CUSTOMER TABLE] ---> Create TopCustomer Dataset from Data Catalog ---> (Cataloged via crawler + step functions in the previous lab after removing duplicates with ML Transform job) ---> Create Income Band from RDS connection (delete income band from crawler - check template and lab instruction) [PROFILE THE CUSTOMER DATASET WITH DATA QUALITY RULES - 3-5 minutes for profile job to finish] --> Full dataset --> Output Settings: Navigate to -> s3://$BUCKET_NAME/etl-ttt-demo/output/databrew/jobprofiles/ [CREATE RULE SET] ---> TopCustomer-Ruleset [ADD RECOMMENDED QUALITY RULES + ANY OTHER I WANT] Rule Set Name: TopCustomer-Ruleset Associated Dataset: TopCustomers recommended rules: --> duplicates? (The rule will pass if dataset has duplicate rows count <= 1 ) --> missing values less than 2% custom rules: --> The rule will pass if c_email_address has length <= 5 FOR greater than or equal to 100% of rows [PROFILE INCOME_BAND DATASET WITH 1 QUALITY RULE] Full dataset Rule Name: Income Check for Negative Values Rule Summary: The rule will pass if ib_lower_bound , ib_upper_bound has values >= 0 FOR greater than or equal to 100% of rows 2. Working with DataBrew Recipes & Projects \u00b6 <<<<< write the right stuff [CREATE A PROJECT FOR INCOME_BAND DATASET] Project Name: TopCustomers-PerBand ---> show all the transformations available (quick overview) Choose: Join -> Choose Customer Dataset to join with A RIGHT JOIN --> Join by Income_Band Sk and Birthday_day (income band goes from 1-20 only) --> Remove ib_income_band_sk ---> To add the steps: -> Go to Recipes and import the recipe received from data analytics team -> Show all the steps in the recipe and the JSON original recipe. -> Publish the recipe -> Go back to the Project: ---> Import the Recipe into the project ---> Explain step validation (append/overwrite) ---> Reorder the Rename steps (17,18 and 19) ---> Create an Age field using DATE_DIFF only. ---> Drop the c_birth_column, c_login and match_id ---> Save and Publish this as a new recipe. ---> Run the Job and share the results. <<<<< write the right stuff","title":"DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW"},{"location":"07-DataBrewLab/07-databrew-lab/#1-creating-datasets-profiling-the-data-with-quality-rules","text":"<<<<< write the right stuff [CREATE A DATASET FROM THE SHIVS THIRD DEMO (INCOME BAND AND CUSTOMER TABLE] ---> Create TopCustomer Dataset from Data Catalog ---> (Cataloged via crawler + step functions in the previous lab after removing duplicates with ML Transform job) ---> Create Income Band from RDS connection (delete income band from crawler - check template and lab instruction) [PROFILE THE CUSTOMER DATASET WITH DATA QUALITY RULES - 3-5 minutes for profile job to finish] --> Full dataset --> Output Settings: Navigate to -> s3://$BUCKET_NAME/etl-ttt-demo/output/databrew/jobprofiles/ [CREATE RULE SET] ---> TopCustomer-Ruleset [ADD RECOMMENDED QUALITY RULES + ANY OTHER I WANT] Rule Set Name: TopCustomer-Ruleset Associated Dataset: TopCustomers recommended rules: --> duplicates? (The rule will pass if dataset has duplicate rows count <= 1 ) --> missing values less than 2% custom rules: --> The rule will pass if c_email_address has length <= 5 FOR greater than or equal to 100% of rows [PROFILE INCOME_BAND DATASET WITH 1 QUALITY RULE] Full dataset Rule Name: Income Check for Negative Values Rule Summary: The rule will pass if ib_lower_bound , ib_upper_bound has values >= 0 FOR greater than or equal to 100% of rows","title":"1. Creating Datasets &amp; Profiling the Data (with Quality Rules)"},{"location":"07-DataBrewLab/07-databrew-lab/#2-working-with-databrew-recipes-projects","text":"<<<<< write the right stuff [CREATE A PROJECT FOR INCOME_BAND DATASET] Project Name: TopCustomers-PerBand ---> show all the transformations available (quick overview) Choose: Join -> Choose Customer Dataset to join with A RIGHT JOIN --> Join by Income_Band Sk and Birthday_day (income band goes from 1-20 only) --> Remove ib_income_band_sk ---> To add the steps: -> Go to Recipes and import the recipe received from data analytics team -> Show all the steps in the recipe and the JSON original recipe. -> Publish the recipe -> Go back to the Project: ---> Import the Recipe into the project ---> Explain step validation (append/overwrite) ---> Reorder the Rename steps (17,18 and 19) ---> Create an Age field using DATE_DIFF only. ---> Drop the c_birth_column, c_login and match_id ---> Save and Publish this as a new recipe. ---> Run the Job and share the results. <<<<< write the right stuff","title":"2. Working with DataBrew Recipes &amp; Projects"}]}