{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ETL Migration & Modernization Train-The-Trainer Workshop - Hands On Practice!!! Note: To avoid running into unexpected issues, we strongly recommend running this labs on one of the following regions: US-EAST-2 (Ohio) US-WEST-1 (N. California) US-WEST-2.(Oregon)","title":"Home"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"** Setup **"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). Enter the hash provided to you. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation templates \"dayone\", \"emr-on-eks\" and \"smstudio\" are created.","title":"Setup"},{"location":"00-PreSteps/001-intro/","text":"Lab Pre-Steps If you are at an AWS hosted event (such as AWS re:Invent, AWS Summit, Immersions Day, on-site Workshop or other event hosted by AWS employees), you will be provided with a temporary Event Engine AWS account for the workshop. Please follow the AWS Event instructions to access your Event Engine AWS account. If you are running this workshop by yourself, your account must have the ability to create new IAM roles and scope other IAM permissions. Please follow Self Paced Labs instructions to prepare your AWS account.","title":"Intro"},{"location":"00-PreSteps/AWSEvent/002-ee-account-setup/","text":"PREPARING THE CLOUD9 ENVIROMENT You will be provided with an AWS account to run this workshop. The temporary account is being created using Event Engine. You will be provided a participant hash key to login to your temporary account. Follow these steps to start using your account: 1. Go to AWS Event Engine Portal 2. Enter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms & Login. Click on that button to continue. 3. Click on Email One-Time Password (OTP); You also have option to use your personal Amazon.com uid and password (Note - Not your AWS account credentials) 4. Provide your email address. 5. Provide the OTP you have received in your email. 6. Click AWS Console on the dashboard. 7. Take the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab. Now, you should be on the AWS Console page of your workshop environment. \u00b6 Note: If your event hasn\u2019t start yet you will get a message stating so, wait for around the time of the event and try again:","title":"Account Setup"},{"location":"00-PreSteps/AWSEvent/002-ee-account-setup/#now-you-should-be-on-the-aws-console-page-of-your-workshop-environment","text":"Note: If your event hasn\u2019t start yet you will get a message stating so, wait for around the time of the event and try again:","title":"Now, you should be on the AWS Console page of your workshop environment."},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/","text":"PREPARING THE CLOUD9 ENVIROMENT We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the AWS Cloud9 Console in your environment and you should see a Cloud9 Environment named etl-ttt-demo . Select this enviroment and click on the View details In th button. In the details page, under EC2 Instance , click on Go To Instance link. This will open a new tab on your browser and take you to the AWS EC2 Console . TIP: Leave the AWS Cloud9 Console tab open for now! In the AWS EC2 Console , select the Cloud9 EC2 Instance that starts with aws-cloud9- . Click on the Actions dropdown button, followed by Security , then click on Modify IAM Role . In the next screen's dropdown box select the role that contains -AWSEC2InstanceProfileTTT- and click Save . TIP: You can close the AWS EC2 Console tab now! Go back to the AWS Cloud9 Console tab and, while still on the Enviroment Details page, click on the Open IDE button to launch your AWS Cloud9 IDE environment tab. Tip: Now, you can close the AWS Cloud9 Console tab. Allow for the launched AWS Cloud9 Enviroment's setup to finish, then close the Welcome tab and click on the green plus icon (+) to add a New Terminal tab. This will create a new terminal tab and load a command-line terminal . From now on, you will use this terminal window throughout this workshop lab to execute all the AWS CLI commands and scripts. TIP: Keep this tab open for the entire workshop if you can! 1. Setting up Cloud9 Environment Variables \u00b6 During the workshop environment setup, a S3 Bucket is created for storing the Lab Paths, Lab Files and CloudTrail Logs . A RDS MySQL Instance also got created for you. Run the following set of commands to setup and print out the required Enviroment Variables such as the \\${BUCKET_NAME} for your S3 Bucket and \\$mysqlendpoint for the RDS MySQL Instance endpoint : AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint Note: You can save the output values elsewhere for easy reference if you want or always re-run the echo commands to print them out again. 2. Switching Cloud9 Role Credentials \u00b6 After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials which is controlled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far upper right), the Preferences tab will open. There, scroll to AWS Settings and disable the option AWS managed temporary credentials . Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the AWSEC2InstanceProfileTTT you setup earlier in for the Cloud9 EC2 instance . aws configure set region $AWS_REGION aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output. 3. Setting up Security Required Groups Inbound Rules \u00b6 Next, a new Inbound Rule will be added to the Default Security Group of this workshop which is tagged with the name etl-ttt-demo . This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the etl-ttt-demo's Security Group . Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX is the ID of each Security Group, the EC2's and the DefaultVPC's Security Group. [Optional]: You can go to your AWS VPC Console to verify that a new Inbound Rule has been added to the etl-ttt-demo , it should have two Inbound Rules now. 4. Installing Required Libraries (Boto3) \u00b6 In this next step, BOTO3 Library will be installed to your Cloud9 enviroment . That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the Amazon S3 workshop's bucket: etl-ttt-demo-\\${AWS_ACCOUNT_ID}-\\${AWS_REGION} - that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"Prepare the Enviroment"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#1-setting-up-cloud9-environment-variables","text":"During the workshop environment setup, a S3 Bucket is created for storing the Lab Paths, Lab Files and CloudTrail Logs . A RDS MySQL Instance also got created for you. Run the following set of commands to setup and print out the required Enviroment Variables such as the \\${BUCKET_NAME} for your S3 Bucket and \\$mysqlendpoint for the RDS MySQL Instance endpoint : AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint Note: You can save the output values elsewhere for easy reference if you want or always re-run the echo commands to print them out again.","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#2-switching-cloud9-role-credentials","text":"After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials which is controlled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far upper right), the Preferences tab will open. There, scroll to AWS Settings and disable the option AWS managed temporary credentials . Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the AWSEC2InstanceProfileTTT you setup earlier in for the Cloud9 EC2 instance . aws configure set region $AWS_REGION aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output.","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#3-setting-up-security-required-groups-inbound-rules","text":"Next, a new Inbound Rule will be added to the Default Security Group of this workshop which is tagged with the name etl-ttt-demo . This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the etl-ttt-demo's Security Group . Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX is the ID of each Security Group, the EC2's and the DefaultVPC's Security Group. [Optional]: You can go to your AWS VPC Console to verify that a new Inbound Rule has been added to the etl-ttt-demo , it should have two Inbound Rules now.","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/AWSEvent/003-cloud9-enviroment-setup/#4-installing-required-libraries-boto3","text":"In this next step, BOTO3 Library will be installed to your Cloud9 enviroment . That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the Amazon S3 workshop's bucket: etl-ttt-demo-\\${AWS_ACCOUNT_ID}-\\${AWS_REGION} - that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"4. Installing Required Libraries (Boto3)"},{"location":"00-PreSteps/SelfPaced/004-signup-aws/","text":"SIGN UP FOR AWS When you sign up for Amazon Web Services (AWS) , your AWS account is automatically signed up for all services in AWS, including Lake Formation . You are charged only for the services that you use. If you've already signed up for Amazon Web Services (AWS) and have a user with administrator access , you can move on to Deploy CloudFormation Template . If you haven't signed up for AWS account yet, or if you need assistance, follow the following instructions to set up your AWS account for the workshop. Sign Up for AWS \u00b6 To create an AWS account: 1. Open https://aws.amazon.com and then choose C reate an AWS Account . If you previously signed in to the AWS Management Console using AWS account root user credentials, choose Sign in to a different account. If you previously signed in to the console using IAM credentials, choose Sign-in using root account credentials. Then choose Create a new AWS account. 2. Follow the online instructions. Part of the sign-up procedure involves receiving a phone call and entering a verification code using the phone keypad. Note your AWS account number , because you'll need it for the next task.","title":"Sign Up for AWS"},{"location":"00-PreSteps/SelfPaced/004-signup-aws/#sign-up-for-aws","text":"To create an AWS account: 1. Open https://aws.amazon.com and then choose C reate an AWS Account . If you previously signed in to the AWS Management Console using AWS account root user credentials, choose Sign in to a different account. If you previously signed in to the console using IAM credentials, choose Sign-in using root account credentials. Then choose Create a new AWS account. 2. Follow the online instructions. Part of the sign-up procedure involves receiving a phone call and entering a verification code using the phone keypad. Note your AWS account number , because you'll need it for the next task.","title":"Sign Up for AWS"},{"location":"00-PreSteps/SelfPaced/005-create-iam-user/","text":"CREATE IAM USER Services in AWS, such as AWS Glue , require that you provide credentials when you access them, so that the service can determine whether you have permission to access its resources. The console requires your password. You can create access keys for your AWS account to access the command line interface or API . However, we don't recommend that you access AWS using the credentials for your AWS account ; we recommend that you use AWS Identity and Access Management (IAM) instead. Create an IAM user , and then add the user to an IAM group with administrative permissions or grant this user administrative permissions . You can then access AWS using a special URL and the credentials for the IAM user. If you signed up for AWS but have not created an IAM user for yourself, you can create one using the IAM console . If you aren't familiar with using the console, see Working with the AWS Management Console for an overview. To create an IAM user for yourself and add the user to an Administrators group: \u00b6 1. Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at IAM Console We strongly recommend that you adhere to the best practice of using the Administrator IAM user below and securely lock away the root user credentials . 2. Sign in as the root user only to perform a few account and service management tasks . 3. In the navigation pane of the console, choose Users , and then choose Add user . 4. For User name , type Administrator . 5. Select the check box next to AWS Management Console access , select Custom password , and then type the new user's password in the text box. You can optionally select Require password reset to force the user to create a new password the next time the user signs in. 6. Choose Next : Permissions . 7. On the Set permissions page , choose Add user to group . 7* .1 Choose Create group *. 8. In the Create group dialog box, for Group name type Administrators . 9. For Filter policies , select the check box for AWS managed - job function . 10. In the policy list, select the check box for AdministratorAccess . Then choose Create group . 11. Back in the list of groups, select the check box for your new group. Choose Refresh if necessary to see the group in the list. 12. Choose Next : Tags to add metadata to the user by attaching tags as key-value pairs . 13. Choose Next : Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . 14. You can use this same process to create more groups and users , and to give your users access to your AWS account resources . To sign in as this new IAM user , sign out of the AWS console , then use the following URL , where your_aws_account_id is your AWS account number without the hyphens (for example, if your AWS account number is 1234-5678-9012, your AWS account ID is 123456789012) : https://your_aws_account_id.signin.aws.amazon.com/console/ Enter the IAM user name (not your email address) and password that you just created. When you're signed in, the navigation bar displays \"your_user_name @ your_aws_account_id\". If you don't want the URL for your sign-in page to contain your AWS account ID , you can create an account alias . From the IAM console , choose Dashboard in the navigation pane. From the dashboard, choose Customize and enter an alias such as your company name. To sign in after you create an account alias , use the following URL : https://your_account_alias.signin.aws.amazon.com/console/ To verify the sign-in link for IAM users for your account, open the IAM console and check under IAM users sign-in link on the dashboard.","title":"Create IAM User"},{"location":"00-PreSteps/SelfPaced/005-create-iam-user/#to-create-an-iam-user-for-yourself-and-add-the-user-to-an-administrators-group","text":"1. Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at IAM Console We strongly recommend that you adhere to the best practice of using the Administrator IAM user below and securely lock away the root user credentials . 2. Sign in as the root user only to perform a few account and service management tasks . 3. In the navigation pane of the console, choose Users , and then choose Add user . 4. For User name , type Administrator . 5. Select the check box next to AWS Management Console access , select Custom password , and then type the new user's password in the text box. You can optionally select Require password reset to force the user to create a new password the next time the user signs in. 6. Choose Next : Permissions . 7. On the Set permissions page , choose Add user to group . 7* .1 Choose Create group *. 8. In the Create group dialog box, for Group name type Administrators . 9. For Filter policies , select the check box for AWS managed - job function . 10. In the policy list, select the check box for AdministratorAccess . Then choose Create group . 11. Back in the list of groups, select the check box for your new group. Choose Refresh if necessary to see the group in the list. 12. Choose Next : Tags to add metadata to the user by attaching tags as key-value pairs . 13. Choose Next : Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . 14. You can use this same process to create more groups and users , and to give your users access to your AWS account resources . To sign in as this new IAM user , sign out of the AWS console , then use the following URL , where your_aws_account_id is your AWS account number without the hyphens (for example, if your AWS account number is 1234-5678-9012, your AWS account ID is 123456789012) : https://your_aws_account_id.signin.aws.amazon.com/console/ Enter the IAM user name (not your email address) and password that you just created. When you're signed in, the navigation bar displays \"your_user_name @ your_aws_account_id\". If you don't want the URL for your sign-in page to contain your AWS account ID , you can create an account alias . From the IAM console , choose Dashboard in the navigation pane. From the dashboard, choose Customize and enter an alias such as your company name. To sign in after you create an account alias , use the following URL : https://your_account_alias.signin.aws.amazon.com/console/ To verify the sign-in link for IAM users for your account, open the IAM console and check under IAM users sign-in link on the dashboard.","title":"To create an IAM user for yourself and add the user to an Administrators group:"},{"location":"00-PreSteps/SelfPaced/006-deploy-cfn/","text":"DEPLOY CLOUDFORMATION TEMPLATE We will use CloudFormation to set up the AWS environment for the labs. AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources , and provision and manage them in an orderly and predictable fashion. We will create one CloudFormation stack which includes all the resources we will be using in the following labs. When you finish all labs, please be sure to delete the stack as per the Clean up section to avoid unnecessary usage charges. Click here to launch Workshop Cloudformation Click Next in Create stack screen, accept the default stack name etl-ttt-demo in Specify stack details screen and click Next , Click Next in Configure stack options screen, check the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources with custom names . and click Create stack . You should see a new CloudFormation stack with the name etl-ttt-demo being created. Wait for the status of the stack to be CREATE_COMPLETE before moving on. You can click on the stack name and go to Resource tab to check what AWS resoures were created as part of the CloudFormation stack . You should expect to wait approximately 10 minutes for the stack to move to the CREATE_COMPLETE state.","title":"Deploy CloudFormation"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/","text":"PREPARING THE CLOUD9 ENVIROMENT We will use AWS Cloud9 to run shell commands, edit and run Python scripts for the labs. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It combines the rich code editing features of an IDE such as code completion, hinting, and step-through debugging, with access to a full Linux server for running and storing code. Go to the AWS Cloud9 Console in your environment and you should see a Cloud9 Environment named etl-ttt-demo . Select this enviroment and click on the View details In th button. In the details page, under EC2 Instance , click on Go To Instance link. This will open a new tab on your browser and take you to the AWS EC2 Console . TIP: Leave the AWS Cloud9 Console tab open for now! In the AWS EC2 Console , select the Cloud9 EC2 Instance that starts with aws-cloud9- . Click on the Actions dropdown button, followed by Security , then click on Modify IAM Role . In the next screen's dropdown box select the role that contains -AWSEC2InstanceProfileTTT- and click Save . TIP: You can close the AWS EC2 Console tab now! Go back to the AWS Cloud9 Console tab and, while still on the Enviroment Details page, click on the Open IDE button to launch your AWS Cloud9 IDE environment tab. Tip: Now, you can close the AWS Cloud9 Console tab. Allow for the launched AWS Cloud9 Enviroment's setup to finish, then close the Welcome tab and click on the green plus icon (+) to add a New Terminal tab. This will create a new terminal tab and load a command-line terminal . From now on, you will use this terminal window throughout this workshop lab to execute all the AWS CLI commands and scripts. TIP: Keep this tab open for the entire workshop if you can! 1. Setting up Cloud9 Environment Variables \u00b6 During the workshop environment setup, a S3 Bucket is created for storing the Lab Paths, Lab Files and CloudTrail Logs . A RDS MySQL Instance also got created for you. Run the following set of commands to setup and print out the required Enviroment Variables such as the \\${BUCKET_NAME} for your S3 Bucket and \\$mysqlendpoint for the RDS MySQL Instance endpoint : AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint Note: You can save the output values elsewhere for easy reference if you want or always re-run the echo commands to print them out again. 2. Switching Cloud9 Role Credentials \u00b6 After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials which is controlled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far upper right), the Preferences tab will open. There, scroll to AWS Settings and disable the option AWS managed temporary credentials . Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the AWSEC2InstanceProfileTTT you setup earlier in for the Cloud9 EC2 instance . aws configure set region $AWS_REGION aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output. 3. Setting up Security Required Groups Inbound Rules \u00b6 Next, a new Inbound Rule will be added to the Default Security Group of this workshop which is tagged with the name etl-ttt-demo . This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the etl-ttt-demo's Security Group . Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX is the ID of each Security Group, the EC2's and the DefaultVPC's Security Group. [Optional]: You can go to your AWS VPC Console to verify that a new Inbound Rule has been added to the etl-ttt-demo , it should have two Inbound Rules now. 4. Installing Required Libraries (Boto3) \u00b6 In this next step, BOTO3 Library will be installed to your Cloud9 enviroment . That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the Amazon S3 workshop's bucket: etl-ttt-demo-\\${AWS_ACCOUNT_ID}-\\${AWS_REGION} - that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"Prepare the Enviroment"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#1-setting-up-cloud9-environment-variables","text":"During the workshop environment setup, a S3 Bucket is created for storing the Lab Paths, Lab Files and CloudTrail Logs . A RDS MySQL Instance also got created for you. Run the following set of commands to setup and print out the required Enviroment Variables such as the \\${BUCKET_NAME} for your S3 Bucket and \\$mysqlendpoint for the RDS MySQL Instance endpoint : AWS_ACCOUNT_ID = ` aws sts get-caller-identity --query Account --output text ` AWS_REGION = ` aws configure get region ` BUCKET_NAME = etl-ttt-demo- ${ AWS_ACCOUNT_ID } - ${ AWS_REGION } mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) echo \"export BUCKET_NAME=\\\" ${ BUCKET_NAME } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_REGION=\\\" ${ AWS_REGION } \\\"\" >> /home/ec2-user/.bashrc echo \"export AWS_ACCOUNT_ID=\\\" ${ AWS_ACCOUNT_ID } \\\"\" >> /home/ec2-user/.bashrc echo ${ BUCKET_NAME } echo ${ AWS_REGION } echo ${ AWS_ACCOUNT_ID } echo $mysqlendpoint Note: You can save the output values elsewhere for easy reference if you want or always re-run the echo commands to print them out again.","title":"1. Setting up Cloud9 Environment Variables"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#2-switching-cloud9-role-credentials","text":"After running the first set of commands, you need to disable the auto management of AWS Temporary Credentials which is controlled by Cloud9 by default. In your Cloud9 Enviroment click on the Preferences (Gear Icon on the far upper right), the Preferences tab will open. There, scroll to AWS Settings and disable the option AWS managed temporary credentials . Once done, close the Preferences tab and in your Cloud9 terminal tab run the following commands to validate that Cloud9 is now using the AWSEC2ServiceRole-etl-ttt-demo role which belongs to the AWSEC2InstanceProfileTTT you setup earlier in for the Cloud9 EC2 instance . aws configure set region $AWS_REGION aws sts get - caller - identity -- query Arn | grep AWSEC2ServiceRole - etl - ttt - demo - q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" You should see your current region and a message \"IAM role valid\" in the output.","title":"2. Switching Cloud9 Role Credentials"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#3-setting-up-security-required-groups-inbound-rules","text":"Next, a new Inbound Rule will be added to the Default Security Group of this workshop which is tagged with the name etl-ttt-demo . This new Inboud Rule is to allow traffic from the AWS Cloud9 Instance's Security Group to the resources in the etl-ttt-demo's Security Group . Run the following commands to do that: ref_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `aws-cloud9-etl-ttt-demo`) == `true`].GroupId' --output text ) target_sg = $( aws ec2 describe-security-groups --query 'SecurityGroups[?contains(GroupName, `DefaultVPCSecurityGroup`) == `true`].GroupId' --output text ) aws ec2 authorize-security-group-ingress --group-id $target_sg --protocol -1 --source-group $ref_sg echo \"Source SG:\" $ref_sg \"has been added to Target SG:\" $target_sg You should see a message \" Source SG: sg-XXXXX has been added to Target SG: sg-XXXXX \" where XXXXX is the ID of each Security Group, the EC2's and the DefaultVPC's Security Group. [Optional]: You can go to your AWS VPC Console to verify that a new Inbound Rule has been added to the etl-ttt-demo , it should have two Inbound Rules now.","title":"3. Setting up Security Required Groups Inbound Rules"},{"location":"00-PreSteps/SelfPaced/007-cloud9-enviroment-setup/#4-installing-required-libraries-boto3","text":"In this next step, BOTO3 Library will be installed to your Cloud9 enviroment . That step is required in order to support running the Kinesis Streaming Python script during the Part 4 - Orchestration & Data Analysis - of this workshop. Run the following command to do so: sudo pip3 install boto3 Wait untill you see a message that says: \"Successfully installed boto3-1.22.9 botocore-1.25.9 s3transfer-0.5.2\" You are finished with the initial setup of the workshop environment. Take some time the explore the commands you ran here and to check the resources (such as the Amazon S3 workshop's bucket: etl-ttt-demo-\\${AWS_ACCOUNT_ID}-\\${AWS_REGION} - that have been created for you as part of the CloudFormation Template execution. Once you are ready you can move on to Part 1 - Working with TPC-DS Data & RDS MySQL Database!","title":"4. Installing Required Libraries (Boto3)"},{"location":"000-Intro/000-intro/","text":"ETL MIGRATION & MODERNIZATION TRAIN-THE-TRAINER WORKSHOP This workshop contains a set of seven hands-on labs which complementes the Day One of the ETL Migration & Modernization - Train The Trainer Workshop. Participants will build an end-to-end ETL Pipeline that will...[add details of the pipeline...] After completing the labs, the participants will have a high level understand of AWS Glue core capabilities as well as they will be able to demonstrate most of the Glue core capabilities. WORKSHOP PARTS & STEPS Part 0 - PRE STEPS \u00b6 1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3) Part 1 - TPCDS & RDS MySQL \u00b6 1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3 Part 2 - AWS GLUE DISCOVERY COMPONENTS (Databases, Tables, Connections, Crawlers and Classifiers) \u00b6 1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Testing and running pre-created Glue Resources (Connection & MySQL-RDS-Crawler) 3. Creating new Glue Resources (Crawler & Classifier) Part 3 - GLUE (STUDIO) STREAMING \u00b6 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!) Part 4 - ORCHESTRATION & DATA ANALYSIS \u00b6 1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Triggering Orchestration & Following The Flow 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog Part 5 - MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOKS \u00b6 0. (Pre Steps) - Understading & Setting up the Resources for the ML Lab 1. Creating and Training the Glue FindMatches ML Transform 2. Testing FindMatches Transform with Glue Studio Notebook 3. Deploying & Running a FindMatches Glue Job Part 6 - WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS \u00b6 1. Creating the Step Function Workflow 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI) Part 7 - DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW \u00b6 1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Working with DataBrew Recipes & Projects","title":"Introduction"},{"location":"000-Intro/000-intro/#part-0-pre-steps","text":"1. Setting up Cloud9 Environment Variables 2. Switching Cloud9 Role Credentials 3. Setting up Security Required Groups Inbound Rules 4. Installing Required Libraries (Boto3)","title":"Part 0 - PRE STEPS"},{"location":"000-Intro/000-intro/#part-1-tpcds-rds-mysql","text":"1. Preparing & Generating TPCDS Dataset 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3","title":"Part 1 - TPCDS &amp; RDS MySQL"},{"location":"000-Intro/000-intro/#part-2-aws-glue-discovery-components-databases-tables-connections-crawlers-and-classifiers","text":"1. Understanding the Glue Resources Provided (CloudFormation Resources) 2. Testing and running pre-created Glue Resources (Connection & MySQL-RDS-Crawler) 3. Creating new Glue Resources (Crawler & Classifier)","title":"Part 2 - AWS GLUE DISCOVERY COMPONENTS (Databases, Tables, Connections, Crawlers and Classifiers)"},{"location":"000-Intro/000-intro/#part-3-glue-studio-streaming","text":"1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) 3. Creating the Glue Streaming Job (Cloning Jobs!)","title":"Part 3 - GLUE (STUDIO) STREAMING"},{"location":"000-Intro/000-intro/#part-4-orchestration-data-analysis","text":"1. Understanding the Orchestration Flow 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) 3. Creating Event Bridge Rule and Target (via CLI) 4. Triggering Orchestration & Following The Flow 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog","title":"Part 4 - ORCHESTRATION &amp; DATA ANALYSIS"},{"location":"000-Intro/000-intro/#part-5-machine-learning-with-glue-glue-studio-notebooks","text":"0. (Pre Steps) - Understading & Setting up the Resources for the ML Lab 1. Creating and Training the Glue FindMatches ML Transform 2. Testing FindMatches Transform with Glue Studio Notebook 3. Deploying & Running a FindMatches Glue Job","title":"Part 5 - MACHINE LEARNING WITH GLUE &amp; GLUE STUDIO NOTEBOOKS"},{"location":"000-Intro/000-intro/#part-6-workflow-orchestration-with-aws-step-functions","text":"1. Creating the Step Function Workflow 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI)","title":"Part 6 - WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS"},{"location":"000-Intro/000-intro/#part-7-data-quality-preparation-with-aws-glue-databrew","text":"1. Creating Datasets & Profiling the Data (with Quality Rules) 2. Working with DataBrew Recipes & Projects","title":"Part 7 - DATA QUALITY &amp; PREPARATION WITH AWS GLUE DATABREW"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/","text":"WORKING WITH TPC-DS DATA & RDS MYSQL DATABASE Welcome to Part 1! In the Part 1 - Working with TPC-DS Data & RDS MySQL Database - you will be installing and configuring everything that is required to prepare and load TPC-DS* data into a RDS MySQL Database Instance in order to supply the necessary dataset samples for each piece of the ETL Train The Trainer Workshop . 1. Preparing & Generating TPCDS Dataset \u00b6 TPC-DS - TPC data is used for a decision support benchmark. When you load this data the schema tpcds is updated with sample data. For more information about the tpcds data, see TPC-DS . Before you can install and generate TPC-Data , you need to build the necessary folders and download the necessary files. Run the following command to create the base level directory for the ETL Train the Trainer Workshop and to download the TCP-DS tool to your local AWS Cloud9 Enviroment : mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip Next, run the following commands to unzip the previous downloaded (.zip) file and get into the DSGen software's directory to install the tool. unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql The last command ( head ) prints out the TPC-DS sql script for the tables' schemas to validate the instalation of the tool. Now, let's run the DSGen software to generate the sample data for all the TPC-DS tables and store the sample datasets in a temporary directory . TIP: This should take about 3 minutes to complete! mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt [Optional]: (Performing this optional step should allow for enough time for the previous step to complete!) While the above command is running, you can proactively test the reacheability of your RDS MySQL Database Instance for access coming from your AWS Cloud9 Enviroment . To do that, run the following command in a NEW terminal tab ( + icon): sudo yum -y install telnet mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) telnet $mysqlendpoint 3306 TIP: Close this additional Terminal tab once Telnet command succeed and go back to the original one ! 2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset \u00b6 Once the dataset samples get generated, it is time to load the data into the RDS MySQL Database . But first, run the following commands to create the TPC-DS tables' schemas in the MySQL database : tpcds_script_path = ~/environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" The last output should be a list of all the tables that have been created in the database. Now, it is time to load the data into the tables. To easily do that, you can run the following commands to download and run the a Shell Script that does all the loading process into the RDS MySQL Database ! TIP: This should take about 3 minutes to complete! aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint You can see all the tables getting loaded one by one... And here's a snippet of the Shell Script code. NOTE: You don't have to create it but if you want just to have a look at the script, simply run the following commands: cd /tmp/dsd/ cat load_TPC-DS_MySQL.sh 3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3 \u00b6 Finally, running the following commands will: Extract the 3 required tables ( web_page, customers and income_band ) from the MySQL database ; Save these tables in the local temporary directory (in CSV format ); And, subsequently, upload these CSV files into the Workshop's S3 Bucket's path : \" .../etl-ttt-demo/csv_tables/\" mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ You are finished populating the RDS MySQL Database with TPC-DS sample data . Now, feel free to explore the Amazon S3 workshop's bucket again ( etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) and check the path and files inside the etl-ttt-demo/csv_tables/ path. Once you are ready you can move on to Part 2 - AWS GLUE COMPONENTS!","title":"TPCDS & RDS MySQL Lab"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#1-preparing-generating-tpcds-dataset","text":"TPC-DS - TPC data is used for a decision support benchmark. When you load this data the schema tpcds is updated with sample data. For more information about the tpcds data, see TPC-DS . Before you can install and generate TPC-Data , you need to build the necessary folders and download the necessary files. Run the following command to create the base level directory for the ETL Train the Trainer Workshop and to download the TCP-DS tool to your local AWS Cloud9 Enviroment : mkdir -p ~/environment/ttt-demo cd ~//environment/ttt-demo// aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/16d7423c-23d3-4185-8ab4-5b002ec51153-tpc-ds-tool.zip tpc-ds-tool.zip Next, run the following commands to unzip the previous downloaded (.zip) file and get into the DSGen software's directory to install the tool. unzip tpc-ds-tool.zip cd DSGen-software-code-3.2.0rc1/tools/ make head -200 tpcds.sql The last command ( head ) prints out the TPC-DS sql script for the tables' schemas to validate the instalation of the tool. Now, let's run the DSGen software to generate the sample data for all the TPC-DS tables and store the sample datasets in a temporary directory . TIP: This should take about 3 minutes to complete! mkdir -p /tmp/dsd/csv_tables/ ./dsdgen -scale 1 -dir /tmp/dsd -parallel 2 -child 1 cd /tmp/dsd/ ls -lrt [Optional]: (Performing this optional step should allow for enough time for the previous step to complete!) While the above command is running, you can proactively test the reacheability of your RDS MySQL Database Instance for access coming from your AWS Cloud9 Enviroment . To do that, run the following command in a NEW terminal tab ( + icon): sudo yum -y install telnet mysqlendpoint = $( aws cloudformation describe-stacks --query 'Stacks[*].Outputs[?OutputKey==`MySQLEndpoint`].OutputValue | [0] | [0]' --output text ) telnet $mysqlendpoint 3306 TIP: Close this additional Terminal tab once Telnet command succeed and go back to the original one !","title":"1. Preparing &amp; Generating TPCDS Dataset"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#2-populating-the-amazon-rds-mysql-database-with-tpcds-dataset","text":"Once the dataset samples get generated, it is time to load the data into the RDS MySQL Database . But first, run the following commands to create the TPC-DS tables' schemas in the MySQL database : tpcds_script_path = ~/environment/ttt-demo//DSGen-software-code-3.2.0rc1/tools/tpcds.sql mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds < $tpcds_script_path mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"show tables\" The last output should be a list of all the tables that have been created in the database. Now, it is time to load the data into the tables. To easily do that, you can run the following commands to download and run the a Shell Script that does all the loading process into the RDS MySQL Database ! TIP: This should take about 3 minutes to complete! aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/load_TPC-DS_MySQL.sh . . load_TPC-DS_MySQL.sh $mysqlendpoint You can see all the tables getting loaded one by one... And here's a snippet of the Shell Script code. NOTE: You don't have to create it but if you want just to have a look at the script, simply run the following commands: cd /tmp/dsd/ cat load_TPC-DS_MySQL.sh","title":"2. Populating the Amazon RDS-MySQL Database with TPCDS Dataset"},{"location":"01-MySQL-TPCDS/01-mysql-tpcds/#3-unloading-tables-in-csv-from-rds-mysql-database-and-uploading-to-s3","text":"Finally, running the following commands will: Extract the 3 required tables ( web_page, customers and income_band ) from the MySQL database ; Save these tables in the local temporary directory (in CSV format ); And, subsequently, upload these CSV files into the Workshop's S3 Bucket's path : \" .../etl-ttt-demo/csv_tables/\" mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from web_page\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/web_page.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from customer\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/customer.csv mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds --batch -e \"select * from income_band\" | sed 's/\\t/\",\"/g;s/^/\"/;s/$/\"/;s/\\n//g' > /tmp/dsd/csv_tables/income_band.csv aws s3 cp --recursive /tmp/dsd/csv_tables/ s3:// $BUCKET_NAME /etl-ttt-demo/csv_tables/ You are finished populating the RDS MySQL Database with TPC-DS sample data . Now, feel free to explore the Amazon S3 workshop's bucket again ( etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} ) and check the path and files inside the etl-ttt-demo/csv_tables/ path. Once you are ready you can move on to Part 2 - AWS GLUE COMPONENTS!","title":"3. Unloading Tables (in CSV) from RDS MySQL Database and Uploading to S3"},{"location":"02-GlueComponentsLab/02-glue-components-lab/","text":"AWS GLUE DISCOVERY COMPONENTS (Databases, Tables, Connections, Crawlers And Classifiers) Welcome to Part 2! In the Part 2 - AWS Glue Discovery Componets - of this ETL Train The Trainer workshop , you will understand the AWS Glue Data Catalog and all the Glue resources associated with it. At first, you will explore all the Glue resources created for you as part of the CloudFormation Template . Then, you will create and run couple more Glue Resources that will be required for the subsequent parts of the workshop. The Glue components that you work with during this lab include Glue Databases, Tables, Connection, Crawlers and Classifiers . 1. Understanding the Glue Resources Provided (CloudFormation Resources) \u00b6 To browse through all the Glue provided resources first go to the AWS Glue Console by searching for it in the AWS Management Console ( Make sure you are in the right region where you started the lab! ). In the far left menu of the AWS Glue Console you can access all your resources like Database, Tables, Crawlers, Jobs, etc . Databases \u00b6 Definition: A database is a set of associated table definitions, organized into a logical group. To see all your databases, click on Databases . There, you should see 2 databases , the Default - which, as the name suggests, is the default database for Glue in this region - and the glue_ttt_demo_db which is the database where you will be creating and storing all the workshop's tables metadata for all the labs. Tables \u00b6 Definition: A table is the metadata definition that represents your data, including its schema. A table can be used as a source or target in a job definition. To see all your databases, click on Tables , under Databases . There, you should see just one table named web-page-streaming-table . This table has been created as part of the CloudFormation template and you will explore this table in details in the subsequent Part 3 - Glue (Studio) Streaming - lab of this workshop. Connections \u00b6 Definition: A connection contains the properties needed to connect to your data. To see all your connections, click on Connections , under Databases . There, you should see only one connection named rds-mysql-connection which we will explore and test in the next step of this lab. Crawlers \u00b6 Definition: A crawler connects to a data store, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in your data catalog. To see all your crawlers, click on Crawlers . There, you should see 3 crawlers which were created as part of the CloudFormation template : - ml-bootstrap-crawler : This crawler is to bootstrap a table that is required for the Part 5 - Machine Learning with Glue & Glue Studio Notebooks . You will explore it later. - ml-sample-cust-crawler : This crawler is to crawl a sample dataset that is required for the Part 5 - Machine Learning with Glue & Glue Studio Notebooks . You will explore it later. - mysql-rds-crawler : This crawler is to crawl RDS MySQL customer's table . It uses a the above Glue Connection reach the RDS MySQL database instance . You will explore it in the next step. Classifiers \u00b6 Definition: A classifier determines the schema of your data. You can use the AWS Glue built-in classifiers or write your own. To see all your classifiers, click on Classifiers , under Crawlers . There is no classifiers created yet but you will be creating one in the third step step of this lab. 2. Testing and running pre-created Glue Resources (Glue Connection & Glue Crawler for MySQL RDS) \u00b6 Now, let's run a Crawler and verify the tables it will create. But first, let's confirm that the connection that the crawler is going to use is working properly. Click on Connections , then click on the blue name rds-mysql-connection to open the details of that connection. You can see that the connection has a JDBC URL that points to the tpcds database that you created earlier in the RDS MySQL . The JDBC URL uses the RDS instance's endpoint . The connection also has the right VPC and Subnet configured as well as a Security Group . Note 1: As part of the CloudFormation template , this Security Group has been added to an Inboud Rule in the RDS Instance's Security Group which allows traffic on port 3306 (MySQL's default port) of the RDS instance . Click on Connections again, then check rds-mysql-connection connections box and click on Test Connection . In the pop-up screen, choose the required IAM Role to perform the connection test ( AWSGlueServiceRole-etl-ttt-demo ). Finally, click on Test Connection . Note 2: The AWSGlueServiceRole-etl-ttt-demo has been created as part of CloudFormation template and has all the required permissions for all the labs in this workshop. While the connection test is running, you will notice the following banner at the: You don't need to wait for it to finish, you can now click on Crawlers and start exploring the mysql-rds-crawler by click on its blue name. As you can see, this crawler uses the aforementioned Glue Connection to crawl the path \"tpcds/customer\" which is the database/table that you created and loaded data earlier. From this screen, just click on Run Crawler at the top. By starting this crawler, you will notice its Status column progressing from Ready -> Starting -> Xmin elapsed -> Stopping -> Ready again. TIP: This crawler should take about 2-3 minutes to complete so move to the next step ( Step3 ) then come back here in about 3 minutes to verify the results of running the crawler! At this point, the previous connection test should have already succeeded. You will see a green banner at top saying \"rds-mysql-connection connected successfully to your instance.\" . At the crawler's successfull completion, you should also see, in the Tables added column, that 1 table has been added . Go to Databases > Tables on the left menu to check the 1 table created . You will see that, apart from the streaming table, there will be another table named rds_crawled_tpcds_customer created in the glue_ttt_demo_db in which Location says \"tpcds.customers\" and Classification says \"mysql\" . Explore this table by clicking on its blue name. 3. Creating new Glue Resources (New Crawler & Classifier) \u00b6 While the above mysql-rds-crawler is running (if you followed the previous tip), it is time to create couple more resources in Glue. First is a Glue Custom CSV Classifier . Create Classifier To create a classifier, click on Classifiers in the far left menu, then click on Add Classifier . Fill out the details with the following details: Classifier name : My-Custom-CSV-Classifier Classifier type : CSV Column delimiter : Comma (,) Quote symbol : Double-quote (\") Column headings : Has headings (headings list) : c_full_name,c_email_address,total_clicks Click on the Create button at the bottom of pop-up page. A new classifier will be added to the list of classifiers. Create Crawler Next, as an advanced step, you are going to create a new crawler to crawl the path of the future Glue Streaming job's output that you are going to develop in Part 3 - Glue (Studio) Streaming. To create a crawler, click on Crawlers in the far left menu, then click on Add Crawler . Fill out the details on each screen as following details: 1. On Add information about your crawler page, provide a name for the new Crawler such as crawl-streammed-data . Then, expand the option that says \"Tags, description, security configuration, and classifiers (optional)\" and scroll to the bottom to see the list of custom classifiers on the right. Click on Add in front of My-Custom-CSV-Classifier . It will appear on the left side as well now. Click Next . 2. On Specify crawler source type page, simply click Next . 3. On Add a data store page, under Include path , write: s3://\\${BUCKET_NAME}/etl-ttt-demo/output/gluestreaming/total_clicks/ . (Make sure you replace \\${BUCKET_NAME} ) TIP: This path doesn't exist yet so switch back quickly to your Cloud9 enviroment tab and run the following command to build the full path string that you need. Copy it and paste it in the Include path field above: echo s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/gluestreaming/total_clicks/ 4. Still on Add a data store page, under Sample Size (optional) choose 1 . Then, expand where it says \"Exclude patterns (optional)\" and in the Exclude patterns field add **00001 and hit Next (twice). ( Don't miss the 2 asterisks ) 5. On Add another data store page, choose No and click Next . 6. On Choose an IAM role page, click Choose an existing IAM role and pick the role AWSGlueServiceRole-etl-ttt-demo , then click Next . 7. On Create a schedule for this crawler page, just click Next . 8. On Configure the crawler's output page, choose glue_ttt_demo_db from the Database dropdown list. Then, expand where it says \"Grouping behavior for S3 data (optional)\" and check the box \"Create a single schema for each S3 path\" . Click Next . 9. Review everything in the last page and click Finish . !!! !!! DO NOT RUN THIS CRAWLER YET !!! !!! You are finished populating, reviewing and setting up new Glue Resources . Go back to the Crawler you let running to explore the rds_crawled_tpcds_customer table it created. Once you are ready you can move on to Part 3 - Glue (Studio) Streaming!","title":"GLUE DATABASE, TABLES, CONNECTIONS, CRAWLERS, CLASSIFIERS"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#1-understanding-the-glue-resources-provided-cloudformation-resources","text":"To browse through all the Glue provided resources first go to the AWS Glue Console by searching for it in the AWS Management Console ( Make sure you are in the right region where you started the lab! ). In the far left menu of the AWS Glue Console you can access all your resources like Database, Tables, Crawlers, Jobs, etc .","title":"1. Understanding the Glue Resources Provided (CloudFormation Resources)"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#databases","text":"Definition: A database is a set of associated table definitions, organized into a logical group. To see all your databases, click on Databases . There, you should see 2 databases , the Default - which, as the name suggests, is the default database for Glue in this region - and the glue_ttt_demo_db which is the database where you will be creating and storing all the workshop's tables metadata for all the labs.","title":"Databases"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#tables","text":"Definition: A table is the metadata definition that represents your data, including its schema. A table can be used as a source or target in a job definition. To see all your databases, click on Tables , under Databases . There, you should see just one table named web-page-streaming-table . This table has been created as part of the CloudFormation template and you will explore this table in details in the subsequent Part 3 - Glue (Studio) Streaming - lab of this workshop.","title":"Tables"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#connections","text":"Definition: A connection contains the properties needed to connect to your data. To see all your connections, click on Connections , under Databases . There, you should see only one connection named rds-mysql-connection which we will explore and test in the next step of this lab.","title":"Connections"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#crawlers","text":"Definition: A crawler connects to a data store, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in your data catalog. To see all your crawlers, click on Crawlers . There, you should see 3 crawlers which were created as part of the CloudFormation template : - ml-bootstrap-crawler : This crawler is to bootstrap a table that is required for the Part 5 - Machine Learning with Glue & Glue Studio Notebooks . You will explore it later. - ml-sample-cust-crawler : This crawler is to crawl a sample dataset that is required for the Part 5 - Machine Learning with Glue & Glue Studio Notebooks . You will explore it later. - mysql-rds-crawler : This crawler is to crawl RDS MySQL customer's table . It uses a the above Glue Connection reach the RDS MySQL database instance . You will explore it in the next step.","title":"Crawlers"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#classifiers","text":"Definition: A classifier determines the schema of your data. You can use the AWS Glue built-in classifiers or write your own. To see all your classifiers, click on Classifiers , under Crawlers . There is no classifiers created yet but you will be creating one in the third step step of this lab.","title":"Classifiers"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#2-testing-and-running-pre-created-glue-resources-glue-connection-glue-crawler-for-mysql-rds","text":"Now, let's run a Crawler and verify the tables it will create. But first, let's confirm that the connection that the crawler is going to use is working properly. Click on Connections , then click on the blue name rds-mysql-connection to open the details of that connection. You can see that the connection has a JDBC URL that points to the tpcds database that you created earlier in the RDS MySQL . The JDBC URL uses the RDS instance's endpoint . The connection also has the right VPC and Subnet configured as well as a Security Group . Note 1: As part of the CloudFormation template , this Security Group has been added to an Inboud Rule in the RDS Instance's Security Group which allows traffic on port 3306 (MySQL's default port) of the RDS instance . Click on Connections again, then check rds-mysql-connection connections box and click on Test Connection . In the pop-up screen, choose the required IAM Role to perform the connection test ( AWSGlueServiceRole-etl-ttt-demo ). Finally, click on Test Connection . Note 2: The AWSGlueServiceRole-etl-ttt-demo has been created as part of CloudFormation template and has all the required permissions for all the labs in this workshop. While the connection test is running, you will notice the following banner at the: You don't need to wait for it to finish, you can now click on Crawlers and start exploring the mysql-rds-crawler by click on its blue name. As you can see, this crawler uses the aforementioned Glue Connection to crawl the path \"tpcds/customer\" which is the database/table that you created and loaded data earlier. From this screen, just click on Run Crawler at the top. By starting this crawler, you will notice its Status column progressing from Ready -> Starting -> Xmin elapsed -> Stopping -> Ready again. TIP: This crawler should take about 2-3 minutes to complete so move to the next step ( Step3 ) then come back here in about 3 minutes to verify the results of running the crawler! At this point, the previous connection test should have already succeeded. You will see a green banner at top saying \"rds-mysql-connection connected successfully to your instance.\" . At the crawler's successfull completion, you should also see, in the Tables added column, that 1 table has been added . Go to Databases > Tables on the left menu to check the 1 table created . You will see that, apart from the streaming table, there will be another table named rds_crawled_tpcds_customer created in the glue_ttt_demo_db in which Location says \"tpcds.customers\" and Classification says \"mysql\" . Explore this table by clicking on its blue name.","title":"2. Testing and running pre-created Glue Resources (Glue Connection &amp; Glue Crawler for MySQL RDS)"},{"location":"02-GlueComponentsLab/02-glue-components-lab/#3-creating-new-glue-resources-new-crawler-classifier","text":"While the above mysql-rds-crawler is running (if you followed the previous tip), it is time to create couple more resources in Glue. First is a Glue Custom CSV Classifier . Create Classifier To create a classifier, click on Classifiers in the far left menu, then click on Add Classifier . Fill out the details with the following details: Classifier name : My-Custom-CSV-Classifier Classifier type : CSV Column delimiter : Comma (,) Quote symbol : Double-quote (\") Column headings : Has headings (headings list) : c_full_name,c_email_address,total_clicks Click on the Create button at the bottom of pop-up page. A new classifier will be added to the list of classifiers. Create Crawler Next, as an advanced step, you are going to create a new crawler to crawl the path of the future Glue Streaming job's output that you are going to develop in Part 3 - Glue (Studio) Streaming. To create a crawler, click on Crawlers in the far left menu, then click on Add Crawler . Fill out the details on each screen as following details: 1. On Add information about your crawler page, provide a name for the new Crawler such as crawl-streammed-data . Then, expand the option that says \"Tags, description, security configuration, and classifiers (optional)\" and scroll to the bottom to see the list of custom classifiers on the right. Click on Add in front of My-Custom-CSV-Classifier . It will appear on the left side as well now. Click Next . 2. On Specify crawler source type page, simply click Next . 3. On Add a data store page, under Include path , write: s3://\\${BUCKET_NAME}/etl-ttt-demo/output/gluestreaming/total_clicks/ . (Make sure you replace \\${BUCKET_NAME} ) TIP: This path doesn't exist yet so switch back quickly to your Cloud9 enviroment tab and run the following command to build the full path string that you need. Copy it and paste it in the Include path field above: echo s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/gluestreaming/total_clicks/ 4. Still on Add a data store page, under Sample Size (optional) choose 1 . Then, expand where it says \"Exclude patterns (optional)\" and in the Exclude patterns field add **00001 and hit Next (twice). ( Don't miss the 2 asterisks ) 5. On Add another data store page, choose No and click Next . 6. On Choose an IAM role page, click Choose an existing IAM role and pick the role AWSGlueServiceRole-etl-ttt-demo , then click Next . 7. On Create a schedule for this crawler page, just click Next . 8. On Configure the crawler's output page, choose glue_ttt_demo_db from the Database dropdown list. Then, expand where it says \"Grouping behavior for S3 data (optional)\" and check the box \"Create a single schema for each S3 path\" . Click Next . 9. Review everything in the last page and click Finish .","title":"3. Creating new Glue Resources (New Crawler &amp; Classifier)"},{"location":"03-StreamingLab/03-streaming-lab/","text":"GLUE (STUDIO) STREAMING The Part 3 - Glue (Studio) Streaming - of the ETL Train the Trainer workshop is where you are going to use AWS Glue Studio Graphical Interface for the first time. You are going to use Glue Studio do build 2 jobs here. The first job is a dummy Glue Streaming Job that will validate the JOIN between the customer RDS table and a web_page CSV data that you uploaded earlier to S3. Note: This CSV data is basically a representation of the actual web-page-streaming-table (with the different of that one being in JSON format) that you will use in real Glue Streaming Job Apart from validate the JOIN between datasets, this dummy job will also validate the SQL code used in the SQL Transform node of this lab. If validation succeeds, you will then be able to Preview the Data before outputting it or, in this case, before building the real Glue Streaming Job . 1. Understanding the Streaming Resources Provided (CloudFormation & Scripts) \u00b6 For this lab to work, the CloudFormation template provided few resources for you already. Kinesis Data Stream A Kinesis Data Stream named etl-ttt-demo-stream has been added already which will receive the stream of data from a script you will run later on this lab. Click here to search for Amazon Kinesis and explore the Kinesis Data Stream if you want. Cataloged Streaming Table As you already saw it, a table named web-page-streaming-table has been created for you with a proper schema definition and all the necessary settings. You can see the details of this table in the AWS Glue Console . Here's a snapshot of that table's schema. Kinesis Data Ingestion Script (python) As mentioned, you will be running the following script in order to simulate streaming data being added to your Kinesis Data Stream etl-ttt-demo-stream To download the script (and to take a loot at it), run the following commands: cd ~/environment/ttt-demo/ aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py RDS MySQL Customer Table For the Part 3 only of this workshop , you need to delete multiple rows from the RDS MySQL's customer table . That's because the customer is too big originally so previewing such amount of data with Glue Studio Data Preview feature is inneficient and may fail . Run the following SQL Delete Statement to reduce the table to only 1000 rows : mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"delete from customer where c_customer_sk > 1000\" ; mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"commit\" ; 2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job) \u00b6 Let's start building the dummy job to validate our Glue Streaming Job logic. From the AWS Glue Console , click on AWS Glue Studio under the ETL section in the far left menu. This will take you to the AWS Glue Studio Console where you will be authoring the jobs graphically. For that, click on Jobs on the far left menu, select Visual with a blank canvas option and click on Create . You will be presented with a blank canvas. The first thing you must do there is to rename you job. Just click where it says \"Untitled job\" , type dummy-streaming-job and click out the Job's name box. Now, the next thing you must do, is to set your jobs detail. Click on the tab Job details tab and set the following configurations: Under IAM role select AWSGlueServiceRole-etl-ttt-demo Check the Automatically scale the number of workers option (GA regions only, otherwise set number of workers to 4 ) Under Job bookmark select Disable Set Number of retries to 0 Note: In production environments, you want to enable the bookmark and set retries to bigger than 0. You don't need to change any other settings here, but you should take some time to explore what settings are available in this tab. When you are done exploring, click Save on the upper right to save the changed settings. Click the Visual tab again to go back to visual editor. You should see 3 dropdown buttons: Source , Transform , and Target . Let's start creating the following 2 sources: Web Page Source For the Web Page CSV source, click on the Source dropdown icon and choose Amazon S3 in dropdown list. Click on the node that has been automatically added to the canvas to highlight it. Make the following changes to it: Click Node properties tab Set Name to Web Page S3 Click Data source properties - S3 tab Under S3 source type select S3 location To set the S3 URL , use the Browse S3 button to navigate to: s3://${BUCKET_NAME}/etl-ttt-demo/csv_tables/web_page.csv Uncheck the Recursive option Click Infer schema button at the bottom Click on Output schema tab to see schema infered, then click Save Customer RDS Source For the Customer RDS source, click on the Source dropdown icon and choose AWS Glue Data Catalog in the dropdown list. Click Node properties tab Change Name to Customer RDS Click Data source properties - Data Catalog tab Under Database select glue_ttt_demo_db Under Table select rds_crawled_tpcds_customer Click on Output schema tab to see schema, then click Save To join both datasets, click on the Web Page S3 node first to highlight it, then click on Transform dropdown icon. Note: You will notice there are pre-build transformations and custom transformations. Glue Studio is designed to be used by developers who could write custom Apache Spark, Glue and SQL code , but it also provides pre-build common transformations. For this lab you wil lbe using a custom SQL Transform to write your own SQL Join code. In the list of transforms that appears, scroll to the bottom and choose SQL . A new SQL node will be linked to the Web Page S3 node. Click on this new SQL node to highligt it and do the following: Click Node properties tab Under Node parents , select Customer RDS by clicking the checkbox next to it Click Transform tab Set Input sources Web Page S3 with Spark SQL aliases value to wp Set Input sources Customer RDS with Spark SQL aliases value to cust Copy the following code to SQL query and click Save select CONCAT ( cust . c_first_name , ' ' , cust . c_last_name ) as full_name , cust . c_email_address , count ( wp . wp_char_count ) as total_clicks from cust left join wp on cust . c_customer_sk = wp . wp_customer_sk where 1 = 2 --remove the where clause after preview!! group by 1 , 2 order by total_clicks desc Click on Data Preview tab Click on the Start data preview session button there. In the pop-up window that will open, choose the IAM role AWSGlueServiceRole-etl-ttt-demo Wait for Data Preview to finish (it takes about 5 minutes to preview) Note: It will return zero rows because you are previewing the query with where 1=2 first. This is to make previewing faster and to avoid issues. If issues happen (or previewing is taking more than 5 minutes), Save this job, go back to Jobs in the Glue Studio left menu and, under Your jobs select the dummy job you created and clone it into a new one. Rename it dummy-streaming-job-2 . TIP: You can delete the first job dummy-streaming-job later too. Once preview is completed, Click on Output schema tab to see the schemas(s) Note that you have 2 outputs: Output 1 and Output 2 . Click on the Use datapreview schema button that you see right there in the Output schema tab. Note that the schema now matches only the relevant columns of the query. Click on Transform tab again and remove the where 1=2 clause. Click on the Data Preview tab one last time to see the data appearing there. Note that that only relevant columns were brought from the SQL query. Click Save . Now, add an Apply Mapping transform by clickinng on the SQL node first to highlight it, then click on Transform dropdown icon and choose Apply Mapping from the list. A new Apply Mapping node will be linked to the SQL node. Click on this new Apply Mapping node to highligt it and do the following: Click on Transform tab Change Source key full_name to c_full_name Click on Output schema tab to see the change and Save . Finally, add a Target to the job. To do this, first click on the Apply Mapping node to highlight it, then click on Target dropdown, choose Amazon S3 from the list and a new Amazon S3 node will be linked to the Apply Mapping node. Click on this new Amazon S3 node to highligt it and do the following: Click on Data target properties - S3 tab Set Format to CSV Set S3 Target Location to s3://$BUCKET_NAME/etl-ttt-demo/output/gluestreaming/total_clicks/ TIP: Switch back quickly to your Cloud9 enviroment and use the following command to build the entire path you need for the S3 Target Location above. echo \"s3:// $BUCKET_NAME /etl-ttt-demo/output/gluestreaming/total_clicks/\" Click Save . !!! You can now save this job for the last time but DO NOT RUN IT!!!! 3. Creating the Glue Streaming Job (Cloning Jobs!) \u00b6 Go back to Jobs in the Glue Studio left menu and, under Your jobs select the dummy-streaming-job (or any clone of it). Click on the Actions dropdown button and choose Clone Job . The visual canvas will open and you will notice that the cloned job contains the exact same nodes with the exact same definitions you choose for the dummy job. Rename this new job to glue-streaming-job and Save it. Now, all you have to do is: 1. Remove the Web Page S3 by clicking on it to highlight it first, then clicking on Remove (trash icon) 2. click on the Source dropdown icon and choose Amazon Kinesis in dropdown list. A new and isolated Amazon Kinesis node will appear. 3. Click on the new Amazon Kinesis node to highlight it, go to Node Properties tab and name it Web Page Stream . 4. Click on Data source properties - Kinesis Stream tab and under Amazon Kinesis Source choose Data Catalog table . Then, under Database choose glue_ttt_demo_db and under Table choose web-page-streaming-table . 5. Uncheck Detect Schema , set Starting position to Latest and set Window size to 60 . Click Save 6. Now, click on the SQL joint node at the center of the canvans to highlight it. Go to Node Properties tab and under Node parents check the Web Page Stream node to complete the Join . 7. Click on the Transform tab and fix the SQL aliases for the Amazon Kinesis Input Source by typing wp there to match the alias in the SQL query . Click Save . !!! !!! DO NOT RUN THIS JOB YET !!! !!! Before running this job, go back to your Cloud9 Enviroment tab and run the following commands to repopulate the RDS MySQL's customer table : cd /tmp/dsd/ file = customer_1_2.dat mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd --local-infile -Dtpcds -e \"truncate table tpcds.customer\" mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd --local-infile -Dtpcds -e \\ \"load data local infile ' $file ' replace into table tpcds.customer character set latin1 fields terminated by '|'\" mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd --local-infile -Dtpcds -e \"select count(*) from tpcds.customer\" You should have 100.000 rows in the customer table now. Now, you are free to run the Glue Streaming Job . Go back to the Glue Studio tab and click Run . A green banner will appear at the top with a message \"Successfully started job glue-streaming-job. Navigate to Run Details for more details.\" . Click on Run Details to confirm that the job is indeed Running . TIP: You can delete all the dummy jobs now (Once you validate the data previewed previously). You have finshed creating & running a Glue Streaming Job . At this point, there's no data being pushed into the Kinesis Data Stream source of this job. Once you are ready move to the Part 4 - Orchestration & Data Analytsis where you will start to push data into the Kinesis Stream .","title":"GLUE (STUDIO) STREAMING"},{"location":"03-StreamingLab/03-streaming-lab/#1-understanding-the-streaming-resources-provided-cloudformation-scripts","text":"For this lab to work, the CloudFormation template provided few resources for you already. Kinesis Data Stream A Kinesis Data Stream named etl-ttt-demo-stream has been added already which will receive the stream of data from a script you will run later on this lab. Click here to search for Amazon Kinesis and explore the Kinesis Data Stream if you want. Cataloged Streaming Table As you already saw it, a table named web-page-streaming-table has been created for you with a proper schema definition and all the necessary settings. You can see the details of this table in the AWS Glue Console . Here's a snapshot of that table's schema. Kinesis Data Ingestion Script (python) As mentioned, you will be running the following script in order to simulate streaming data being added to your Kinesis Data Stream etl-ttt-demo-stream To download the script (and to take a loot at it), run the following commands: cd ~/environment/ttt-demo/ aws s3 cp s3://ee-assets-prod-us-east-1/modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/PutRecord_Kinesis.py . cat PutRecord_Kinesis.py RDS MySQL Customer Table For the Part 3 only of this workshop , you need to delete multiple rows from the RDS MySQL's customer table . That's because the customer is too big originally so previewing such amount of data with Glue Studio Data Preview feature is inneficient and may fail . Run the following SQL Delete Statement to reduce the table to only 1000 rows : mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"delete from customer where c_customer_sk > 1000\" ; mysql -h ${ mysqlendpoint } -u etluser -petltttdemopwd -Dtpcds -e \"commit\" ;","title":"1. Understanding the Streaming Resources Provided (CloudFormation &amp; Scripts)"},{"location":"03-StreamingLab/03-streaming-lab/#2-validating-streaming-job-logic-and-data-glue-studio-dummy-job","text":"Let's start building the dummy job to validate our Glue Streaming Job logic. From the AWS Glue Console , click on AWS Glue Studio under the ETL section in the far left menu. This will take you to the AWS Glue Studio Console where you will be authoring the jobs graphically. For that, click on Jobs on the far left menu, select Visual with a blank canvas option and click on Create . You will be presented with a blank canvas. The first thing you must do there is to rename you job. Just click where it says \"Untitled job\" , type dummy-streaming-job and click out the Job's name box. Now, the next thing you must do, is to set your jobs detail. Click on the tab Job details tab and set the following configurations: Under IAM role select AWSGlueServiceRole-etl-ttt-demo Check the Automatically scale the number of workers option (GA regions only, otherwise set number of workers to 4 ) Under Job bookmark select Disable Set Number of retries to 0 Note: In production environments, you want to enable the bookmark and set retries to bigger than 0. You don't need to change any other settings here, but you should take some time to explore what settings are available in this tab. When you are done exploring, click Save on the upper right to save the changed settings. Click the Visual tab again to go back to visual editor. You should see 3 dropdown buttons: Source , Transform , and Target . Let's start creating the following 2 sources: Web Page Source For the Web Page CSV source, click on the Source dropdown icon and choose Amazon S3 in dropdown list. Click on the node that has been automatically added to the canvas to highlight it. Make the following changes to it: Click Node properties tab Set Name to Web Page S3 Click Data source properties - S3 tab Under S3 source type select S3 location To set the S3 URL , use the Browse S3 button to navigate to: s3://${BUCKET_NAME}/etl-ttt-demo/csv_tables/web_page.csv Uncheck the Recursive option Click Infer schema button at the bottom Click on Output schema tab to see schema infered, then click Save Customer RDS Source For the Customer RDS source, click on the Source dropdown icon and choose AWS Glue Data Catalog in the dropdown list. Click Node properties tab Change Name to Customer RDS Click Data source properties - Data Catalog tab Under Database select glue_ttt_demo_db Under Table select rds_crawled_tpcds_customer Click on Output schema tab to see schema, then click Save To join both datasets, click on the Web Page S3 node first to highlight it, then click on Transform dropdown icon. Note: You will notice there are pre-build transformations and custom transformations. Glue Studio is designed to be used by developers who could write custom Apache Spark, Glue and SQL code , but it also provides pre-build common transformations. For this lab you wil lbe using a custom SQL Transform to write your own SQL Join code. In the list of transforms that appears, scroll to the bottom and choose SQL . A new SQL node will be linked to the Web Page S3 node. Click on this new SQL node to highligt it and do the following: Click Node properties tab Under Node parents , select Customer RDS by clicking the checkbox next to it Click Transform tab Set Input sources Web Page S3 with Spark SQL aliases value to wp Set Input sources Customer RDS with Spark SQL aliases value to cust Copy the following code to SQL query and click Save select CONCAT ( cust . c_first_name , ' ' , cust . c_last_name ) as full_name , cust . c_email_address , count ( wp . wp_char_count ) as total_clicks from cust left join wp on cust . c_customer_sk = wp . wp_customer_sk where 1 = 2 --remove the where clause after preview!! group by 1 , 2 order by total_clicks desc Click on Data Preview tab Click on the Start data preview session button there. In the pop-up window that will open, choose the IAM role AWSGlueServiceRole-etl-ttt-demo Wait for Data Preview to finish (it takes about 5 minutes to preview) Note: It will return zero rows because you are previewing the query with where 1=2 first. This is to make previewing faster and to avoid issues. If issues happen (or previewing is taking more than 5 minutes), Save this job, go back to Jobs in the Glue Studio left menu and, under Your jobs select the dummy job you created and clone it into a new one. Rename it dummy-streaming-job-2 . TIP: You can delete the first job dummy-streaming-job later too. Once preview is completed, Click on Output schema tab to see the schemas(s) Note that you have 2 outputs: Output 1 and Output 2 . Click on the Use datapreview schema button that you see right there in the Output schema tab. Note that the schema now matches only the relevant columns of the query. Click on Transform tab again and remove the where 1=2 clause. Click on the Data Preview tab one last time to see the data appearing there. Note that that only relevant columns were brought from the SQL query. Click Save . Now, add an Apply Mapping transform by clickinng on the SQL node first to highlight it, then click on Transform dropdown icon and choose Apply Mapping from the list. A new Apply Mapping node will be linked to the SQL node. Click on this new Apply Mapping node to highligt it and do the following: Click on Transform tab Change Source key full_name to c_full_name Click on Output schema tab to see the change and Save . Finally, add a Target to the job. To do this, first click on the Apply Mapping node to highlight it, then click on Target dropdown, choose Amazon S3 from the list and a new Amazon S3 node will be linked to the Apply Mapping node. Click on this new Amazon S3 node to highligt it and do the following: Click on Data target properties - S3 tab Set Format to CSV Set S3 Target Location to s3://$BUCKET_NAME/etl-ttt-demo/output/gluestreaming/total_clicks/ TIP: Switch back quickly to your Cloud9 enviroment and use the following command to build the entire path you need for the S3 Target Location above. echo \"s3:// $BUCKET_NAME /etl-ttt-demo/output/gluestreaming/total_clicks/\" Click Save .","title":"2. Validating Streaming Job Logic and Data (Glue Studio Dummy Job)"},{"location":"03-StreamingLab/03-streaming-lab/#3-creating-the-glue-streaming-job-cloning-jobs","text":"Go back to Jobs in the Glue Studio left menu and, under Your jobs select the dummy-streaming-job (or any clone of it). Click on the Actions dropdown button and choose Clone Job . The visual canvas will open and you will notice that the cloned job contains the exact same nodes with the exact same definitions you choose for the dummy job. Rename this new job to glue-streaming-job and Save it. Now, all you have to do is: 1. Remove the Web Page S3 by clicking on it to highlight it first, then clicking on Remove (trash icon) 2. click on the Source dropdown icon and choose Amazon Kinesis in dropdown list. A new and isolated Amazon Kinesis node will appear. 3. Click on the new Amazon Kinesis node to highlight it, go to Node Properties tab and name it Web Page Stream . 4. Click on Data source properties - Kinesis Stream tab and under Amazon Kinesis Source choose Data Catalog table . Then, under Database choose glue_ttt_demo_db and under Table choose web-page-streaming-table . 5. Uncheck Detect Schema , set Starting position to Latest and set Window size to 60 . Click Save 6. Now, click on the SQL joint node at the center of the canvans to highlight it. Go to Node Properties tab and under Node parents check the Web Page Stream node to complete the Join . 7. Click on the Transform tab and fix the SQL aliases for the Amazon Kinesis Input Source by typing wp there to match the alias in the SQL query . Click Save .","title":"3. Creating the Glue Streaming Job (Cloning Jobs!)"},{"location":"04-OrchestrationLab/04-orchestration-lab/","text":"ORCHESTRATION & DATA ANALYSIS In the Part 4 - Orchestration & Data Analysis - of this workshop you will combine the forces of AWS CloudTrail, Amazon EventBridge, AWS Glue Workflows and Amazon Athena to orchestrate the entire data pipeline - from the streaming of JSON data until the query analysis in Amazon Athena . 1. Understanding the Orchestration Flow \u00b6 To understand the Orchestration flow, first you need to take a look at the pre-created resources you havefrom the CloudFormation Template for this lab. Go to AWS CloudTrail console (switch to your right region if needed!) to explore the Trail created for your named etl-ttt-demo-trail Note that this Trail is monitoring for write events for two paths in the etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} Bucket as you can see in the following picture: [ADD UPDATED PICTURE HERE WITH THE TWO DATA EVENTS - NEW CFN TEMPLATE!!!0 By having such Trail configuration , you will be able to leverage AWS Event Bridge during this lab to trigger (Step Function and Glue) Workflows as soon as streaming data lands in the trail-enabled S3 Bucket . That is possible because once a file lands in this bucket it records a write event in the Trail . This write event further triggers an EventBridge Rule which, consequently, cat fire (Step Functionn and Glue) Workflows . 2. Creating Glue Workflow and Glue Event Based Trigger (via CLI) \u00b6 In this lab, we will create a Glue Workflow that will get triggered upon a write event notification received by EventBridge . This workflow will then initiate a Crawler to immediatelly crawl the file that just landed in the S3 bucket So, let's create the Glue Workflow followed by a Glue Trigger . To do this, go back to your Cloud9 Enviroment Terminal and run the following AWS CLI commands in sequence. First the Glue Workflow : aws glue create-workflow --name etl-ttt-event-driven-workflow Then the Glue Trigger : aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl-streammed-data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" After running both commands, go to the Glue Console and click on Workflows under the ETL section of the Glue Console's left menu . You will see a new Glue Workflow created there. Click on the radio button close to its name to see the workflow in the Graph Canvans as bellow: Note: You can also see the Glue Trigger in the Trigger menu under the ETL section. 3. Creating Event Bridge Rule and Target (via CLI) \u00b6 Now, let's create the EventBridge Event Rule followed by an EventBridge Event Target which is, of course, the Glue Trigger created in the previous step. Run the following AWS CLI commands in sequence to create everything. First the Event Rule : aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" Then the Event Target : aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"etl-ttt-event-driven-workflow\",\"Arn\"=\"arn:aws:glue: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } After running both commands, go to AWS EventBridge console. (switch to your right region if needed!) . On the left side menu, click on Rules , then click on top of the rule total-clicks-rule to open its details. You should see under the Event Pattern tab the following: This pattern is basically saying that this EventRule is watching for every PutObject event that occur in the bucket/prefix specified in there. Click on the Targets tab. Here, you can see that the Target Name points to the Glue Workflow you just created etl-ttt-event-driven-workflow . Also that it has a EventBridge Role , with all the required permissions, associated with it. This Role has also been created as part of the CloudFormation template . 4. Triggering Orchestration & Following The Flow \u00b6 Now, it is time to push data into the Kinesis Data Stream . You will run the Kinesis Ingestion Python Script you saw in the previous lab: Part 3 - Glue (Studio) Streaming - to push data into the stream. To start the script, run the following in the Cloud9 Enviroment : cd ~/environment/ttt-demo/ python PutRecord_Kinesis.py You will start to see data flowing on your Cloud9 Terminal , wait for few seconds for this to complete. In the meantime, go back to the Glue Studio Console and verify that the Glue Streaming Job is still running. Once the script finishes and you have verified that the Glue Streaming Job is running . Go back to the AWS Glue Console and look for your workflow. Refresh the Workflows screen and select your etl-ttt-event-driven-workflow . (it may take couple of minutes for data to land in the buckect, just wait and keep refreshing at every 15 seconds!) Then click on the History tab at the bottom of the page, select the Run ID you see there and click on View run details button to see the workflow flowing : While the workflow is running, you can check the Glue Crawler that the workflow is triggering. You can also go to you S3 Bucket and see the files that were generated there by the AWS Glue Streaming Job which was sent from the Kinesis Data Stream (after running the Kinesis Ingestion Script ): Running Crawler's name : crawl_streammed_data Streaming Job's target path : s3://etl-ttt-demo-${AWS\\_ACCOUNT\\_ID}-${AWS_REGION}/etl-ttt-demo/output/gluestreaming/total_clicks/ NOTE: To keep this workflow from getting trigger multiple times, delete it now as you don't need it anymore! 5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog \u00b6 Once the workflow completes, you will noticed that the Crawler triggered by the workflow actually produced 1 new table . In the Glue Console , go to Tables to see a new table named total_clicks . (Feel free to explore this table's details as much as you want!) In the Glue Console's Table page, click on the checkbox near the total_clicks table, then click on the Action dropdown button to launch the table in the Amazon Athena Console . (Hit the Preview Button if a pop-up is prompted) In the Athena Query Editor Console , you may see a blue banner at the top that says \"Before you run your first query, you need to set up a query result location in Amazon S3.\" . Just click on the View settings button you see there, then click on the Manage button in the far top right of the page, then click on Browse S3 . Search for a bucket that starts with aws-glue-assets- and select it. Complement this bucket with /athena-output/ and hit Save Click on Editor to start running your queries. In the Query 1 tab, erase everything there, paste and Run the following query: SELECT * FROM \"AwsDataCatalog\" . \"glue_ttt_demo_db\" . \"total_clicks\" order by 3 desc limit 10 ; You should see the query results at the bottom: Now, you are going to create a View on top of this table to allow for aggregation of the underlying files that compose this table's data. Run the following command to create a view that is meant to select the Top 5 customers aggregated by their number of clicks in a particular website: (That's the web_page streaming data you have captured!) CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\" ( CAST ( \"total_clicks\" AS integer )) total_clicks FROM \"total_clicks\" GROUP BY 1 , 2 ORDER BY 3 DESC In a new Query Tab , run below query to query the view now: SELECT * FROM \"glue_ttt_demo_db\" . \"tpc_customer_inter\" ; Note down the results you see: Now, go back to your Cloud9 Enviroment and run the Kinesis Ingestion Python Script again: cd ~/environment/ttt-demo/ python PutRecord_Kinesis.py Allow for it to complete and wait for about 30-60 seconds and re-run the last Athena query. SELECT * FROM \"glue_ttt_demo_db\" . \"tpc_customer_inter\" ; Note: If you noticed that above results hasn't changed. It might be that your calendar Year, Month, Day, or Hour has changed (most probably the Hour ). Run the crawl_streammed_data crawler again to update partitions or try the MSCK REPAIR TABLE `total_clicks`; command from within Athena itself. You have reached to the end of this lab. If you want, you can keep pushing more data into your Kinesis Data Stream by running the Kinesis Ingestion Python Script multiple times and allowing few seconds each time for the Glue Streaming Job to process it. Once you are ready, manually stop the glue-streaming-job and move to Part 5 - Machine Learning with Glue & Glue Studio Notebooks .","title":"ORCHESTRATION & DATA ANALYSIS"},{"location":"04-OrchestrationLab/04-orchestration-lab/#1-understanding-the-orchestration-flow","text":"To understand the Orchestration flow, first you need to take a look at the pre-created resources you havefrom the CloudFormation Template for this lab. Go to AWS CloudTrail console (switch to your right region if needed!) to explore the Trail created for your named etl-ttt-demo-trail Note that this Trail is monitoring for write events for two paths in the etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} Bucket as you can see in the following picture: [ADD UPDATED PICTURE HERE WITH THE TWO DATA EVENTS - NEW CFN TEMPLATE!!!0 By having such Trail configuration , you will be able to leverage AWS Event Bridge during this lab to trigger (Step Function and Glue) Workflows as soon as streaming data lands in the trail-enabled S3 Bucket . That is possible because once a file lands in this bucket it records a write event in the Trail . This write event further triggers an EventBridge Rule which, consequently, cat fire (Step Functionn and Glue) Workflows .","title":"1. Understanding the Orchestration Flow"},{"location":"04-OrchestrationLab/04-orchestration-lab/#2-creating-glue-workflow-and-glue-event-based-trigger-via-cli","text":"In this lab, we will create a Glue Workflow that will get triggered upon a write event notification received by EventBridge . This workflow will then initiate a Crawler to immediatelly crawl the file that just landed in the S3 bucket So, let's create the Glue Workflow followed by a Glue Trigger . To do this, go back to your Cloud9 Enviroment Terminal and run the following AWS CLI commands in sequence. First the Glue Workflow : aws glue create-workflow --name etl-ttt-event-driven-workflow Then the Glue Trigger : aws glue create-trigger \\ --workflow-name etl-ttt-event-driven-workflow \\ --type EVENT \\ --name s3-object-trigger \\ --actions CrawlerName=crawl-streammed-data \\ --event-batching-condition \"{\\\"BatchSize\\\": 2, \\\"BatchWindow\\\": 900}\" After running both commands, go to the Glue Console and click on Workflows under the ETL section of the Glue Console's left menu . You will see a new Glue Workflow created there. Click on the radio button close to its name to see the workflow in the Graph Canvans as bellow: Note: You can also see the Glue Trigger in the Trigger menu under the ETL section.","title":"2. Creating Glue Workflow and Glue Event Based Trigger (via CLI)"},{"location":"04-OrchestrationLab/04-orchestration-lab/#3-creating-event-bridge-rule-and-target-via-cli","text":"Now, let's create the EventBridge Event Rule followed by an EventBridge Event Target which is, of course, the Glue Trigger created in the previous step. Run the following AWS CLI commands in sequence to create everything. First the Event Rule : aws events put-rule \\ --name \"total-clicks-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/output/gluestreaming/total_clicks/\\\"}] } \\ } \\ }\" Then the Event Target : aws events put-targets \\ --rule total-clicks-rule \\ --targets \"Id\"=\"etl-ttt-event-driven-workflow\",\"Arn\"=\"arn:aws:glue: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :workflow/etl-ttt-event-driven-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } After running both commands, go to AWS EventBridge console. (switch to your right region if needed!) . On the left side menu, click on Rules , then click on top of the rule total-clicks-rule to open its details. You should see under the Event Pattern tab the following: This pattern is basically saying that this EventRule is watching for every PutObject event that occur in the bucket/prefix specified in there. Click on the Targets tab. Here, you can see that the Target Name points to the Glue Workflow you just created etl-ttt-event-driven-workflow . Also that it has a EventBridge Role , with all the required permissions, associated with it. This Role has also been created as part of the CloudFormation template .","title":"3. Creating Event Bridge Rule and Target (via CLI)"},{"location":"04-OrchestrationLab/04-orchestration-lab/#4-triggering-orchestration-following-the-flow","text":"Now, it is time to push data into the Kinesis Data Stream . You will run the Kinesis Ingestion Python Script you saw in the previous lab: Part 3 - Glue (Studio) Streaming - to push data into the stream. To start the script, run the following in the Cloud9 Enviroment : cd ~/environment/ttt-demo/ python PutRecord_Kinesis.py You will start to see data flowing on your Cloud9 Terminal , wait for few seconds for this to complete. In the meantime, go back to the Glue Studio Console and verify that the Glue Streaming Job is still running. Once the script finishes and you have verified that the Glue Streaming Job is running . Go back to the AWS Glue Console and look for your workflow. Refresh the Workflows screen and select your etl-ttt-event-driven-workflow . (it may take couple of minutes for data to land in the buckect, just wait and keep refreshing at every 15 seconds!) Then click on the History tab at the bottom of the page, select the Run ID you see there and click on View run details button to see the workflow flowing : While the workflow is running, you can check the Glue Crawler that the workflow is triggering. You can also go to you S3 Bucket and see the files that were generated there by the AWS Glue Streaming Job which was sent from the Kinesis Data Stream (after running the Kinesis Ingestion Script ): Running Crawler's name : crawl_streammed_data Streaming Job's target path : s3://etl-ttt-demo-${AWS\\_ACCOUNT\\_ID}-${AWS_REGION}/etl-ttt-demo/output/gluestreaming/total_clicks/ NOTE: To keep this workflow from getting trigger multiple times, delete it now as you don't need it anymore!","title":"4. Triggering Orchestration &amp; Following The Flow"},{"location":"04-OrchestrationLab/04-orchestration-lab/#5-exploring-and-analyzing-tables-data-cataloged-in-glue-data-catalog","text":"Once the workflow completes, you will noticed that the Crawler triggered by the workflow actually produced 1 new table . In the Glue Console , go to Tables to see a new table named total_clicks . (Feel free to explore this table's details as much as you want!) In the Glue Console's Table page, click on the checkbox near the total_clicks table, then click on the Action dropdown button to launch the table in the Amazon Athena Console . (Hit the Preview Button if a pop-up is prompted) In the Athena Query Editor Console , you may see a blue banner at the top that says \"Before you run your first query, you need to set up a query result location in Amazon S3.\" . Just click on the View settings button you see there, then click on the Manage button in the far top right of the page, then click on Browse S3 . Search for a bucket that starts with aws-glue-assets- and select it. Complement this bucket with /athena-output/ and hit Save Click on Editor to start running your queries. In the Query 1 tab, erase everything there, paste and Run the following query: SELECT * FROM \"AwsDataCatalog\" . \"glue_ttt_demo_db\" . \"total_clicks\" order by 3 desc limit 10 ; You should see the query results at the bottom: Now, you are going to create a View on top of this table to allow for aggregation of the underlying files that compose this table's data. Run the following command to create a view that is meant to select the Top 5 customers aggregated by their number of clicks in a particular website: (That's the web_page streaming data you have captured!) CREATE OR REPLACE VIEW \"tpc_customer_inter\" AS SELECT \"c_full_name\" , \"c_email_address\" , \"sum\" ( CAST ( \"total_clicks\" AS integer )) total_clicks FROM \"total_clicks\" GROUP BY 1 , 2 ORDER BY 3 DESC In a new Query Tab , run below query to query the view now: SELECT * FROM \"glue_ttt_demo_db\" . \"tpc_customer_inter\" ; Note down the results you see: Now, go back to your Cloud9 Enviroment and run the Kinesis Ingestion Python Script again: cd ~/environment/ttt-demo/ python PutRecord_Kinesis.py Allow for it to complete and wait for about 30-60 seconds and re-run the last Athena query. SELECT * FROM \"glue_ttt_demo_db\" . \"tpc_customer_inter\" ; Note: If you noticed that above results hasn't changed. It might be that your calendar Year, Month, Day, or Hour has changed (most probably the Hour ). Run the crawl_streammed_data crawler again to update partitions or try the MSCK REPAIR TABLE `total_clicks`; command from within Athena itself. You have reached to the end of this lab. If you want, you can keep pushing more data into your Kinesis Data Stream by running the Kinesis Ingestion Python Script multiple times and allowing few seconds each time for the Glue Streaming Job to process it. Once you are ready, manually stop the glue-streaming-job and move to Part 5 - Machine Learning with Glue & Glue Studio Notebooks .","title":"5. Exploring and Analyzing Table's Data Cataloged in Glue Data Catalog"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/","text":"MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOKS Welcome to Part 5! The Part 5 - Machine Learning with Glue & Glue Studio Notebooks - of the ETL Train The Trainer Workshop is going to cover the steps required to work with the Glue ML Transforms, more specifically, it will teach you about how to create , train (with labeling files ), and write Glue ETL code that leverages the Glue's FindMatches ML Transform using Glue Studio Notebooks that you can further operationalize it into Glue Jobs . 0. (Pre Steps) - Understading & Setting up the Resources for ML Lab \u00b6 To begin, you must first run the following commands in your Cloud9 Terminal to download the the ML Lab files from the Workshop's default S3 Bucket . These commands will: Create the local directory structure for this lab in your Cloud9 Envirment Place each file in its respective folder. Re-upload all these files to your own S3 Bucket Building the required S3 Bucket Path structure for this lab: mkdir -p /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling /tmp/dsd/csv_tables/ml-lab/ml-customer-full /tmp/dsd/csv_tables/ml-lab/ml-labeling-file aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/sample-customer/sample-top-customer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling/sample-top-customer.csv aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/top-customer/full-top-customer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-top-customer.csv aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/labeled-customer/top-customer-labeling-file.csv /tmp/dsd/csv_tables/ml-lab/ml-labeling-file/top-customer-labeling-file.csv aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-notebook/ml-lab-notebook-job.ipynb /tmp/dsd/csv_tables/ml-lab/ml-notebook/ml-lab-notebook-job.ipynb aws s3 cp --recursive /tmp/dsd/csv_tables/ml-lab/ s3:// $BUCKET_NAME /etl-ttt-demo/ml-lab/ For this part of the ETL Train the Trainer Workshop , two crawlers have been created for you as part of the CloudFormation Template : The ml-sample-cust-crawler will create the S3-based table named ml_sample-customer which has much less amount of data but enought to be used as sample source data to train and label the FindMatches ML Transform you are going to create next. The ml-bootstrap-crawler will create the S3-based table named ml_dedup_sample_customer which is the full dataset that the future Notebook Job will try to deduplicate using the FindMatch ML Transform that you trained using the smaller ml_sample-customer dataset. 1. Creating and Training the Glue FindMatches ML Transform \u00b6 Run both crawler to create the aforementioned tables. (Both tables' schema are exactly the same as the original RDS customer . They both contain 14 columns ) Once ml-bootstrap-crawler finish, click on ML Transforms menu, under the ETL section on the menu at the left. There, click on Add transform or the Create ML transform buttons. 1. On Configure transform properties page, name the Trasform as FindMatches . Also, set the IAM Role to AWSGlueServiceRole-etl-ttt-demo . Click Next 2. On Choose a data source page, select the table ml_sample_customer . Click Next . 3. On Choose a primary key page, select the table c_customer_id . Click Next . 4. On Tune transform page, under Recall vs. precision select the option Favor precision (0.9) . Keep the option Balanced (0.5) selected. Click Next . 5. Review everything on the Review page and Click Finish to create the FindMatches ML Transform. Now, it is time to teach your ML Transform. To do this, select the FindMatches Transform you just created, then click on the Action dropdown button and choose Teach Transform . 1. On Teach the transform using labels page, under Labeling file choose the option I don't have labels , then click on the Generate labeling file button. 2. On Generate labeling file pop-up window, choose to S3 Path where you want to store the generated label file. Navigate to s3://\\${BUCKET_NAME}/etl-ttt-demo/output , then add /ml_gen_label_file/ and click Generate . Note: You don't need to wait for this generating labeling file process to finish because you already downloaded a fully labeled file (during the first step of this lab) to be used here. Jump to step 3 or, if you have enough time, you can wait for it to finish, then click on Download labeling file and perform the labeling process yourself. 3. Click Next and, On the Upload labels , under Labeling file , now choose I have labels . Then click on Upload labeling file from S3 and navigate to the path: s3://\\${BUCKET_NAME}/etl-ttt-demo/ml-lab/ml-labeling-file/top-customer-labeling-file.csv and click Upload , then click Next . 4. On Estimate quality metrics (optional) just click Complete Later to finish teaching your FindMatches ML Transform . By selection your Transform , you can see at the bottom, in the Task type column, that the Label Uploading Process has succeeded. 2. Testing FindMatches Transform with Glue Studio Notebook \u00b6 Note: Before start testing, you need to download the Notebook file you will use for this section of the lab. Go to your S3 Bucket s3://\\${BUCKET_NAME}/etl-ttt-demo/ml-lab/ml-notebook/ and download the file ml-lab-notebook-job.ipynb to your local computer. You will need it next! Now that the FindMatches Transform is trained, go to the AWS Glue Studio once again and click on Jobs in the left side menu. On the Jobs page, choose the Jupyter Notebook option and, under Options , choose the option Upload and edit an existing notebook and click on Choose File . Choose the file that has been shared with you which is located . Click on Create afterwards. On the Notebook setup page, give the name to the Job as ml-lab-notebook-job and set the IAM to AWSGlueServiceRole-etl-ttt-demo . Click on Start notebook job button to create a Jupyter Notebook from an unpload notebook file. Once the Notebook is fully ready, copy the following code and replace the Cell [1] with it. Afterwards, click on the Play button ( |> ) at the top-center-left of the notebook interface: ---DO NOT click on the orange run button on the far left--- Cell [1] ( replace the existing Cell [*] ): % glue_version 2.0 % idle_timeout 60 import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglueml.transforms import FindMatches sc = SparkContext . getOrCreate () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) Once you see a message saying \"Session XXXX-XXXX-XXXX has been created\", run the Cell [2] : You should see the ml_to_dedup_top_customer table's schema in the above output. Before you can run the Cell [3] , replace the \\${YOUR_TRANSFORM_ID} placeholder for the Transform ID of the the FindMatches Transform that you created. Go back to your Glue Console and, under the Details tab copy the Transform ID Once Transform ID is correctly replaced, run the Cell [3] . (This Cell should take about 6 minutes to finish!) You should see the output displaying the message: \"Duplicate count of matching IDs : 530\" Also, while this is running, you can go to your Glue Console and see, in the History Tab that a new task for your FindML Transform will start to run thanks to the line of code you just executed. 3. Validating and Deploying the FindMatches Glue Job \u00b6 Now, to validate the FindML Transform results , simply run the Cell [4] : You should also see that the FindMatches Transform has added another column named match_id to identify matching records in the output. That means that rows with the same match_id are considered matching records. To confirm that the FindML Transform worked, you can actually see that from the 10 rows returned in the output 5 are duplicates . This proves that the FindMatches ML Transform is working! . Now, let's leverage the match_id column created by the FindMatches Transform to eliminate the duplicates from the dataset. Run the Cell 5 : Notice how the duplicates are now gone. That means that you were able to create a code that leverages the a ML Transform that you created to help you find and eliminate duplicates in your dataset. You can now operationalize this code into a Glue Studio Job, but first, you must run the last cell, the Cell 6 , in order to sink the output data into the S3 Bucket . This Bucket will be used in the next lab (Part 5) to act as a shared data repository where the Data Analytics team uploads their files for further Preparation & Analysis . Replace the \\$BUCKET_NAME placeholder in the last cell with the following S3 Bucket path : s3://${BUCKET_NAME}/etl-ttt-demo/output/data_analytics_team_folder/top-customers/ TIP: Run the following command in your Cloud9 Terminal to build the full path you need: echo \"s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/data_analytics_team_folder/top-customers/\" Run the code to create your deduplicated version of the full top customer dataset. Once the code in the Cell 6 finish, check the S3 Bucket Path to confirm the file is there. TIP: Optionally, you can also run S3 Select queries on the data generated to validate as seen in the picture below: Finally, Save your job and download the notebook by clicking on Download Notebook button at the top-right of the Glue Studio Notebook Page in order keep a copy of the notebook locally on your computer for safety. You have now concluded the Part 5 - Machine Learning with Glue & Glue Studio Notebooks . Once you are ready move to the Part 6 - Workflow Orchestration with AWS Step Function .","title":"MACHINE LEARNING WITH GLUE & GLUE STUDIO NOTEBOOKS"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#0-pre-steps-understading-setting-up-the-resources-for-ml-lab","text":"To begin, you must first run the following commands in your Cloud9 Terminal to download the the ML Lab files from the Workshop's default S3 Bucket . These commands will: Create the local directory structure for this lab in your Cloud9 Envirment Place each file in its respective folder. Re-upload all these files to your own S3 Bucket Building the required S3 Bucket Path structure for this lab: mkdir -p /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling /tmp/dsd/csv_tables/ml-lab/ml-customer-full /tmp/dsd/csv_tables/ml-lab/ml-labeling-file aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/sample-customer/sample-top-customer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-sampling/sample-top-customer.csv aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/top-customer/full-top-customer.csv /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-top-customer.csv aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-customer/labeled-customer/top-customer-labeling-file.csv /tmp/dsd/csv_tables/ml-lab/ml-labeling-file/top-customer-labeling-file.csv aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/ml-notebook/ml-lab-notebook-job.ipynb /tmp/dsd/csv_tables/ml-lab/ml-notebook/ml-lab-notebook-job.ipynb aws s3 cp --recursive /tmp/dsd/csv_tables/ml-lab/ s3:// $BUCKET_NAME /etl-ttt-demo/ml-lab/ For this part of the ETL Train the Trainer Workshop , two crawlers have been created for you as part of the CloudFormation Template : The ml-sample-cust-crawler will create the S3-based table named ml_sample-customer which has much less amount of data but enought to be used as sample source data to train and label the FindMatches ML Transform you are going to create next. The ml-bootstrap-crawler will create the S3-based table named ml_dedup_sample_customer which is the full dataset that the future Notebook Job will try to deduplicate using the FindMatch ML Transform that you trained using the smaller ml_sample-customer dataset.","title":"0. (Pre Steps) - Understading &amp; Setting up the Resources for ML Lab"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#1-creating-and-training-the-glue-findmatches-ml-transform","text":"Run both crawler to create the aforementioned tables. (Both tables' schema are exactly the same as the original RDS customer . They both contain 14 columns ) Once ml-bootstrap-crawler finish, click on ML Transforms menu, under the ETL section on the menu at the left. There, click on Add transform or the Create ML transform buttons. 1. On Configure transform properties page, name the Trasform as FindMatches . Also, set the IAM Role to AWSGlueServiceRole-etl-ttt-demo . Click Next 2. On Choose a data source page, select the table ml_sample_customer . Click Next . 3. On Choose a primary key page, select the table c_customer_id . Click Next . 4. On Tune transform page, under Recall vs. precision select the option Favor precision (0.9) . Keep the option Balanced (0.5) selected. Click Next . 5. Review everything on the Review page and Click Finish to create the FindMatches ML Transform. Now, it is time to teach your ML Transform. To do this, select the FindMatches Transform you just created, then click on the Action dropdown button and choose Teach Transform . 1. On Teach the transform using labels page, under Labeling file choose the option I don't have labels , then click on the Generate labeling file button. 2. On Generate labeling file pop-up window, choose to S3 Path where you want to store the generated label file. Navigate to s3://\\${BUCKET_NAME}/etl-ttt-demo/output , then add /ml_gen_label_file/ and click Generate . Note: You don't need to wait for this generating labeling file process to finish because you already downloaded a fully labeled file (during the first step of this lab) to be used here. Jump to step 3 or, if you have enough time, you can wait for it to finish, then click on Download labeling file and perform the labeling process yourself. 3. Click Next and, On the Upload labels , under Labeling file , now choose I have labels . Then click on Upload labeling file from S3 and navigate to the path: s3://\\${BUCKET_NAME}/etl-ttt-demo/ml-lab/ml-labeling-file/top-customer-labeling-file.csv and click Upload , then click Next . 4. On Estimate quality metrics (optional) just click Complete Later to finish teaching your FindMatches ML Transform . By selection your Transform , you can see at the bottom, in the Task type column, that the Label Uploading Process has succeeded.","title":"1. Creating and Training the Glue FindMatches ML Transform"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#2-testing-findmatches-transform-with-glue-studio-notebook","text":"Note: Before start testing, you need to download the Notebook file you will use for this section of the lab. Go to your S3 Bucket s3://\\${BUCKET_NAME}/etl-ttt-demo/ml-lab/ml-notebook/ and download the file ml-lab-notebook-job.ipynb to your local computer. You will need it next! Now that the FindMatches Transform is trained, go to the AWS Glue Studio once again and click on Jobs in the left side menu. On the Jobs page, choose the Jupyter Notebook option and, under Options , choose the option Upload and edit an existing notebook and click on Choose File . Choose the file that has been shared with you which is located . Click on Create afterwards. On the Notebook setup page, give the name to the Job as ml-lab-notebook-job and set the IAM to AWSGlueServiceRole-etl-ttt-demo . Click on Start notebook job button to create a Jupyter Notebook from an unpload notebook file. Once the Notebook is fully ready, copy the following code and replace the Cell [1] with it. Afterwards, click on the Play button ( |> ) at the top-center-left of the notebook interface: ---DO NOT click on the orange run button on the far left--- Cell [1] ( replace the existing Cell [*] ): % glue_version 2.0 % idle_timeout 60 import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglueml.transforms import FindMatches sc = SparkContext . getOrCreate () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) Once you see a message saying \"Session XXXX-XXXX-XXXX has been created\", run the Cell [2] : You should see the ml_to_dedup_top_customer table's schema in the above output. Before you can run the Cell [3] , replace the \\${YOUR_TRANSFORM_ID} placeholder for the Transform ID of the the FindMatches Transform that you created. Go back to your Glue Console and, under the Details tab copy the Transform ID Once Transform ID is correctly replaced, run the Cell [3] . (This Cell should take about 6 minutes to finish!) You should see the output displaying the message: \"Duplicate count of matching IDs : 530\" Also, while this is running, you can go to your Glue Console and see, in the History Tab that a new task for your FindML Transform will start to run thanks to the line of code you just executed.","title":"2. Testing FindMatches Transform with Glue Studio Notebook"},{"location":"05-GlueStudioNotebookLab/05-gstudio-notebook-lab/#3-validating-and-deploying-the-findmatches-glue-job","text":"Now, to validate the FindML Transform results , simply run the Cell [4] : You should also see that the FindMatches Transform has added another column named match_id to identify matching records in the output. That means that rows with the same match_id are considered matching records. To confirm that the FindML Transform worked, you can actually see that from the 10 rows returned in the output 5 are duplicates . This proves that the FindMatches ML Transform is working! . Now, let's leverage the match_id column created by the FindMatches Transform to eliminate the duplicates from the dataset. Run the Cell 5 : Notice how the duplicates are now gone. That means that you were able to create a code that leverages the a ML Transform that you created to help you find and eliminate duplicates in your dataset. You can now operationalize this code into a Glue Studio Job, but first, you must run the last cell, the Cell 6 , in order to sink the output data into the S3 Bucket . This Bucket will be used in the next lab (Part 5) to act as a shared data repository where the Data Analytics team uploads their files for further Preparation & Analysis . Replace the \\$BUCKET_NAME placeholder in the last cell with the following S3 Bucket path : s3://${BUCKET_NAME}/etl-ttt-demo/output/data_analytics_team_folder/top-customers/ TIP: Run the following command in your Cloud9 Terminal to build the full path you need: echo \"s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/data_analytics_team_folder/top-customers/\" Run the code to create your deduplicated version of the full top customer dataset. Once the code in the Cell 6 finish, check the S3 Bucket Path to confirm the file is there. TIP: Optionally, you can also run S3 Select queries on the data generated to validate as seen in the picture below: Finally, Save your job and download the notebook by clicking on Download Notebook button at the top-right of the Glue Studio Notebook Page in order keep a copy of the notebook locally on your computer for safety. You have now concluded the Part 5 - Machine Learning with Glue & Glue Studio Notebooks . Once you are ready move to the Part 6 - Workflow Orchestration with AWS Step Function .","title":"3. Validating and Deploying the FindMatches Glue Job"},{"location":"06-StepFunctionsLab/06-step-functions-lab/","text":"WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTIONS In the Part 6 - Workflow Orchestration with AwS Step Functions - of this training you will be implement similar orchestration as the one you did during the Part 4 - Orchestration & Data Analysis - . This time, as soon as a file land in the Amazon S3 Bucket a write event gets recorded in the etl-ttt-demo-trail Cloud Trail that you explored earlier in Part 4 . Note that this Trail monitors write events for two paths in the etl-ttt-demo-${AWS_ACCOUNT_ID}-${AWS_REGION} Bucket , as you can see in the following picture: For this lab, you are interested in the /etl-ttt-demo/data_analytics_team_folder/ path which simulates a shared repository where the Data Analytics team uploads their files for further Preparation & Analysis . Furthermore, as previously explained, at every upload to this folder a write event gets generated. You will then create a new EventBridge Rule to listen to those events and automatically trigger a Step Function's State Machine Workfow that you will also create as part of this lab's steps. 1. Creating the Step Function Workflow \u00b6 Before creating the Step Function Workflow , let's first create one more Crawler . This cralwer is actualy part of the Step Function Workflow you are going to build and it is required! Final Crawler \u00b6 Run the following code on your Cloud9 Enviroment Terminal to create the ml-final-crawler : aws glue create-crawler \\ --name ml-final-crawler \\ --role AWSGlueServiceRole-etl-ttt-demo \\ --database-name glue_ttt_demo_db \\ --table-prefix ml_final_\\ --targets \"{\\\"S3Targets\\\": [{\\\"Path\\\": \\\"s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/data_analytics_team_folder/top-customers/\\\"}]}\" As you can see, the ml-final-crawler will crawl the shared Analytics Team Folder once the files land there. Step Function Workflow \u00b6 Now, let's finally create the Step Function Workflow . For this, go to the AWS Step Function console (switch to your right region if needed!) and click on State machines , under Step Functions label on the far left menu of the AWS Step Function Console. Then, click on Create state machine . On the Choose authoring method , select Write your workflow in code and under Type choose Standard . At the bottom of the same page, under Definition paste the following code: { \"Comment\" : \"A description of my state machine\" , \"StartAt\" : \"Find Match Job\" , \"States\" : { \"Find Match Job\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::glue:startJobRun.sync\" , \"Parameters\" : { \"JobName\" : \"ml-lab-notebook-job\" }, \"Comment\" : \"To remove duplicates in the full customer dataset using FIndMatches ML Transform and output the results in another output bucket\" , \"Next\" : \"Crawl Dedup Data\" }, \"Crawl Dedup Data\" : { \"Type\" : \"Task\" , \"End\" : true , \"Parameters\" : { \"Name\" : \"ml-final-crawler\" }, \"Resource\" : \"arn:aws:states:::aws-sdk:glue:startCrawler\" , \"Comment\" : \"To crawl the data after FindMatches dedup higiene process\" } } } You should see the Step Function Workflow Studio Canvans getting updated with the graphical flow of the steps you set with the code. Click Next and, on the Specity details page, do the following: Under Name give your State Machine Name the name of findmatches-ML-dedup-workflow . Under Permissions , select the Choose an existing role option and set the IAM Role to AWSStepFunctionRole-etl-ttt-demo . Scroll further down and click on the Create state machine button to create the Step Function State Machine Workflow . 2. Creating an EventBridge Rule & Target for the Step Function Workflow (via CLI) \u00b6 Now, to be able to trigger the Step Function Workflow , you also need to create a new EventBridge Rule & Target . Run the following steps in your Cloud9 Enviroment Terminal in sequence: First the Event Rule : aws events put-rule \\ --name \"find-matches-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/data_analytics_team_folder/\\\"}] } \\ } \\ }\" Then the Event Target : aws events put-targets \\ --rule find-matches-rule \\ --targets \"Id\"=\"findmatches-ML-dedup-workflow\",\"Arn\"=\"arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:findmatches-ML-dedup-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } After running both commands, go to AWS EventBridge console. (switch to your right region if needed!) . On the left side menu, click on Rules , then click on top of the rule find-matches-rule to open its details. You should see under the Event Pattern tab the following: This pattern is basically saying that this EventRule is watching for every PutObject event that occur in the bucket/prefix specified in there. Click on the Targets tab. Here, you can see that the Target Name points to the Step Functions Workflow you just created findmatches-ML-dedup-workflow . Also that it has a EventBridge Role , with all the required permissions, associated with it. This Role has also been created as part of the CloudFormation template . You are done creating the resources required for this Orchestration to work. Now, you need to simulate files being upladed into the central shared bucket repository of the Data Analytics Team, that means, the etl-ttt-demo/output/data_analytics_team_folder/top-customers/ . To do that, go back to your Cloud9 Enviroment Terminal and run the following S3 command to upload a file that contains duplicated data into the Data Analytics Team's shared folder : aws s3 cp /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-top-customer.csv s3://$BUCKET_NAME/etl-ttt-demo/data_analytics_team_folder/ Once the above file is uploaded, it will then trigger the Step Function Workflow that you created which, consequently, will run the ml-lab-notebook-job that contains the FindMatches Transform and all the necessary code you wrote to transform the source duplicated data into a deduplicated version of the full top customers dataset that the fictitious Data Analytics Team will use in order to proceed with their Data Preparation work . Following the flow \u00b6 You can follow the flow by going first to the Step Function Workflow page and looking your findmatches-ML-dedup-workflow workflow. There, under Executions , you can click on the one that Status column says Running and click on View Details . You should see the graphical workflow changing its states. From there, go to your Glue Studio Console (switch to your right region if needed!) and look for your job ml-lab-notebook-job under the Monitoring page. Then, click on the big blue number under Running to scroll to the running Jobs, and confirm that your job is running. Finally, go to the Glue Console (switch to your right region if needed!) and look for two things: The FindMatches ML Transform : This should take about 6-7 minutes to complete: The ml_final_crawler : This will start as soon as your ml-lab-notebook-job succeed and it will create 1 new table : Once the Crawler finishes, it will create the new table based on the deduplicated version of the full top customers dataset. This deuplicated version will then be served as the source dataset for the fictitious Data Analytics Team to work with it the AWS Glue Databrew. Once you are ready move to the Part 7 - Data Quality & Data Preparation with AWS Glue DataBrew .","title":"WORKFLOW ORCHESTRATION WITH AWS STEP FUNCTION"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#1-creating-the-step-function-workflow","text":"Before creating the Step Function Workflow , let's first create one more Crawler . This cralwer is actualy part of the Step Function Workflow you are going to build and it is required!","title":"1. Creating the Step Function Workflow"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#final-crawler","text":"Run the following code on your Cloud9 Enviroment Terminal to create the ml-final-crawler : aws glue create-crawler \\ --name ml-final-crawler \\ --role AWSGlueServiceRole-etl-ttt-demo \\ --database-name glue_ttt_demo_db \\ --table-prefix ml_final_\\ --targets \"{\\\"S3Targets\\\": [{\\\"Path\\\": \\\"s3:// ${ BUCKET_NAME } /etl-ttt-demo/output/data_analytics_team_folder/top-customers/\\\"}]}\" As you can see, the ml-final-crawler will crawl the shared Analytics Team Folder once the files land there.","title":"Final Crawler"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#step-function-workflow","text":"Now, let's finally create the Step Function Workflow . For this, go to the AWS Step Function console (switch to your right region if needed!) and click on State machines , under Step Functions label on the far left menu of the AWS Step Function Console. Then, click on Create state machine . On the Choose authoring method , select Write your workflow in code and under Type choose Standard . At the bottom of the same page, under Definition paste the following code: { \"Comment\" : \"A description of my state machine\" , \"StartAt\" : \"Find Match Job\" , \"States\" : { \"Find Match Job\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::glue:startJobRun.sync\" , \"Parameters\" : { \"JobName\" : \"ml-lab-notebook-job\" }, \"Comment\" : \"To remove duplicates in the full customer dataset using FIndMatches ML Transform and output the results in another output bucket\" , \"Next\" : \"Crawl Dedup Data\" }, \"Crawl Dedup Data\" : { \"Type\" : \"Task\" , \"End\" : true , \"Parameters\" : { \"Name\" : \"ml-final-crawler\" }, \"Resource\" : \"arn:aws:states:::aws-sdk:glue:startCrawler\" , \"Comment\" : \"To crawl the data after FindMatches dedup higiene process\" } } } You should see the Step Function Workflow Studio Canvans getting updated with the graphical flow of the steps you set with the code. Click Next and, on the Specity details page, do the following: Under Name give your State Machine Name the name of findmatches-ML-dedup-workflow . Under Permissions , select the Choose an existing role option and set the IAM Role to AWSStepFunctionRole-etl-ttt-demo . Scroll further down and click on the Create state machine button to create the Step Function State Machine Workflow .","title":"Step Function Workflow"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#2-creating-an-eventbridge-rule-target-for-the-step-function-workflow-via-cli","text":"Now, to be able to trigger the Step Function Workflow , you also need to create a new EventBridge Rule & Target . Run the following steps in your Cloud9 Enviroment Terminal in sequence: First the Event Rule : aws events put-rule \\ --name \"find-matches-rule\" \\ --event-pattern \"{ \\ \\\"source\\\": [\\\"aws.s3\\\"], \\ \\\"detail-type\\\": [\\\"AWS API Call via CloudTrail\\\"], \\ \\\"detail\\\": { \\ \\\"eventSource\\\": [\\\"s3.amazonaws.com\\\"], \\ \\\"eventName\\\": [\\\"PutObject\\\"], \\ \\\"requestParameters\\\": { \\ \\\"bucketName\\\": [\\\" ${ BUCKET_NAME } \\\"], \\ \\\"key\\\": [{\\\"prefix\\\": \\\"etl-ttt-demo/data_analytics_team_folder/\\\"}] } \\ } \\ }\" Then the Event Target : aws events put-targets \\ --rule find-matches-rule \\ --targets \"Id\"=\"findmatches-ML-dedup-workflow\",\"Arn\"=\"arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:findmatches-ML-dedup-workflow\",\"RoleArn\"=\"arn:aws:iam:: ${ AWS_ACCOUNT_ID } :role/AWSEventBridgeInvokeRole-etl-ttt-demo\" \\ --region ${ AWS_REGION } After running both commands, go to AWS EventBridge console. (switch to your right region if needed!) . On the left side menu, click on Rules , then click on top of the rule find-matches-rule to open its details. You should see under the Event Pattern tab the following: This pattern is basically saying that this EventRule is watching for every PutObject event that occur in the bucket/prefix specified in there. Click on the Targets tab. Here, you can see that the Target Name points to the Step Functions Workflow you just created findmatches-ML-dedup-workflow . Also that it has a EventBridge Role , with all the required permissions, associated with it. This Role has also been created as part of the CloudFormation template . You are done creating the resources required for this Orchestration to work. Now, you need to simulate files being upladed into the central shared bucket repository of the Data Analytics Team, that means, the etl-ttt-demo/output/data_analytics_team_folder/top-customers/ . To do that, go back to your Cloud9 Enviroment Terminal and run the following S3 command to upload a file that contains duplicated data into the Data Analytics Team's shared folder : aws s3 cp /tmp/dsd/csv_tables/ml-lab/ml-customer-full/full-top-customer.csv s3://$BUCKET_NAME/etl-ttt-demo/data_analytics_team_folder/ Once the above file is uploaded, it will then trigger the Step Function Workflow that you created which, consequently, will run the ml-lab-notebook-job that contains the FindMatches Transform and all the necessary code you wrote to transform the source duplicated data into a deduplicated version of the full top customers dataset that the fictitious Data Analytics Team will use in order to proceed with their Data Preparation work .","title":"2. Creating an EventBridge Rule &amp; Target for the Step Function Workflow (via CLI)"},{"location":"06-StepFunctionsLab/06-step-functions-lab/#following-the-flow","text":"You can follow the flow by going first to the Step Function Workflow page and looking your findmatches-ML-dedup-workflow workflow. There, under Executions , you can click on the one that Status column says Running and click on View Details . You should see the graphical workflow changing its states. From there, go to your Glue Studio Console (switch to your right region if needed!) and look for your job ml-lab-notebook-job under the Monitoring page. Then, click on the big blue number under Running to scroll to the running Jobs, and confirm that your job is running. Finally, go to the Glue Console (switch to your right region if needed!) and look for two things: The FindMatches ML Transform : This should take about 6-7 minutes to complete: The ml_final_crawler : This will start as soon as your ml-lab-notebook-job succeed and it will create 1 new table : Once the Crawler finishes, it will create the new table based on the deduplicated version of the full top customers dataset. This deuplicated version will then be served as the source dataset for the fictitious Data Analytics Team to work with it the AWS Glue Databrew. Once you are ready move to the Part 7 - Data Quality & Data Preparation with AWS Glue DataBrew .","title":"Following the flow"},{"location":"07-DataBrewLab/07-databrew-lab/","text":"< THIS LAB IS STILL UNDER CONSTRUCTION > DATA QUALITY & DATA PREPARATION WITH AWS GLUE DATABREW AWS Glue DataBrew is a visual data preparation tool that makes it easy for data analysts and data scientists to prepare data with an interactive, point-and-click visual interface without writing code. With Glue DataBrew, you can easily visualize, clean, and normalize terabytes, and even petabytes of data directly from your data lake, data warehouses, and databases, including Amazon S3, Amazon Redshift, Amazon Aurora, and Amazon RDS. AWS Glue DataBrew is built for users who need to clean and normalize data for analytics and machine learning. Data analysts and data scientists are the primary users. In this final lab, Part 7 - Data Quality & Data Preparation with AWS Glue DataBrew - You, as part of the fictitious Data Analytics Team , will work with it the AWS Glue Databrew to create, profile and prepare datasets while, at the same time, work, evolve and public Step Recipes using Glue DataBrew Recipes. 1. Creating Datasets & Profiling the Data (with Quality Rules). (switch to your right region if needed!) \u00b6 Go to the AWS Glue DataBrew console. [CREATE A DATASET FROM THE SHIVS THIRD DEMO (INCOME BAND AND CUSTOMER TABLE] Create TopCustomer Dataset from Data Catalog (Cataloged via crawler + step functions in the previous lab after removing duplicates with ML Transform job) [PROFILE THE CUSTOMER DATASET WITH DATA QUALITY RULES - 3-5 minutes for profile job to finish] Full dataset Output Settings: Navigate to - s3://$BUCKET_NAME/etl-ttt-demo/output/databrew/jobprofiles/ [CREATE RULE SET] TopCustomer-Ruleset [ADD RECOMMENDED QUALITY RULES + ANY OTHER I WANT] Rule Set Name: TopCustomer-Ruleset Associated Dataset: TopCustomers recommended rules: duplicates? (The rule will pass if dataset has duplicate rows count <= 1 ) missing values less than 2% custom rules: The rule will pass if c_email_address has length >= 5 FOR greater than or equal to 100% of rows Create Income Band from RDS (JDBC connection) - Table Name: income_band \u00b6 [PROFILE INCOME_BAND DATASET WITH 1 QUALITY RULE] Full dataset Ruleset: Income-Band-Ruleset Rule Name: Income Check for Negative Values Rule Summary: The rule will pass if ib_lower_bound , ib_upper_bound has values >= 0 FOR greater than or equal to 100% of rows 2. Working with DataBrew Recipes & Projects \u00b6 [CREATE A PROJECT FOR INCOME_BAND DATASET] Project Name: TopCustomers-PerBand show all the transformations available (quick overview) Choose: Join - Choose Customer Dataset to join with A RIGHT JOIN -Join by Income_Band Sk and Birthday_day (income band goes from 1-20 only) -Remove ib_income_band_sk Ask people to download from here: [fix the command - that is an upload command!!] mkdir -p /tmp/dsd/databrew-recipe/ aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/databrew/TopCustomers-PerBand-recipe.json /tmp/dsd/databrew-recipe/TopCustomers-PerBand-recipe.json aws s3 cp --recursive /tmp/dsd/databrew-recipe/ s3:// $BUCKET_NAME /etl-ttt-demo/databrew-recipe/ To add the steps: Go to Recipes and import the recipe received from data analytics team Show all the steps in the recipe and the JSON original recipe. Publish the recipe Go back to the Project: - Import the Recipe into the project (Append!!) - Explain step validation (append/overwrite) - Reorder the Rename steps (17,18 and 19) - Create an Age field using DATE_DIFF only. (between date_of_birth and todays date ) - Drop the c_birth__month, date_of_birth, c_login and match_id columns - Save and Publish this as a new recipe. Run the Job and share the results.","title":"DATA QUALITY & PREPARATION WITH AWS GLUE DATABREW"},{"location":"07-DataBrewLab/07-databrew-lab/#1-creating-datasets-profiling-the-data-with-quality-rules-switch-to-your-right-region-if-needed","text":"Go to the AWS Glue DataBrew console. [CREATE A DATASET FROM THE SHIVS THIRD DEMO (INCOME BAND AND CUSTOMER TABLE] Create TopCustomer Dataset from Data Catalog (Cataloged via crawler + step functions in the previous lab after removing duplicates with ML Transform job) [PROFILE THE CUSTOMER DATASET WITH DATA QUALITY RULES - 3-5 minutes for profile job to finish] Full dataset Output Settings: Navigate to - s3://$BUCKET_NAME/etl-ttt-demo/output/databrew/jobprofiles/ [CREATE RULE SET] TopCustomer-Ruleset [ADD RECOMMENDED QUALITY RULES + ANY OTHER I WANT] Rule Set Name: TopCustomer-Ruleset Associated Dataset: TopCustomers recommended rules: duplicates? (The rule will pass if dataset has duplicate rows count <= 1 ) missing values less than 2% custom rules: The rule will pass if c_email_address has length >= 5 FOR greater than or equal to 100% of rows Create Income Band from RDS (JDBC connection)","title":"1. Creating Datasets &amp; Profiling the Data (with Quality Rules). (switch to your right region if needed!)"},{"location":"07-DataBrewLab/07-databrew-lab/#-table-name-income_band","text":"[PROFILE INCOME_BAND DATASET WITH 1 QUALITY RULE] Full dataset Ruleset: Income-Band-Ruleset Rule Name: Income Check for Negative Values Rule Summary: The rule will pass if ib_lower_bound , ib_upper_bound has values >= 0 FOR greater than or equal to 100% of rows","title":"- Table Name: income_band"},{"location":"07-DataBrewLab/07-databrew-lab/#2-working-with-databrew-recipes-projects","text":"[CREATE A PROJECT FOR INCOME_BAND DATASET] Project Name: TopCustomers-PerBand show all the transformations available (quick overview) Choose: Join - Choose Customer Dataset to join with A RIGHT JOIN -Join by Income_Band Sk and Birthday_day (income band goes from 1-20 only) -Remove ib_income_band_sk Ask people to download from here: [fix the command - that is an upload command!!] mkdir -p /tmp/dsd/databrew-recipe/ aws s3 cp s3://ee-assets-prod- ${ AWS_REGION } /modules/31e125cc66e9400c9244049b3b243c38/v1/downloads/etl-ttt-workshop/databrew/TopCustomers-PerBand-recipe.json /tmp/dsd/databrew-recipe/TopCustomers-PerBand-recipe.json aws s3 cp --recursive /tmp/dsd/databrew-recipe/ s3:// $BUCKET_NAME /etl-ttt-demo/databrew-recipe/ To add the steps: Go to Recipes and import the recipe received from data analytics team Show all the steps in the recipe and the JSON original recipe. Publish the recipe Go back to the Project: - Import the Recipe into the project (Append!!) - Explain step validation (append/overwrite) - Reorder the Rename steps (17,18 and 19) - Create an Age field using DATE_DIFF only. (between date_of_birth and todays date ) - Drop the c_birth__month, date_of_birth, c_login and match_id columns - Save and Publish this as a new recipe. Run the Job and share the results.","title":"2. Working with DataBrew Recipes &amp; Projects"},{"location":"08-CleanUp/08-clean-up/","text":"CLEAN UP If you are at an AWS hosted event and used Event Engine for your labs, you can skip the following clean up steps. If you are using your own AWS account for the labs, we strongly recommend you to perform the clean up steps as soon as you finish the labs to avoid extra charges . Execute the commands below in Cloud9 terminal to delete the AWS resources created during labs. If you choose to use names that are different than the default names provided in the lab instructions, please modify the names to match your own resources in the commands below. 1. Delete the DataBrew project , jobs , and dataset . aws databrew delete-job --name covid-testing-recipe-job aws databrew delete-job --name covid-testing-profile-job aws databrew delete-project --name covid-testing aws databrew delete-dataset --name covid-testing 2. Find the recipe versions in the recipe store and delete the versions list by below cli command. aws databrew list-recipe-versions --name covid-testing-recipe Replace the recipe version number with what you find from command above. aws databrew batch-delete-recipe-version --name covid-testing-recipe --recipe-version \"1.0\" 3. Delete Glue jobs created during the labs. aws glue delete-job --job-name glueworkshop-lab3-etl-job aws glue delete-job --job-name glueworkshop-lab3-etl-ddb-job aws glue delete-job --job-name glueworkshop_lab4_glue_streaming aws glue delete-job --job-name glueworkshop-lab6-basic-job aws glue delete-job --job-name glueworkshop-lab6-advance-job aws glue delete-job --job-name glueworkshop-lab6-streaming-job 4. Delete database and crawlers created in Part XXX. aws glue delete-crawler --name console-lab1 aws glue delete-crawler --name cli-lab1 aws glue delete-crawler --name event-notification-lab1 aws glue delete-database --name console-glueworkshop aws glue delete-database --name cli-glueworkshop aws glue delete-database --name covid19_gluedemocicdtest aws glue delete-database --name covid19parquet_gluedemocicdtest Depends on which optional section you finished in Lab 01, you may need to perform additional cleanup with the commands below. aws glue delete-crawler --name python-lab1 aws glue delete-crawler --name lab1-without-partition-index aws glue delete-crawler --name lab1-with-partition-index aws glue delete-database --name athena_glueworkshop aws glue delete-database --name python-glueworkshop aws glue delete-database --name spark_glueworkshop aws glue delete-database --name partition_index_glueworkshop 5. Delete Lab 8 objects aws glue delete-workflow --name glueworkshop-event-driven aws glue delete-workflow --name COVID-19-workflow aws glue delete-trigger --name s3-object-trigger aws events list-targets-by-rule --rule glueworkshop-rule aws events remove-targets --rule glueworkshop-rule --ids \"glueworkflow\" aws events remove-targets --rule glueworkshop-rule --ids \"stepfunction\" aws events delete-rule --name glueworkshop-rule aws stepfunctions delete-state-machine --state-machine-arn arn:aws:states: ${ AWS_REGION } : ${ AWS_ACCOUNT_ID } :stateMachine:COVID-19-workflow 6. Delete the lab S3 bucket content. aws s3 rm s3:// ${ BUCKET_NAME } --recursive 7. Delete the lab Cloudformation stacks. aws cloudformation delete-stack --stack-name glueworkshop aws cloudformation delete-stack --stack-name glueworkshop-cicd If you used cloudformation option in Lab 01, you need to cleanup the Cloudforamtion stack for the Glue Data Catalog as well. aws cloudformation delete-stack --stack-name demo-covid-lake 8. Close the browser tab of Cloud9 IDE. Now you have cleaned up all the resources created during the workshop.","title":"Clean Up"}]}